[{"title":"Mujoco - 高场hfield相关","url":"/2024/09/26/01-Hfield/","content":"\r\nThe hfield type defines a height field geom. The\r\ngeom must reference the desired height field asset with the hfield\r\nattribute below. The position and orientation of the geom set the\r\nposition and orientation of the height field. The size of the geom is\r\nignored, and the size parameters of the height field asset are used\r\ninstead. See the description of the hfield\r\nelement. Similar to planes, height field geoms can only be attached to\r\nthe world body or to static children of the world.\r\n\r\n\r\n可从PNG的灰度图像加载高场数据。每个像素即为一个高度，黑低白高。\r\n可从bin文件读入，格式如下：\r\n(int32)   nrow(int32)   ncol(float32) data[nrow*ncol]\r\n高度数据可以在编译时保持未定义。\r\n\r\n\r\n编译器会自动把高度数据归一化到[0,1]\r\n高场的位置和方向由geom确定，空间范围由hfield的size字段决定。（与mesh相同）\r\n高场视为三棱柱的并集，碰撞时首先确认可能碰撞的棱柱网格，然后通过凸面碰撞器计算。高场和geom的碰撞上限限制为50，超过的则被舍弃。\r\n\r\n参数\r\n\r\nname:\r\n名称，用于引用。如果忽略name，可用不带路径和后缀名的文件名代替引用。\r\ncontent_type:\r\n目前支持image/png和image/vnd.mujoco.hfield。\r\nfile:\r\n文件名，若后缀为.png（不区分大小写），则按图像读入；否则以二进制文件读入。\r\nnrow, ncol: 行数和列数。默认值 0\r\n表示将从文件加载数据。\r\nelevation: 高场，自动归一，默认值0。\r\nsize:\r\n(radius_x、radius_y、elevation_z、base_z)，分别是x、y方向的半径，最大高度，和基础厚度。\r\n\r\n使用样例\r\n&lt;mujoco&gt;    &lt;asset&gt;      &lt;hfield file=&quot;./data/height_field.bin&quot; name=&quot;customTerrain&quot; ncol=&quot;100&quot; nrow=&quot;100&quot; size=&quot;50 50 1 0.1&quot;/&gt;    &lt;/asset&gt;    &lt;worldbody&gt;      &lt;geom hfield=&quot;customTerrain&quot; pos=&quot;0 0 0&quot; type=&quot;hfield&quot;/&gt;    &lt;/worldbody&gt;&lt;/mujoco&gt;\r\n","categories":["机器仿真"],"tags":["mujoco","hfield"]},{"title":"Mujoco - 碰撞凸几何体要求","url":"/2024/09/29/02-Mujoco_notes/","content":"Collision\r\nComputation\r\n- MuJoCo Documentation\r\n\r\nWe have chosen to limit collision detection to convex geoms.\r\nAll primitive types are convex. Height fields are not convex but\r\ninternally they are treated as unions of triangular prisms (using custom\r\ncollision pruning beyond the filters described above). Meshes specified\r\nby the user can be non-convex, and are rendered as such. For collision\r\npurposes however they are replaced with their convex hulls.\r\n\r\n\r\n碰撞检测限制在凸几何体\r\n所有原始类型都是凸的\r\n高度字段不是凸的，但在内部它们被视为三棱柱的并集\r\n网格可以是非凸的，并且如此渲染。然而，出于碰撞目的，它们被替换为凸包\r\n\r\n\r\nIn order to model a non-convex object other than a height field, the\r\nuser must decompose it into a union of convex geoms (which can be\r\nprimitive shapes or meshes) and attach them to the same body. Open tools\r\nlike the CoACD library\r\ncan be used outside MuJoCo to automate this process. Finally, all\r\nbuilt-in collision functions can be replaced with custom callbacks. This\r\ncan be used to incorporate a general-purpose “triangle soup” collision\r\ndetector for example. However we do not recommend such an approach.\r\nPre-processing the geometry and representing it as a union of convex\r\ngeoms takes some work, but it pays off at runtime and yields both faster\r\nand more stable simulation.\r\n为了对高度场以外的非凸对象进行建模，用户必须将其分解为凸几何体（可以是原始形状或网格）的并集并将它们附加到同一实体。可以在\r\nMuJoCo 外部使用CoACD\r\n库等开放工具来自动化此过程。最后，所有内置碰撞函数都可以替换为自定义回调。例如，这可用于合并通用“三角汤”碰撞检测器。但是我们不推荐这种方法。预处理几何体并将其表示为凸几何体的联合需要一些工作，但它在运行时得到回报，并产生更快、更稳定的模拟。\r\n\r\n","categories":["机器仿真"],"tags":["mujoco","collision","convex"]},{"title":"NeRF和3DGS对比","url":"/2024/10/30/04-NeRFvs3DGS/","content":"NeRF\r\n原理\r\nNeRF（Neural Radiance\r\nFields）是一种新型的3D场景表示方法，通过神经网络来生成逼真的3D场景。NeRF\r\n的核心思想是利用神经网络来表示一个场景的体素，从而可以实现从不同角度对场景进行渲染，生成高质量的图像。\r\n特点：\r\n\r\nNeRF中三维模型的信息是以“隐式”的方法存储，而非点云、体素、网格等显式的表达方式。\r\nNeRF使用类似光线追踪的方式创建新视角的图像。输入是采样点和观测的方向，输出是对应的RGB值和不透明度。\r\n\r\n流程\r\n\r\n\r\n输入信息：NeRF\r\n将3D空间中的点（例如摄像机的位置）和观察方向作为输入。\r\n神经网络映射：通过一个MLP（多层感知器）神经网络，将每个3D坐标点和观察方向映射到颜色和体素密度。网络的输出包含颜色（RGB）和体密度（衡量光线穿过该点的透明度）。\r\n体渲染技术：NeRF\r\n使用体渲染公式，即光线穿过场景的过程。具体来说，它沿着光线方向对多个点进行采样，并根据体密度和颜色计算每个点的贡献，进而得出最终的像素值。\r\n损失函数：通过多视图监督进行训练，NeRF会在场景中从多个视角捕获图像，将渲染的像素值与真实图像进行对比，并通过优化损失函数不断调整网络参数，逐步逼近真实场景。\r\n生成新视角：训练完成后，NeRF\r\n能够在未见过的视角上生成图像，实现自由的3D视角切换和逼真的场景合成。\r\n\r\n网络结构\r\n\r\n\r\n不透明度只和空间位置有关，颜色与空间位置和视角有关。\r\n\r\n实现细节\r\nMLP网络\r\n\r\n层数：NeRF的网络由10个隐藏层组成，前9层包含256个神经元，最后一层包含128个神经元。\r\n分支结构：NeRF的MLP有一个特定的分支结构。前8层用于处理空间位置信息(x,y,z)，生成了隐变量（latent\r\nfeatures）。这些特征被用来预测体密度（density），以表示该点在场景中的不透明度。\r\n跳跃连接：位置编码后的输入会与网络中间层（第5层）相连，作为跳跃连接（skip\r\nconnection），帮助网络在深层结构中保持空间信息的细节。\r\n\r\n位置编码\r\n\r\nNeRF\r\n使用了傅里叶特征来将输入的3D坐标和视角方向进行编码，这种方式称为位置编码（Positional\r\nEncoding）。具体来说，它将输入扩展到高频率空间，这样网络就能够学习更细腻的细节。\r\n对于坐标 (x,y,z)(x, y, z)(x,y,z) 和方向\r\n(θ,ϕ)，每个输入维度会生成多个不同频率的正余弦特征，以捕捉场景中的复杂空间结构。\r\n\r\n体渲染\r\n​\r\n从焦点到一个像素上连的射线为r(t)=o+td，其中其中o是原点，t是距离。距离起点（near\r\nbound）和距离终点（far bound）为tn和tf。\r\n​ 获得像素颜色的公式：\r\n​ \r\n​ 这个式子积分里面是T(t)\r\n、密度 σ(r(t))和颜色c(r(t),d)的乘积，其中T(t)是累积透光率，表示光线射到这“还剩多少光”。实际渲染过程是把射线平均分成N个小区间，每个区间随机采样一个点，对采样得到的点的颜色进行某种加权求和。\r\n相关工作\r\nNeRF2Mesh\r\n\r\n最左侧的立方体就是Nerf所构建的三维数据，它包含离散点的三维坐标、不透明度（density）以及rgb色彩。\r\nNerf2Mesh的整体结构也像图中分为上下两个分支：\r\n\r\n密度分支 (Density\r\nfield)：首先利用NeRF生成的密度场划分出体素，再用Marching\r\nCubes生成三角网格，再进行优化。\r\n外观分支 (Appearance\r\nField)：Nerf输出的内容（RGB）经过MLP1提取特征，然后分成两个分支分别提取漫反射和镜面反射（镜面反射会多经过一个MLP层）的分量。\r\n\r\n3D Gaussian Splatting\r\n原理\r\n3D Gaussian Splatting 的核心思想是用高斯分布来表示 3D\r\n空间中的点云，将场景中的点用 3D\r\n高斯函数表示。这些高斯函数即“高斯球”，通过一组参数（均值、协方差矩阵等）来描述位置、方向、大小和形状。相比于单纯的点云，使用高斯分布可以更好地对点的位置和形状进行逼真地表达，使得结果更平滑并且抗噪性更好。\r\n在渲染过程中，场景以相机视角来观察这些高斯分布的点，生成一个图像。每个像素的颜色和透明度是通过聚合沿视线方向的高斯球信息来计算的。这些高斯分布产生的重叠区域，通过数学上的加权平均可以很好地呈现出自然的模糊边缘，避免了传统点云中颗粒状的视觉问题。\r\n流程\r\n\r\n\r\n点云采样：使用SfM从一组图像中估计出点云，可以直接调用\r\nCOLMAP\r\n库操作\r\n高斯分布：将每个点分配一个高斯分布。初始高斯分布通过位置、颜色、透明度和协方差矩阵等参数来定义。\r\n投影变换：通过投影操作将 3D\r\n高斯分布映射到相机视角下的 2D 图像平面。\r\n光栅化：对投影后的高斯分布进行渲染。\r\n优化：通过计算图像与目标图像之间的损失，反向传播梯度（Gradient\r\nFlow）来调整高斯参数。以及自适应地调整高斯分布的密度。\r\n\r\n实现细节\r\n三维高斯属性\r\n用三维高斯分布构建基础元素，属性有中心μ、不透明度α、三维协方差矩阵（表示缩放程度）Σ和颜色c。其中颜色c与视角有关，由球谐函数表示。\r\n球谐函数的示例：\r\n\r\n（图源：3DGS综述以及对3DGS的理解：A\r\nSurvey on 3D Gaussian Splatting - 知乎）\r\n自适应密度控制\r\n\r\n\r\nUnder-Reconstruction:\r\n复制一个当前高斯分布的副本，然后沿着位置梯度方向移动它。\r\nOver-Reconstruction:\r\n将当前高斯分布分割成两个较小的高斯分布，再对其进行移动。\r\n\r\n点的剪枝\r\n对于冗余的高斯分布，在迭代过程中会逐渐消除。不透明度太低的高斯分布和过大的高斯分布都会在迭代中逐渐消除。以节省资源。\r\nTile\r\n为了降低运算成本，3DGS将图像分割为数个不重叠的patch，称为tile，每个tile为16×16像素。在此基础上，3DGS计算投影后的高斯与tile的相交情况。由于高斯可能与多个tile相交，所以对其进行了复制，并为其分配tile的标识符。不同的tile可以在不同的线程或\r\nGPU 核心上同时计算，避免了对同一像素的竞争。\r\n对比\r\n场景绘制\r\n\r\nNeRF使用光线追踪来创建新视点的图像。通过模拟从像素位置发射的光线来计算图像中每个像素的颜色，运行速度较慢。但渲染更真实。\r\n3DGS\r\n使用光栅化来创建新视点的图像，通过Splatting进行渲染，运算速度更快，但3DGS创建了数百万个gaussion，占用的内存比NeRF多数倍。\r\n\r\n3D重建\r\n\r\nNeRF的输出需要借助其他方法转为显式输出，如NeRF2Mesh。\r\n3DGS训练后最终得到的是一个文件，其中包含每个高斯的高斯参数列表，例如位置、大小、颜色和透明度。3DGS的输出是比较显式的表达。\r\n\r\n在CSDN上有一个3DGS和NeRF的对比表格，但数据来源并未标明，可靠性存疑（而且他的3DGS怎么是3D\r\nGeometry Sensing的缩写，虽然他给的结构图确实是3D高斯的）：\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n对比维度\r\n3DGS (3D Geometry Sensing)\r\nNeRF (Neural Radiance Fields)\r\n\r\n\r\n\r\n\r\n基本原理\r\n基于几何推断，通过多视角图像、深度传感器、LiDAR等获取显式3D几何信息。\r\n基于神经网络拟合体积辐射场，通过多视角图像学习隐式表示，渲染出场景。\r\n\r\n\r\n输入数据\r\n多视角图像、深度信息（LiDAR、ToF相机）、位姿数据、点云。\r\n多视角图像（通常包括相机位姿），不需要显式的几何信息。\r\n\r\n\r\n输出结果\r\n点云、网格、三角形模型、深度图、纹理映射等显式几何结构。\r\n通过体积渲染生成逼真图像（视角相关），不直接输出几何模型。\r\n\r\n\r\n数据处理方式\r\n使用几何关系（如三角测量、立体视觉等）来显式重建场景结构。\r\n使用神经网络隐式建模颜色和密度，通过体积渲染生成图像。\r\n\r\n\r\n几何信息\r\n显式获取3D几何信息，可以精确测量物体的距离和形状。\r\n隐式推断几何信息，主要用于图像渲染，几何结构不直接输出。\r\n\r\n\r\n渲染效果\r\n依赖于重建的几何结构，渲染效果有限，尤其在复杂光线场景下效果一般。\r\n渲染效果非常逼真，尤其在反射、遮挡、折射等复杂光照场景表现优异。\r\n\r\n\r\n计算资源需求\r\n需要较强的几何计算能力，数据获取通常依赖于多传感器系统（LiDAR等）。\r\n需要高计算资源，特别是训练神经网络的过程计算量大，通常依赖于GPU。\r\n\r\n\r\n渲染速度\r\n实时性较好，特别是有深度传感器时可实现快速重建。\r\n渲染速度较慢，尤其在训练阶段耗时长，但有即时渲染版本。\r\n\r\n\r\n应用场景\r\n自动驾驶、机器人导航、工业检测、3D建模、AR/VR、精密测量。\r\n电影视觉特效、虚拟旅游、虚拟现实内容生成、复杂光照场景的渲染。\r\n\r\n\r\n优点\r\n可以显式建模、精确几何测量、适用于实时应用；深度传感器辅助时重建精度高。\r\n渲染质量极高，能处理复杂的光照、遮挡问题；不需要昂贵的深度传感器。\r\n\r\n\r\n缺点\r\n在处理复杂光照（如透明物体、反射面）时效果不佳，依赖昂贵的传感器数据。\r\n渲染速度慢，训练时间长，初始设计不擅长生成明确的几何信息。\r\n\r\n\r\n几何建模精度\r\n高，适合用于需要精确几何信息的场景（如测量、导航、物理模拟等）。\r\n几何建模是隐式的，主要依赖于神经网络推断，不适合用于测量等任务。\r\n\r\n\r\n光照处理\r\n处理复杂光线条件较困难，通常需要额外的算法来应对光线反射和折射。\r\n对复杂光照场景处理效果出色，能够处理反射、折射、遮挡等问题。\r\n\r\n\r\n实时性\r\n实时性较强，特别是在配合LiDAR等传感器时。\r\n需要较长的时间进行训练和渲染，不适合实时应用（加速版本除外）。\r\n\r\n\r\n数据获取成本\r\n高，需要多视角相机或昂贵的深度传感器（如LiDAR）。\r\n低，仅需多视角图像数据，不依赖于专门的传感器。\r\n\r\n\r\n\r\n（表源：【3dgs】3DGS与NeRF对比_nerf和3dgs区别-CSDN博客）\r\n","categories":["三维重建"],"tags":["NeRF","3DGS"]},{"title":"GauU-Scene V2 论文解读","url":"/2024/11/05/05-GauU_Scene_V2/","content":"论文：\r\n\r\nGauU-Scene V2: Assessing the Reliability of Image-Based Metrics with\r\nExpansive Lidar Image Dataset Using 3DGS and NeRF\r\n\r\n数据集\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n方法\r\n做法\r\n缺点\r\n\r\n\r\n\r\n\r\nbungeenerf\r\n卫星捕获图像\r\n时间差异、无地面实况\r\n\r\n\r\nKITTI\r\n汽车雷达捕获点云数据\r\n屋顶、高层建筑捕获存在不足\r\n\r\n\r\nblockNeRF\r\nStyle Transformation解决时间差异\r\n不提供公开可用的点云数据集\r\n\r\n\r\nUrbanBIS\r\n多视图相机捕获点云\r\n未用高精度激光雷达\r\n\r\n\r\nUrbanscene3D\r\n无人机配合激光雷达\r\n坐标差异，雷达点云和图像关系不明确\r\n\r\n\r\n\r\n优势：\r\n\r\n利用Zenmuse L1来获取地面真实几何，而大多数数据集（ blocknerf ）（\r\nmegaNeRF ）（ UrbanBIS\r\n）依赖单目或多视图相机进行数据采集，这更适合新视图合成而不是场景重建。\r\n提供了城市规模的信息，包括高层建筑、湖泊、山脉和屋顶，而其他数据集很少提供。\r\ndouble-return技术，去除移动物体，确保更稳定的光照效果。\r\n去除飞行路线中连续图像之间的冗余信息，图像更少，但信息量仍然具有可比性。\r\n\r\n评估指标\r\nPSNR\r\nPSNR(Peak signal-to-noise ratio 峰值信噪比)\r\n用于表示信号的最大可能功率与影响其表示的保真度的破坏噪声的功率之间的比率，通常使用分贝标度表示为对数量。\r\n\r\n\r\nSSIM\r\nSSIM全称为Structural\r\nSimilarity，即结构相似性。算法会提取以下三个特征。\r\n\r\n亮度\r\n对比度\r\n结构\r\n\r\n亮度的估计与平均灰度有关：\r\n对比度的估计则用标准差衡量：\r\n结构比较是通过使用一个合并公式来完成。\r\n三个对比函数分别如下：\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n公式\r\n\r\n\r\n\r\n\r\n亮度\r\n\r\n\r\n\r\n对比度\r\n\r\n\r\n\r\n结构\r\n其中\r\n\r\n\r\n\r\n结合即得到SSIM函数：\r\n\r\nLPIPS\r\n学习感知图像块相似度(Learned Perceptual Image Patch Similarity,\r\nLPIPS)，通过深度学习模型来评估两个图像之间的感知差异。使用预训练的深度网络（如\r\nVGG、AlexNet）来提取图像特征，然后计算这些特征之间的距离，以评估图像之间的感知相似度。\r\n\r\n图示是将左右两幅图像与中间图像对比的结果。可以看到，传统方法（L2/PSNR,\r\nSSIM,\r\nFSIM）的结果与人的感知相反。而后三行通过神经网络提取特征的方法，能更符合人的感知，来评判图片的相似度。\r\n倒角距离\r\n出自：\r\n\r\nH. Fan, S. Hao, and L. Guibas, “A point set generation network for 3D\r\nobject reconstruction from a single image,” CVPR,\r\n2017.\r\n\r\n\r\n算法：\r\n\r\n对S1中任意一点x，计算它与S2中所有点的距离，取最小距离的平方。\r\n遍历S1中的点，重复1中过程，求和所有距离平方。\r\n同样的步骤，对S2中所有点遍历，重复1、2过程。\r\n将两个求和结果相加，作为倒角距离。\r\n\r\n倒角距离用于衡量两个点云之间的相似度。如果该距离较大，则说明两组点云区别较大；如果距离较小，则说明重建效果较好。\r\n实验\r\n实验环境\r\n\r\nVanilla Gaussian Splatting: RTX 3090 * 1\r\nNeRF-based models: RTX 3090 * 4\r\n\r\n实验结果\r\n\r\n\r\n3DGS和SuGaR在图像的渲染中，有更优的效果（三个参数都更优），且训练时间更短。\r\n\r\n\r\n\r\nNeRF: 使用ns-export生成3D点云。\r\n3DGS: 每个Gaussian Splatting的均值作为一个点，输出点云。\r\n神经辐射场生成的点云，通常包含很多与场景无关的异常值。\r\n3DGS也存在边缘效应，边缘会模糊。\r\n\r\n\r\n（图源项目网址：GauU-Scene）\r\n\r\nNeRFacto，虽然在图像生成得分最低，但是倒角距离最小。而Instant-NGP和SuGaR分别是倒数第一和倒数第二。这一实验结果揭示了基于图像的测量无法代表底层几何结构的基本事实。\r\nSuGaR\r\n是一种专为几何对齐设计的方法，排名却倒数。再对SuGaR进行定量分析，发现SuGaR在几何重建方面确实有更好的表现。\r\n\r\n\r\n图中的蓝点在其他方法中很常见，而绿色甚至略带红色的点在其他方法中却很少见。更何况SuGaR中绿色点如此之多。\r\n从定量的角度来看，如果我们忽略这里显示的异常值，SuGaR\r\n的确是最好的方法。\r\n\r\n对高斯泼溅的alpha值进行了简单分析，其中几乎三分之二的alpha值几乎是透明的。通过删除这些值，渲染的图像变得更加清晰，几乎没有信息丢失。这些近乎透明的高斯飞溅实例漂浮在3D空间中。尽管它们在渲染图像中看不到，但它们会导致几何测量指标的退化。\r\n","categories":["三维重建"],"tags":["NeRF","3DGS"]},{"title":"Mujoco - CoACD简略教程","url":"/2024/09/29/03-CoACD_notes/","content":"CoACD是一个凸分解工具，可以将凹模型粗略粗分解为凸几何体的并集。\r\n一、源代码编译\r\n安装教程\r\n(1) 克隆代码：\r\ngit clone --recurse-submodules https://github.com/SarahWeiii/CoACD.git\r\n(2) 安装依赖：\r\ncmake &gt;= 3.24g++ &gt;= 9, &lt; 12\r\n\r\n在我的Ubuntu22.04中，apt里的cmake包版本是3.22，不能用。用snap成功安装3.30版本。（源码安装好像也行，不过我懒得添加系统变量，就还是用snap安装了。\r\n\r\n(3) 编译\r\ncd CoACD \\&amp;&amp; mkdir build \\&amp;&amp; cd build \\\r\n​ 然后编译：\r\ncmake .. -DCMAKE_BUILD_TYPE=Release \\&amp;&amp; make main -j\r\n\r\n这里出了很多warning，但是好像不影响使用。\r\n\r\n(4) 使用\r\n./main -i PATH_OF_YOUR_MESH -o PATH_OF_OUTPUT\r\n参数说明\r\n\r\n-nm/--no-merge : 禁用合并后处理，默认为false。\r\n-c/--max-convex-hull :\r\n凸包上限，默认-1表示无限制。仅在启用合并时才有效。\r\n-ex/--extrude : 沿着重叠面挤出相邻的凸包。\r\n-am/--approximate-mode :\r\n近似形状类型（“ch”表示凸包，“box”表示立方体）。\r\n--seed : 随机种子，默认是random()。\r\n\r\n说明：\r\n\r\n大多数情况下，只需调整threshold\r\n（0.01~1）即可平衡细节程度和分解成分的数量。值越高，结果越粗，值越低，结果越细。\r\n默认参数是快速版本。可以牺牲运行时间获取更多组件数量，增加searching depth (-md)\r\n、\r\nsearching node (-mn)和searching iteration (-mi)可以获得更好的切割策略。\r\n\r\n二、PyPI\r\n安装\r\npip install coacd\r\n使用\r\nimport coacdmesh = trimesh.load(input_file, force=&quot;mesh&quot;)mesh = coacd.Mesh(mesh.vertices, mesh.faces)parts = coacd.run_coacd(mesh) # a list of convex hulls.\r\n参数\r\n官方文档里没有提到详细的参数列表，通过以下指令自己查询了下：\r\nimport inspectprint(inspect.getsource(coacd.run_coacd))\r\n查询到的参数和对应的解释如下：\r\n\r\nthreshold (float):\r\n用于决定分解精度的阈值。较低的阈值意味着更精确的分解，但可能需要更多的计算资源。默认值为\r\n0.05。\r\nmax_convex_hull (int):\r\n最大凸包数量。如果你希望限制生成的凸包数量，可以设置此值。例如，如果设置为\r\n5，则最多生成 5 个凸包，默认值为 -1，表示不限制。\r\npreprocess_mode (str):\r\n预处理模式，\"on\" 启用，\"off\"\r\n禁用，\"auto\"\r\n自动选择（推荐）。预处理可能影响分解的速度和结果。\r\nresolution (int):\r\n处理网格的分辨率，值越高，结果越精确，但计算量也越大。默认值为\r\n2000。\r\nmcts_nodes (int):\r\nMCTS（蒙特卡洛树搜索）中每个节点的最大数量。值越大，搜索的深度越高，可能会增加分解的准确度，但会消耗更多的计算资源。\r\nmcts_iterations (int):\r\nMCTS的迭代次数，更多的迭代次数意味着更高的准确性，但会增加计算负担。\r\npca (bool):\r\n是否使用主成分分析（PCA）来减少维度。True\r\n表示启用，False 表示禁用。\r\nmerge (bool): 是否合并小的凸包。如果设置为\r\nTrue，可能会减少生成的凸包数量。\r\ndecimate (bool):\r\n是否在生成凸包时进行简化，减少网格面片的数量。\r\nmax_ch_vertex (int):\r\n每个凸包允许的最大顶点数量。\r\nextrude (bool):\r\n是否对凸包进行拉伸。True 表示拉伸，False\r\n表示不拉伸。\r\nextrude_margin (float): 如果 extrude\r\n为 True，设置拉伸的边距。\r\napx_mode (str): 设置近似方式，\"ch\"\r\n代表使用凸包，\"box\" 表示使用包围盒。\r\nseed (int):\r\n随机种子，设置它可以帮助重现相同的结果。\r\n\r\n根据参数，一个简要的包含输出的代码如下：\r\nimport coacdimport trimeshimport osinput_file = &quot;input.obj&quot;mesh = trimesh.load(input_file, force=&quot;mesh&quot;)mesh = coacd.Mesh(mesh.vertices, mesh.faces)parts = coacd.run_coacd(    mesh,    threshold=0.05,  # 精度阈值，默认: 0.05    max_convex_hull=-1,  # 最大凸包数量，默认: -1 (无限制)    preprocess_mode=&quot;auto&quot;,  # 预处理模式，默认: &quot;auto&quot;    preprocess_resolution=30,  # 预处理分辨率，默认: 30    resolution=2000,  # 分解分辨率，默认: 2000    mcts_nodes=20,  # MCTS节点数，默认: 20    mcts_iterations=150,  # MCTS迭代次数，默认: 150    mcts_max_depth=3,  # MCTS最大深度，默认: 3    pca=False,  # 是否启用PCA降维，默认: False    merge=True,  # 是否合并小凸包，默认: True    decimate=False,  # 是否简化凸包，默认: False    max_ch_vertex=256,  # 每个凸包的最大顶点数，默认: 256    extrude=False,  # 是否拉伸凸包，默认: False    extrude_margin=0.01,  # 拉伸边距，默认: 0.01    apx_mode=&quot;ch&quot;,  # 近似模式，默认: &quot;ch&quot; (凸包)    seed=0  # 随机种子，默认: 0)output_dir = &quot;./output_parts/&quot;if not os.path.exists(output_dir):    os.makedirs(output_dir)for i, part in enumerate(parts):    part_mesh = trimesh.Trimesh(vertices=part[0], faces=part[1])    part_mesh.export(os.path.join(output_dir, f&quot;part_&#123;i&#125;.obj&quot;))print(f&quot;Exported &#123;len(parts)&#125; convex parts to &#123;output_dir&#125;&quot;)\r\n","categories":["机器仿真"],"tags":["mujoco","convex","CoACD"]},{"title":"Mathjax与渲染引擎marked冲突解决方案","url":"/2024/11/11/07-Build_blog/","content":"问题说明\r\nhexo在解析markdown的时候，会对一些符号如_进行转义，将其转为&lt;em&gt;标签。而在公式块中，_是作为渲染下标所用的符号。但是hexo的优先级比mathjax更高，且不会判断_是否在公式内，所以很容易造成冲突，导致公式没法正常渲染。比如笔者在编写博客时，发现两个相邻公式中的_被当成斜体渲染，导致两个公式都没正常渲染出来。\r\n踩坑过程\r\n1.修改源码（失败）\r\n太长了，折叠一下，想看就展开吧👇。\r\nFolding 点击查看更多搜到的大部分解决方案，都是让更改marked源码，即将nodes_modules/lib/marked/lib/marked.js文件进行更改：\r\n将\r\nescape: /^\\\\([\\\\`*{}\\[\\]()# +\\-.!_&gt;])/,\r\n替换为\r\nescape: /^\\\\([`*{}\\[\\]()# +\\-.!_&gt;])/,\r\n从而去除\\\\的转义。\r\n将\r\nem: /^\\b_((?:[^_]|__)+?)_\\b|^\\*((?:\\*\\*|[\\s\\S])+?)\\*(?!\\*)/,\r\n替换为\r\nem:/^\\*((?:\\*\\*|[\\s\\S])+?)\\*(?!\\*)/,\r\n从而去除_的斜体转义。\r\n然而不知道是博主使用的版本问题还是什么原因，并没有找到该文件。虽然后面在nodes_modules/marked/bin里找到了marked.js，它通过调用了一个库\r\nmarked.esm.js 来执行具体的 Markdown\r\n解析操作。因此博主在marked.esm.js进行了相应的更改。\r\n将\r\nemStrong: {  lDelim: /^(?:\\*+(?:([punct_])|[^\\s*]))|^_+(?:([punct*])|([^\\s_]))/,   //        (1) and (2) can only be a Right Delimiter. (3) and (4) can only be Left.  (5) and (6) can be either Left or Right.  //          () Skip orphan inside strong                                      () Consume to delim     (1) #***                (2) a***#, a***                             (3) #***a, ***a                 (4) ***#              (5) #***#                 (6) a***a  rDelimAst: /^(?:[^_*\\\\]|\\\\.)*?\\_\\_(?:[^_*\\\\]|\\\\.)*?\\*(?:[^_*\\\\]|\\\\.)*?(?=\\_\\_)|(?:[^*\\\\]|\\\\.)+(?=[^*])|[punct_](\\*+)(?=[\\s]|$)|(?:[^punct*_\\s\\\\]|\\\\.)(\\*+)(?=[punct_\\s]|$)|[punct_\\s](\\*+)(?=[^punct*_\\s])|[\\s](\\*+)(?=[punct_])|[punct_](\\*+)(?=[punct_])|(?:[^punct*_\\s\\\\]|\\\\.)(\\*+)(?=[^punct*_\\s])/,  rDelimUnd: /^(?:[^_*\\\\]|\\\\.)*?\\*\\*(?:[^_*\\\\]|\\\\.)*?\\_(?:[^_*\\\\]|\\\\.)*?(?=\\*\\*)|(?:[^_\\\\]|\\\\.)+(?=[^_])|[punct*](\\_+)(?=[\\s]|$)|(?:[^punct*_\\s\\\\]|\\\\.)(\\_+)(?=[punct*\\s]|$)|[punct*\\s](\\_+)(?=[^punct*_\\s])|[\\s](\\_+)(?=[punct*])|[punct*](\\_+)(?=[punct*])/ // ^- Not allowed for _},\r\n替换为\r\nemStrong: {  // 仅匹配 * 而不匹配 _  lDelim: /^(?:\\*+(?:([punct*])|[^\\s*]))/,   // 保持原有的 * 匹配  rDelimAst: /^(?:[^_*\\\\]|\\\\.)*?\\_\\_(?:[^_*\\\\]|\\\\.)*?\\*(?:[^_*\\\\]|\\\\.)*?(?=\\_\\_)|(?:[^*\\\\]|\\\\.)+(?=[^*])|[punct_](\\*+)(?=[\\s]|$)|(?:[^punct*_\\s\\\\]|\\\\.)(\\*+)(?=[punct_\\s]|$)|[punct_\\s](\\*+)(?=[^punct_\\s])|[\\s](\\*+)(?=[punct_])|[punct_](\\*+)(?=[punct_])|(?:[^punct*_\\s\\\\]|\\\\.)(\\*+)(?=[^punct*_\\s])/,  // 禁用对下划线的右界定符匹配  rDelimUnd: null}\r\n然而后面看到该文件开头有这样一句：\r\n/** * DO NOT EDIT THIS FILE * The code in this file is generated from files in ./src/ */\r\n好嘛，原来这个文件是通过其他源文件生成的，于是又转头更改nodes_modules/marked/src/rules.js。同上面的规则更改emStrong。\r\n然而，并无卵用。基本宣告这条路失败。\r\n\r\n2.手动escape（成功但丑陋）\r\n最简单的方法，博主最初的妥协。\r\n就是将公式中的_前面加上\\进行转义。这样的结果就是网站上能正常显示，然而用自己的markdown编辑软件，下标就会消失，变成真正的下横线。如果单纯只对网站更改，这个方法无疑是最方便的，但是但凡要在本地编辑，就会很难受。\r\n3.更换kramed引擎（没用）\r\n直接卸载源引擎，更换kramed：\r\nnpm uninstall hexo-renderer-marked --savenpm install hexo-renderer-pandoc --save\r\n然而kramed引擎还是存在相同的问题。（所以为什么要让我改啊！改了还容易报一堆错）\r\n还是需要修改node_modules/kramed/lib/rules/inline.js文件。\r\n将\r\nem: /^\\b_((?:__|[\\s\\S])+?)_\\b|^\\*((?:\\*\\*|[\\s\\S])+?)\\*(?!\\*)/,\r\n更改为\r\nem: /^\\*((?:\\*\\*|[\\s\\S])+?)\\*(?!\\*)/,\r\n将\r\nescape: replace(inline.escape)('])', '~|])')(),\r\n更改为\r\nescape: replace(inline.escape)('])', '~])')(),\r\n这不是和上面一样的吗，而且博主依然没找到这个文件。不知道是不是版本迭代更改了结构。\r\n解决方案\r\n最终的解决方案，是博主最开始最不愿意的方案，就是替换成pandoc引擎。\r\n\r\n安装Pandoc。\r\n在官网Pandoc -\r\nInstalling\r\npandoc下载安装pandoc，博主是在windows上部署的博客，直接下载了msi安装包进行安装，安装完成后重启电脑后才生效。\r\n更换引擎。\r\nnpm uninstall hexo-renderer-marked --savenpm install hexo-renderer-pandoc --save\r\nmathjax的安装和配置可参考网上的教程。\r\n重新构建。\r\nhexo clhexo d -g\r\n\r\n然后应该就能正常显示公式了，这下舒服了。不得不说，pandoc，你是我的神！\r\n","categories":["博客搭建"],"tags":["blog","mathjax","marked"]},{"title":"SplatSim 论文解读","url":"/2024/11/06/06-SplatSim/","content":"背景\r\nSim2Real是机器人技术中的一个核心问题，涉及将模拟环境中学到的控制策略转移到现实世界环境中。目前的方法基本都依赖于深度、触觉传感或点云输入等感知方式。相比之下，RGB\r\n图像很少用作机器人学习应用中的主要传感方式。优于 Sim2Real\r\n传输中的其他常用方式。它们捕捉关键的视觉细节，例如颜色、纹理、照明和表面反射率等，这对于理解复杂的环境至关重要。此外，RGB\r\n图像很容易在现实环境中使用相机获取，并且与人类感知紧密结合，使其非常适合解释动态和复杂场景中的复杂细节。\r\n为什么很难将使用 RGB\r\n信息进行模拟训练的策略部署到现实世界呢？是因为机器人在模拟器中观察到的图像分布与它在现实世界中看到的图像分布有很大不同。本文提出了一种新颖的方法来减少\r\nRGB 图像的 Sim2Real 差距。利用 Gaussian Splatting\r\n作为照片级真实感渲染，使用现有模拟器作为物理主干。利用 Gaussian\r\nSplatting\r\n作为主要渲染基元，取代现有模拟器中传统的基于网格的表示，以显著提高渲染场景的照片真实感。\r\n方法\r\n\r\n关键前提：准确分割现实场景中高斯分布表示的每个刚体，并识别其相对于模拟器的相应的齐次变换。那么就可以渲染新姿势下的刚体。\r\n底层表示：不使用网格图元，而是使用高斯图作为底层表示。\r\n\r\nA. 问题描述\r\n\r\n表示真实场景的Gaussian Splat。 \r\n表示场景中第k个object的Gaussian Splat。\r\n目标是为任何模拟器中的机器人，使用  来生成真实的渲染 \r\n。然后在这样的表示下，收集专家的演示 ​​ 来用于训练基于RGB的策略。\r\n专家  生成由状态-动作对组成的轨迹\r\n 。每个时间步  的状态定义为 \r\n。其中 \r\n代表机器人的关节角度。 表示第  个 object 的位置  和方向  。对应的动作  指末端执行器的位置\r\n 和方向\r\n 。\r\n渲染图  ，由模拟状态\r\n 推导而来，作为输入训练策略\r\n\r\n。测试时，该策略只依赖于现实世界的RGB图像  。\r\nB. 坐标系定义和变换\r\n：真实世界坐标系 -\r\n主要参考系。\r\n：真实世界机器人坐标系。\r\n：模拟器坐标系。\r\n 和  都会与 \r\n进行对齐，以确保模拟器中机器人底座和现实世界共享相同的坐标系。\r\nC. 机器人 Splat 模型\r\n\r\n首先创建场景的高斯分布（其中机器人位于其原始位置），在静态场景中对机器人进行可视化。使用\r\nICP\r\n算法手动分割机器人的点云并与标准机器人框架对齐。然后对每个机器人关节进行分段，并应用正向运动学变换，从而能够以任意关节配置渲染机器人。\r\nD. Object Splat模型\r\n与机器人渲染类似，使用 ICP 来对齐每个对象的 3D 高斯 ​​\r\n到其模拟的真实点云。 \r\nE. 连接的物体\r\n\r\n虽然 CAD\r\n轴对齐的边界框允许对机器人连杆进行直接分割，但某些物体（例如平行钳口夹具）由于与标准轴未对准而带来了挑战，也就是说，仅使用边界框无法将夹具连杆整齐地分割出来。文章使用基于\r\nKNN 的分类器对平行颚式夹具等铰接物体的链接进行分段。\r\nF. 使用 SplatSim\r\n渲染模拟轨迹\r\n既然已经能够在场景中渲染单个刚体，那么可以用它来表示任何模拟轨迹\r\n\r\n。用这些基于状态的转换和C、D里提到的方法，获得演示，从而让策略从 \r\n进行学习。\r\nG. 策略训练和部署\r\n为了在模拟器中从生成的演示 ​ 中学习\r\n，采用扩散策略。尽管论文的方法显著缩小了 Sim2Real\r\n视觉上的差距，但模拟环境和现实环境之间的差异仍然存在。例如，模拟场景缺乏阴影，刚体的假设会导致机器人电缆等可动部件的渲染不当。为了解决这些问题，论文在策略训练期间结合了图像增强，其中包括添加高斯噪声、随机擦除以及调整图像的亮度和对比度。这些增强显著增强了策略的稳健性并提高了其在实际部署过程中的性能。\r\n实验\r\nA. 现实世界和模拟中的演示\r\n\r\n在现实世界中，每项任务的演示都是由人类专家手动收集的。\r\n相比之下，模拟器通过采用基于特权信息的运动规划器简化了这一过程，运动规划器利用特权信息自动生成数据，例如场景中每个刚体的位置和方向。\r\n在有人类专家参与的情况下，模拟器不仅能在演示之间自动重置，从而减少工作量，更重要的是，它利用运动规划器，完全消除了人类干预的需要。这样，只需极少的人工输入，就能生成大规模、高质量的演示数据集。\r\n因此，模拟器大大减少了数据收集所需的时间和精力。 如表 I\r\n所示，在现实世界中收集演示数据需要约 20.5 个小时，而在模拟器中只需 3\r\n个小时就能完成同样的任务，这充分体现了方法的高效性和可扩展性。\r\nB. 零样本策略部署结果\r\n\r\n以任务成功率为主要指标，评估了策略在四个接触丰富的真实世界任务中的部署情况。\r\n如表 I 所示，方法实现了 86.25% 的 Sim2Real\r\n传输平均成功率，而直接在真实世界数据上训练的策略成功率为\r\n97.5%，这凸显了方法的有效性。 所有实验均使用配备了 Robotiq 2F-85 抓手和\r\n2 个英特尔 Realsense D455 摄像头的 UR5 机器人，并在英伟达 RTX 3080Ti GPU\r\n上部署了扩散策略。\r\n\r\nT-Push 任务： T-Push 任务由 Diffusion\r\nPolicy推广，可捕捉非触觉操作的动态，其中涉及控制物体移动和接触力。\r\n在训练中，人类专家使用 Gello teleoper-ation，在模拟中收集了 160 次演示。\r\n测试时，机器人从随机位置出发，在零样本 Sim2Real 传输中取得了 90%\r\n的成功率（36/40 次试验），如表 I 所示。\r\n这一结果表明，框架在真实世界的演示中无需微调就能有效处理推动的动态过程。\r\n此外，方法与 Real2Real（40/40）和 Sim2Sim（40/40）的性能相当。\r\nPick-Up-Apple 任务： Pick-Up-Apple\r\n任务涉及在三维空间中抓取和操纵物体的完整姿态（即位置和方向）。\r\n该任务旨在评估机器人在使用论文的模拟渲染场景进行训练时的抓取能力。\r\n运动规划器利用模拟器中的特权状态信息（场景中每个刚体的准确位置和方向），生成了\r\n400 个具有随机末端执行器位置和方向的演示。 如表 I\r\n所示，在真实世界的试验中，策略在零样本 Sim2Real 传输中取得了 95%\r\n的成功率（38/40 次试验）。\r\nOrange on Plate 任务：\r\n在这项任务中，机器人必须捡起一个橘子并将其放在盘子里。\r\n在模拟过程中，运动规划器获取了特权信息，并生成了 400 次演示。\r\n在训练过程中，末端执行器的位置和初始抓手状态是随机的。\r\n测试期间，机器人总是从原点开始。 论文在 Sim2Real 的零点转移中取得了 90%\r\n的成功率（36/40 次试验）。\r\nAssembly 任务：\r\n在这项任务中，机器人必须将一个长方体块放在另一个长方体块的顶部。\r\n机器人从原点开始抓取绿色立方体，并将其放到红色立方体的顶部。\r\n这项任务特别艰巨，因为机器人必须精确放置，否则立方体就会掉落，导致失败。\r\n论文的 Sim2Real 策略在这项任务中的表现为 70%（28/40 次试验），而 Sim2Sim\r\n的表现为 95%，Real2Real 的表现为 90%。\r\n\r\nC. 量化机器人渲染\r\n通过与真实世界的图像进行比较，论文定量评估了在不同关节配置下渲染的机器人图像的准确性。\r\n论文评估了 300 个不同机器人关节角度下的机器人渲染质量。\r\n为了衡量渲染图像与真实世界图像之间的相似性，论文采用了图像渲染评估中常用的两个指标：\r\n峰值信噪比（PSNR）和结构相似性指数（SSIM）。\r\n尽管关节配置各不相同，但渲染图像的平均 PSNR 和 SSIM 分别达到了 22.62 和\r\n0.7845，表明模拟图像非常接近真实世界 RGB 观察图像的视觉质量。\r\nD. 数据增强的效果\r\n为了量化数据增强对策略在模拟与真实环境中性能的影响，论文对经过训练的策略进行了有增强和无增强的对比实验。\r\n虽然在一致的环境（如 Sim2Sim 或 Real2Real\r\n场景）中，扩散策略在没有增强的情况下也能有效执行，但将在模拟环境中训练的策略转移到真实世界时，由于渲染无法捕捉动态细节（如不断变化的反射和阴影），因此会引入领域偏移，从而需要额外的鲁棒性。\r\n论文在训练过程中加入了随机噪音添加、色彩抖动和随机擦除等增强功能，以应对这些变化。\r\n在 B 节的四项任务中，这些增强措施将该策略的性能从 21% 提高到\r\n86.25%。\r\n结论\r\n在这项工作中，论文利用高斯泼溅技术（Gaussian\r\nSplatting）作为一种逼真的渲染技术，并与现有的基于物理交互的模拟器集成，从而缩小了基于\r\nRGB 的操作策略的模拟与真实之间的差距。\r\n论文的框架实现了在模拟中训练好的基于 RGB\r\n的操作策略到真实环境中的零样本转移。\r\n虽然论文的框架推动了当前最先进技术的发展，但它仍局限于刚体操纵，无法处理布、液体或植物等复杂物体。\r\n未来计划将现有的框架与基于强化学习的方法相结合，以获得更多动态技能。还将进一步改进系统，以便在现实世界中训练和部署机器人执行高度复杂和接触丰富的任务。\r\n","categories":["三维重建"],"tags":["3DGS","Sim2Real"]},{"title":"主流三维重建方法对比","url":"/2024/11/18/09-Reconstruction/","content":"MeshAnything\r\n\r\n问题背景\r\n当前网格提取方法生成的网格明显不如艺术家创建的网格 (AM,\r\nArtist-Created\r\nMeshes)，即由人类艺术家创建的网格。具体来说，当前的网格提取方法依赖于密集的面并忽略几何特征，导致效率低下、后处理复杂且表示质量较低等问题。为了解决这些问题，论文引入了\r\nMeshAnything\r\n，该模型将网格提取视为生成问题，生成与指定形状对齐的 AM。\r\n\r\n上图是对真实形状使用Marching Cubes和MeshAnything，然后对不同voxel size的Marching Cubes进行remesh的结果。现有方法以重构方式提取网格，忽略了对象的几何特征并产生拓扑较差的密集网格。这些方法从根本上无法捕捉锐利的边缘和平坦的表面，如放大图所示。\r\n核心方法\r\n\r\n重新定义问题：\r\n\r\n将网格提取看作一个生成任务，而非重建任务。\r\n目标是生成与给定形状对齐的高效拓扑网格（AMs），使其更接近人工设计。\r\n\r\n模型架构：\r\n\r\n使用 VQ-VAE 学习网格的“词汇”（网格编码）。\r\n基于“词汇”训练一个仅解码自回归Transformer，以形状条件为输入，生成网格。\r\n增强了\r\nVQ-VAE解码器，使其抗噪，能够处理低质量的令牌序列，并利用形状信息辅助解码。\r\n\r\n形状条件的设计：\r\n\r\n选择点云作为输入形状条件，因其具有连续表示性且易于编码。\r\n在训练时通过对真实网格进行降质（如使用Marching\r\nCubes生成粗网格）来缩小训练与推理的域差距。\r\n\r\n优化训练：\r\n\r\n在VQ-VAE训练完成后，额外细调解码器，模拟推理中可能生成的低质量令牌序列，并提高对噪声的鲁棒性。\r\n\r\n实验结果\r\n\r\n生成效率： MeshAnything\r\n生成的网格面数比传统方法减少数百倍，同时保留较高的形状对齐质量。\r\n拓扑质量：\r\n在用户研究中，MeshAnything的形状和拓扑质量显著优于现有方法，表明其生成的网格更接近艺术家手工制作的标准。\r\n泛化能力：\r\n能处理来自各种3D表示（如点云、体素、NeRF等）的输入，支持广泛的3D资产生产管线。\r\n\r\n存在问题\r\n第一眼见这个工作的时候，有被震撼到。以为这个工作将多种三维重建方式整合起来了。从它的演示中，看见它可以从三维高斯、图像、文本直接生成几何形状完整但面数较精简的网格。然而自己部署运行之后发现好像并不是这样。\r\n在查阅代码仓库的readme时，发现所给的示例代码全是从点云或者网格进行转化的代码。正疑惑如何将其他表达方式进行转化，于是看了下issues：\r\n\r\n关于3D Gaussian转为mesh：\r\nAbout 3D\r\nGaussians · Issue #29 · buaacyw/MeshAnything\r\n\r\nWe suggest converting the 3D Gaussians into a mesh before inputting\r\nthem into our method.\r\n\r\n作者直接建议将3D Gaussian转为mesh再作为该方法的输入。（迷惑）\r\nHow to\r\nconvert 3D gaussian splatting to mesh? · Issue #6 ·\r\nbuaacyw/MeshAnything\r\n\r\nHi, I use SUGAR to get\r\nhigh quality mesh from GS and then sample point cloud on the 3D GS.\r\nOther methods like 2DGS should\r\nalso work well. I suggest you run these methods directly in their repos\r\nand use the results as inputs for our method.\r\n\r\n另一个issue里，作者提到了两个方法来进行这种转化：SuGaR和2DGS。\r\n关于大规模点云转mesh：\r\nlarge\r\nscale pointCloud · Issue #9 · buaacyw/MeshAnything\r\nMeshAnything目前限制在800个face，对复杂mesh或大规模点云可能无法比较好地转化。\r\n关于image和text转mesh：\r\nHow to do\r\ntext and images? · Issue #4 · buaacyw/MeshAnything\r\n\r\nThe image/text to mesh is achieved by combining with 3D generation\r\nmethods. We first obtain dense meshes from 3D generation methods and use\r\nthem as input to our methods. Note that the shape quality of dense\r\nmeshes should be high enough. Thus, feed-forward 3D generation methods\r\nmay often produce bad results due to insufficient shape quality. We\r\nsuggest using results from SDS-based pipelines (like DreamCraft3D) as\r\nthe input of MeshAnything as they produce better shape quality.\r\n\r\n意思就是，通过其他方法生成dense mesh，然后再作为该方法的输入呗？\r\n\r\n大概总结一下就是，这个方法只是将已有的Mesh转化为Artist-Created Meshes而已，包括之前提到的3DGS、image、text转mesh，根本就是通过别的方法实现的。它是将精度高的密mesh在保留大部分几何特征的情况下进行的“简化”和“优化”，让面片数更精简且拓扑结构更符合人类直觉。\r\n（吐槽一句，我感觉这个标题还有最开始的演示，有种标题党的意味。还以为真的是mesh\r\nanything）\r\nSuGaR\r\n\r\n问题背景\r\n3D Gaussian Splatting 是一种高效的 3D\r\n场景渲染方法，但其优化后生成的大量无结构的 3D Gaussians\r\n不便于生成可编辑的网格（Mesh）。\r\n当前使用 Neural SDF\r\n提取网格的方法计算量大，需多GPU训练，且耗时较长（通常需24小时以上）。\r\n核心方法\r\n表面对齐正则化：\r\n\r\n引入一个正则化项，强制优化后的 3D Gaussians\r\n分布于场景表面，从而更好地捕获场景几何信息。\r\n通过优化 Signed Distance Function (SDF)，使 Gaussians\r\n平坦化，并沿着表面分布。\r\n\r\n高效网格提取：\r\n\r\n基于优化后的 Gaussians，采样水平集上的点，使用 Poisson\r\n重建算法生成三角形网格。相比于 Marching Cubes\r\n算法，该方法对稀疏数据更鲁棒，且计算更高效。\r\n\r\n绑定 Gaussians 到网格：\r\n\r\n将优化后的 Gaussians 与生成的网格表面绑定，进一步优化 Gaussians\r\n和网格，通过 Gaussian Splatting 渲染，提升渲染质量。\r\n这种绑定使得可以通过编辑网格来实现对场景的修改。\r\n\r\n实验结果\r\n\r\n速度：SuGaR 方法在单 GPU\r\n上仅需几分钟即可提取出高质量网格，远快于基于 SDF 的方法。\r\n质量：在多个数据集上的定量评估显示，SuGaR\r\n的渲染质量（PSNR、SSIM、LPIPS）优于其他基于网格的方法，并与仅关注渲染质量的顶尖方法接近。\r\n可编辑性：生成的网格与绑定的 Gaussians\r\n可以直接用于动画、重光照、雕刻等操作，提供了更高的灵活性。\r\n\r\n踩坑记录\r\n\r\nenvironment.yml 创建环境\r\nfail：创建空环境一步一步安装依赖。\r\npytorch找不到包：使用国内镜像（需要注意是使用-f而不是--index-url）：\r\npip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 -f https://mirrors.aliyun.com/pytorch-wheels/cu118\r\npytorch3d找不到包：去Anaconda官网找到对应版本的安装包复制链接再install：\r\nconda install https://anaconda.org/pytorch3d/pytorch3d/0.7.4/download/linux-64/pytorch3d-0.7.4-py39_cu118_pyt201.tar.bz2\r\n其他 conda 包找不到：直接pip\r\ninsatll。（换了国内源都没解决掉，只能出此下策）\r\nCUDA-capable device(s) is/are busy or\r\nunavailable：不会解决了，换回AutoDL了。在AutoDL配环境没遇到这么多问题。\r\n卡在 'computing mesh...' : 在 issue\r\n看到了一模一样的情况，目前还没解决这个问题。\r\n\r\n2DGS\r\n问题背景\r\n在计算机图形学和视觉领域，视角一致的几何重建与真实感的新视角合成（NVS）一直是重要的研究目标。近年来，基于\r\n3D Gaussian Splatting (3DGS)\r\n的显式方法因其实时渲染能力而受到关注。然而，3DGS\r\n在以下方面存在显著问题：\r\n\r\n表面几何不精确：3D高斯的体积表示与物体的薄表面特性冲突，导致表面重建质量较低。\r\n多视角不一致性：3DGS\r\n在不同视角下计算交点时，由于投影平面变化，产生深度和法线的不一致性。\r\n\r\n为了解决这些问题，本文提出了一种新的显式建模方法——2D Gaussian\r\nSplatting\r\n(2DGS)，以提高几何重建的精度和视角一致性，同时保留快速渲染能力。\r\n核心方法\r\n\r\n方法概述\r\n\r\n\r\n通过二维高斯平面（即二维椭圆盘）来替代三维高斯球体，将场景建模为一组二维高斯分布的集合。\r\n这些二维高斯盘通过显式的光线与平面交点计算，实现几何表面和光照的高效表示。\r\n结合梯度优化，从稀疏点云和多视角图像中同时优化外观与几何。\r\n\r\n\r\n关键技术\r\n\r\n\r\n透视准确的二维高斯渲染：利用光线与二维高斯交点的显式计算，避免传统近似方法带来的透视误差。\r\n正则化损失：\r\n\r\n深度失真损失：通过约束二维高斯沿光线分布的紧密性，确保几何表面稳定。\r\n法线一致性损失：约束二维高斯表面的法线方向与深度图梯度对齐，保证表面平滑性。\r\n\r\n高效实现：基于CUDA的自定义内核加速训练和实时渲染。\r\n\r\n\r\n优化流程\r\n\r\n\r\n初始化稀疏点云并结合多视角RGB图像，优化二维高斯参数（位置、缩放、方向）。\r\n使用正则化损失减少噪声，增强几何表面和视角一致性。\r\n\r\n实验结果\r\n\r\n实验设置\r\n\r\n\r\n数据集：DTU、Tanks and Temples 和\r\nMip-NeRF360，涵盖不同场景和分辨率。\r\n比较方法：隐式表征方法（如NeRF、NeuS）与显式方法（如3DGS、SuGaR）。\r\n\r\n\r\n主要实验结果\r\n\r\n\r\n几何重建：\r\n\r\n在DTU数据集上，Chamfer距离优于现有方法，特别是在细节捕捉和噪声消除方面。\r\n比隐式方法快100倍的训练速度，比其他显式方法（如SuGaR）快3倍以上。\r\n\r\n新视角合成：\r\n\r\n在Mip-NeRF360数据集上，新视角渲染质量接近3DGS，且同时提供了更高的几何精度。\r\n\r\n消融实验：\r\n\r\n验证了正则化项的有效性：缺失法线一致性会导致表面方向噪声，缺失深度失真损失会导致表面模糊。\r\n\r\n\r\n\r\n定量与定性表现\r\n\r\n\r\n定量结果：在DTU数据集上，2DGS的Chamfer距离和PSNR均达到当前最优，同时显著减少了训练时间和模型存储需求。\r\n定性结果：相比3DGS和SuGaR，2DGS在复杂几何和细节捕捉上表现更优，能更好地重建噪声较少的表面。\r\n\r\n重建效果\r\n\r\n上图是训练30000个iteration后生成的mesh。可以看出2DGS生成的mesh还是比较精确的。\r\nGOF\r\n未完待续...\r\n","categories":["三维重建"],"tags":["NeRF","3DGS","2DGS"]},{"title":"NeRF Studio简要教程","url":"/2024/11/12/08-NeRF_Studio/","content":"准备工作\r\n安装NeRF Studio\r\n官方仓库\r\n写的教程已经很详尽了。\r\ngit clone https://github.com/nerfstudio-project/nerfstudio.gitcd nerfstudiopip install --upgrade pip setuptoolspip install -e .\r\n值得注意的是，open3d库只支持python 3.8-3.11，博主是用python 3.10安装的依赖。后面租了个服务器用python 3.12，结果找不到相应版本的open3d，建议还是按推荐配置来。\r\n安装tiny-cuda-nn\r\n在训练过程中，终端出现了如下的warning：\r\nWARNING: Using a slow implementation for the SHEncoding module. 🏃 🏃 Install tcnn for speedups 🏃 🏃pip install git+https://github.com/NVlabs/tiny-cuda-nn/#subdirectory=bindings/torchWARNING: Using a slow implementation for the NeRFEncoding module. 🏃 🏃 Install tcnn for speedups 🏃 🏃pip install git+https://github.com/NVlabs/tiny-cuda-nn/#subdirectory=bindings/torchWARNING: Using a slow implementation for the MLPWithHashEncoding module. 🏃 🏃 Install tcnn for speedups 🏃 🏃pip install git+https://github.com/NVlabs/tiny-cuda-nn/#subdirectory=bindings/torchWARNING: Using a slow implementation for the MLP module. 🏃 🏃 Install tcnn for speedups 🏃 🏃pip install git+https://github.com/NVlabs/tiny-cuda-nn/#subdirectory=bindings/torchWARNING: Using a slow implementation for the HashEncoding module. 🏃 🏃 Install tcnn for speedups 🏃 🏃pip install git+https://github.com/NVlabs/tiny-cuda-nn/#subdirectory=bindings/torch\r\n提示你可以用tcnn进行加速。根据它的提示输入指令：\r\npip install git+https://github.com/NVlabs/tiny-cuda-nn/#subdirectory=bindings/torch\r\n不出意外的话就要出意外了：\r\n\r\n先是查看了下文档，说是要求 g++ &lt; 11 ，于是安装了g++-9：\r\nsudo apt install g++-9\r\n然后切换版本：\r\nsudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-9 20\r\n如果有多个版本好像还得执行以下指令切换： sudo update-alternatives --config g++\r\n发现还是不行，看报错里有这样一句：\r\n\r\n说明问题出在lcuda，g++找不到lcuda。因为博主使用的WSL，cuda库存放在/usr/lib/wsl/lib中，将它复制出来即可：\r\nsudo cp /usr/lib/wsl/lib/* /usr/lib\r\n然后再次执行安装就成功了。\r\n值得注意的是，在安装tcnn之前，博主用nerfacto训练30000个step用了两小时，而安装之后仅需20分钟，这个提升还是蛮可观的。\r\n报错及解决方案\r\n使用splatfacto训练报错\r\n1. No CUDA toolkit found.\r\n在使用splatfacto进行训练时报错：\r\n\r\n显示CUDA Tookit找不到，然而我的用户目录里是有的。\r\n在github的issue里找到了解决方案：No CUDA\r\ntoolkit found. gsplat will be disabled. · Issue #249 ·\r\nnerfstudio-project/gsplat\r\n即，将path添加进去：\r\nexport PATH=/usr/local/cuda-12.6/bin$&#123;PATH:+:$&#123;PATH&#125;&#125;\r\n可以将这句添加到~/.bashrc里，每次打开terminal就不用再输入一遍了。\r\n2. ninja: build stopped:\r\nsubcommand failed.\r\n解决上个问题后结果还是报错：\r\n\r\n查阅发现是内存不够，进程直接被kill了。自己的WSL虚拟机内存太少了。尝试租服务器，解决。\r\n使用nerfbusters训练报错（未完全解决）\r\n1.\r\nModuleNotFoundError: No module named\r\n'nerfstudio.fields.visibility_field'\r\n在使用nerfbusters方法时，根据文档中的教程安装nerfbuster之后，简单的使用--help也会出现如下的报错：\r\nModuleNotFoundError: No module named &#x27;nerfstudio.fields.visibility_field&#x27;\r\n切换其他基于NeRF的方法，有的依然会出现这个报错。\r\n然后在issue中找到了相似的情况：\r\nWhere's\r\nnerfstudio VisibilityFIeld come from? · Issue #17 ·\r\nethanweber/nerfbusters\r\nfrom\r\nnerfstudio.fields.visibility_field import VisibilityField\r\nModuleNotFoundError: No module named\r\n'nerfstudio.fields.visibility_field' · Issue #3185 ·\r\nnerfstudio-project/nerfstudio\r\nVisibility\r\nField from Nerfbusters by ethanweber · Pull Request #2264 ·\r\nnerfstudio-project/nerfstudio\r\n其中提到，他们当前使用的branch是nerfbusters-changes，并没有计划把他合并到main\r\nbranch。\r\n所以需要克隆他们的nerfbusters-changes branch：\r\ngit clone -b nerfbusters-changes https://github.com/nerfstudio-project/nerfstudio.git\r\n然后在根目录执行安装：\r\npip install -e .\r\n这样就可以了。\r\n2. `numpy` has\r\nno attribute `bool8`. Did you mean: `bool`?\r\n这是因为numpy在1.24更新后将bool8更名为了bool，降级numpy版本即可：\r\npip install numpy==1.23\r\n3. The viewer\r\nbridge server subprocess failed.\r\n\r\n切换分支后运行原有的方法都会出现如下报错。说是viewer的服务启动失败了，通过--viewer.websocket-port更改窗口依然是相同的报错，于是根据提示查看了log：\r\n\r\n好嘛，给我原来的module搞没了，我又回原来的分支重新pip install -e .，然后再运行。\r\n\r\n闹鬼了，我不玩了行吧，nerfbusters给劳资滚！😠\r\n使用zipnerf报错\r\n1.\r\nERROR: Failed building wheel for cuda_backend 或者 No module named\r\n'_cuda_backend'\r\n在使用\r\npip install git+https://github.com/SuLvXiangXin/zipnerf-pytorch#subdirectory=extensions/cuda\r\n安装依赖的时候，出现如下报错：\r\n\r\n开始没有管他，直到最后训练的时候又弹出报错：\r\n\r\n\r\nimage-20241126233117488\r\n\r\n看样子是逃不掉了。\r\n2.\r\nAssertionError: pipeline.datamanager.dataparser...\r\n\r\n第一次出现这长串报错是因为参数没输对，第二次问gpt说是与\r\ntyro 版本有关，尝试升级 tyro\r\n库以及相关依赖：\r\npip install --upgrade tyro\r\n然后看到error怂了：\r\n\r\n于是又改回了推荐的版本：\r\npip install tyro==2.13.3\r\n然后再运行就可以了。\r\n3.\r\nAssertionError: Colmap path data/processed_truck/sparse/0 does not\r\nexist.\r\n这个原因是zipnerf用的数据集格式和nerfstudio好像不完全一致，我用了下tandt的数据集发现可行，但是它会对图像先进行一次下采样。\r\n(nerf2mesh) root@I1dc2923c3f00801cdf:~/3D-Reconstruction/nerf2mesh# python main.py nerfstudio/poster/ --workspace trial_syn_poster/ -O --bound 1 --scale 0.8 --dt_gamma 0 --stage 0 --lambda_tv 1e-8Warning:Unable to load the following plugins:        libio_e57.so: libio_e57.so does not seem to be a Qt Plugin.Cannot load library /usr/local/miniconda3/envs/nerf2mesh/lib/python3.10/site-packages/pymeshlab/lib/plugins/libio_e57.so: (/usr/lib/x86_64-linux-gnu/libp11-kit.so.0: undefined symbol: ffi_type_pointer, version LIBFFI_BASE_7.0)Loading train data: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 225/225 [00:08&lt;00:00, 25.09it/s][INFO] max_epoch 134, eval every 26, save every 2.[INFO] Trainer: ngp_stage0 | 2024-11-26_15-12-23 | cuda | fp16 | trial_syn_poster/[INFO] #parameters: 18367240Namespace(path=&#x27;nerfstudio/poster/&#x27;, O=True, workspace=&#x27;trial_syn_poster/&#x27;, seed=0, stage=0, ckpt=&#x27;latest&#x27;, fp16=True, sdf=False, tcnn=False, progressive_level=False, test=False, test_no_video=False, test_no_mesh=False, camera_traj=&#x27;&#x27;, data_format=&#x27;nerf&#x27;, train_split=&#x27;train&#x27;, preload=True, random_image_batch=True, downscale=1, bound=1.0, scale=0.8, offset=[0, 0, 0], mesh=&#x27;&#x27;, enable_cam_near_far=False, enable_cam_center=False, min_near=0.05, enable_sparse_depth=False, enable_dense_depth=False, iters=30000, lr=0.01, lr_vert=0.0001, pos_gradient_boost=1, cuda_ray=True, max_steps=1024, update_extra_interval=16, max_ray_batch=4096, grid_size=128, mark_untrained=True, dt_gamma=0.0, density_thresh=10, diffuse_step=1000, diffuse_only=False, background=&#x27;random&#x27;, enable_offset_nerf_grad=False, n_eval=5, n_ckpt=50, num_rays=4096, adaptive_num_rays=True, num_points=262144, lambda_density=0, lambda_entropy=0, lambda_tv=1e-08, lambda_depth=0.1, lambda_specular=1e-05, lambda_eikonal=0.1, lambda_rgb=1, lambda_mask=0.1, wo_smooth=False, lambda_lpips=0, lambda_offsets=0.1, lambda_lap=0.001, lambda_normal=0, lambda_edgelen=0, contract=False, patch_size=1, trainable_density_grid=False, color_space=&#x27;srgb&#x27;, ind_dim=0, ind_num=500, mcubes_reso=512, env_reso=256, decimate_target=300000.0, mesh_visibility_culling=True, visibility_mask_dilation=5, clean_min_f=8, clean_min_d=5, ssaa=2, texture_size=4096, refine=True, refine_steps_ratio=[0.1, 0.2, 0.3, 0.4, 0.5, 0.7], refine_size=0.01, refine_decimate_ratio=0.1, refine_remesh_size=0.02, vis_pose=False, gui=False, W=1000, H=1000, radius=5, fovy=50, max_spp=1, refine_steps=[3000, 6000, 9000, 12000, 15000, 21000])NeRFNetwork(  (encoder): GridEncoder: input_dim=3 num_levels=16 level_dim=1 resolution=16 -&gt; 2048 per_level_scale=1.3819 params=(6119864, 1) gridtype=hash align_corners=False interpolation=linear  (sigma_net): MLP(    (net): ModuleList(      (0): Linear(in_features=19, out_features=32, bias=False)      (1): Linear(in_features=32, out_features=1, bias=False)    )  )  (encoder_color): GridEncoder: input_dim=3 num_levels=16 level_dim=2 resolution=16 -&gt; 2048 per_level_scale=1.3819 params=(6119864, 2) gridtype=hash align_corners=False interpolation=linear  (color_net): MLP(    (net): ModuleList(      (0): Linear(in_features=35, out_features=64, bias=False)      (1): Linear(in_features=64, out_features=64, bias=False)      (2): Linear(in_features=64, out_features=6, bias=False)    )  )  (specular_net): MLP(    (net): ModuleList(      (0): Linear(in_features=6, out_features=32, bias=False)      (1): Linear(in_features=32, out_features=3, bias=False)    )  ))[INFO] Loading latest checkpoint ...[WARN] No checkpoint found, abort loading latest model.Loading val data: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00&lt;00:00, 21.10it/s][mark untrained grid] 9364 from 2097152==&gt; Start Training Epoch 1, lr=0.000100 ...... ...loss=0.091703 (0.104557) lr=0.004555: : 100% 225/225 [00:03&lt;00:00, 69.94it/s]==&gt; Finished Epoch 133, loss = 0.078408.==&gt; Start Training Epoch 134, lr=0.001006 ...loss=0.075468 (0.078282) lr=0.000988: : 100% 225/225 [00:04&lt;00:00, 52.46it/s]==&gt; Finished Epoch 134, loss = 0.078282.[INFO] training takes 10.078925 minutes.Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]/usr/local/miniconda3/envs/nerf2mesh/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter &#x27;pretrained&#x27; is deprecated since 0.13 and may be removed in the future, please use &#x27;weights&#x27; instead.  warnings.warn(/usr/local/miniconda3/envs/nerf2mesh/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for &#x27;weights&#x27; are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.  warnings.warn(msg)Downloading: &quot;https://download.pytorch.org/models/vgg16-397923af.pth&quot; to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 528M/528M [35:11&lt;00:00, 262kB/s]Loading model from: /usr/local/miniconda3/envs/nerf2mesh/lib/python3.10/site-packages/lpips/weights/v0.1/vgg.pth++&gt; Evaluate at epoch 134 ...loss=0.213495 (0.213495): : 100% 1/1 [00:00&lt;00:00,  1.07it/s]PSNR = 6.706122LPIPS (vgg) = 0.767062++&gt; Evaluate epoch 134 Finished, loss = 0.213495==&gt; Start Test, save results to trial_syn_poster/results100% 11/11 [00:03&lt;00:00,  3.40it/s]Traceback (most recent call last):  File &quot;/root/3D-Reconstruction/nerf2mesh/main.py&quot;, line 263, in &lt;module&gt;    trainer.test(test_loader, write_video=True) # test and save video  File &quot;/root/3D-Reconstruction/nerf2mesh/nerf/utils.py&quot;, line 1005, in test    imageio.mimwrite(os.path.join(save_path, f&#x27;&#123;name&#125;_rgb.mp4&#x27;), all_preds, fps=24, quality=8, macro_block_size=1)  File &quot;/usr/local/miniconda3/envs/nerf2mesh/lib/python3.10/site-packages/imageio/v2.py&quot;, line 494, in mimwrite    with imopen(uri, &quot;wI&quot;, **imopen_args) as file:  File &quot;/usr/local/miniconda3/envs/nerf2mesh/lib/python3.10/site-packages/imageio/core/imopen.py&quot;, line 281, in imopen    raise err_type(err_msg)ValueError: Could not find a backend to open `trial_syn_poster/results/ngp_stage0_ep0134_rgb.mp4`` with iomode `wI`.Based on the extension, the following plugins might add capable backends:  FFMPEG:  pip install imageio[ffmpeg]  pyav:  pip install imageio[pyav]100% 11/11 [00:04&lt;00:00,  2.67it/s]\r\n运行结果\r\nnerfacto和splatfacto渲染效果对比\r\n由于背景场景太过杂乱，在导出的时候选择了crop一下，只保留了主体。\r\n对于基于NeRF的方法nerfacto，可以选择导出点云或者网格。导出的网格在meshlab进行可视化，效果如下：\r\n\r\n可以看到，poster的内容是比较清晰地还原出来了，然而椅子的形状结构却有些损坏，尤其是越靠近中心缺损越严重。\r\n而对于基于3DGS的方法splatfacto，它只能选择导出gaussian\r\nsplat。为了对其进行可视化，选择使用了 supersplat\r\n这个工具：\r\n\r\n可以看到，除了中心的主体外，为了渲染出背景，在即便是很远的地方也生成了很多个gaussian\r\nsplat。\r\n\r\n将摄像头拉近到主体，发现这个重构效果还是蛮好的。然而如何将gaussian\r\nsplat转化为网格形式，这是后续工作的重点。\r\nMesh导出\r\n在NeRF Studio中可以对训练结果进行可视化的导出。支持3D gaussian,\r\npoint cloud和mesh\r\n，但是对于不同的方法，导出的格式不同。如基于NeRF的方法只能导出点云和网格，而基于3DGS的方法只能导出三维高斯。\r\n训练代码示例：\r\nns-train nerfacto --data data/processed_truck/ --output-dir outputs/truck_100000 --max-num-iterations 100000\r\n可以先对训练结果进行可视化：\r\nns-viewer --load-config path/to/your/trainresult/config.yml\r\n然后在网页的export栏可以选择导出参数，并复制导出指令，以possion的导出为例：\r\nns-export poisson --load-config outputs/truck_100000/processed_truck/nerfacto/2024-11-26_142623/config.yml --output-dir exports/mesh/truck_100000iter_50000face --target-num-faces 50000 --num-pixels-per-side 2048 --num-points 1000000 --remove-outliers True --normal-method open3d --obb_center 0.0000000000 0.0000000000 0.0000000000 --obb_rotation 0.0000000000 0.0000000000 0.0000000000 --obb_scale 1.0000000000 1.0000000000 1.0000000000\r\n还可以选择其他的导出方式，如tsdf：\r\nns-export tsdf --load-config outputs/truck_100000/processed_truck/nerfacto/2024-11-26_142623/config.yml --output-dir exports/mesh/truck_100000iter_50000face_tsdf --target-num-faces 50000 --num-pixels-per-side 2048\r\n或者marching-cubes：\r\nns-export marching-cubes --load-config outputs/truck_100000/processed_truck/nerfacto/2024-11-26_142623/config.yml --output-dir exports/mesh/truck_100000iter_50000face_tsdf --target-num-faces 50000 --num-pixels-per-side 2048\r\n（好像nerfacto的结果不能用marching-cubes）\r\n下面直观展示下导出mesh的效果（前四种是nerfacto，最后一个是用2DGS方法生成的结果）：\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\npossion (30k iters, 50k faces)\r\n\r\n\r\n\r\n\r\n\r\npossion (100k iters, 50k\r\nfaces)\r\n\r\n\r\n\r\npossion (100k iters, 100k\r\nfaces)\r\n\r\n\r\n\r\ntsdf (100k iters, 50k\r\nfaces)\r\n\r\n\r\n\r\n2DGS (2m faces)\r\n\r\n\r\n\r\n\r\n可以看到，tsdf提取的网格质量过低，而possion提取的网格相较而言则更精确。总体而言，对于nerfacto方法来说，iteration的提升貌似没有对最终的网格产生比较大的改善，30k的迭代次数已经足够，而face的增加其实也显得不是很必要，50k的面数已经足够表达一个复杂的结构体了。对于2DGS，由于它没有整合到NeRF Studio里，它的重建没有固定面数，最终生成了2m的面，虽然重建效果好很多，但是对计算负载的压力更大。\r\n","categories":["三维重建"],"tags":["NeRF","3DGS"]},{"title":"将现有场景生成网格导入mujoco的简要pipeline","url":"/2024/12/10/11-MeshToMujoco/","content":"一、数据预处理\r\nnerfstudio里整合了COLMAP工具，可通过它的指令直接对现有的数据进行预处理，转化为nerfstudio需要的格式。支持的数据类型如下表所示：\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nData\r\nCapture Device\r\nRequirements\r\nns-process-data Speed\r\n\r\n\r\n\r\n\r\n📷 Images\r\nAny\r\nCOLMAP\r\n🐢\r\n\r\n\r\n📹 Video\r\nAny\r\nCOLMAP\r\n🐢\r\n\r\n\r\n🌎 360\r\nData\r\nAny\r\nCOLMAP\r\n🐢\r\n\r\n\r\n📱 Polycam\r\nIOS with LiDAR\r\nPolycam App\r\n🐇\r\n\r\n\r\n📱 KIRI\r\nEngine\r\nIOS or Android\r\nKIRI Engine App\r\n🐇\r\n\r\n\r\n📱 Record3D\r\nIOS with LiDAR\r\nRecord3D app\r\n🐇\r\n\r\n\r\n📱 Spectacular\r\nAI\r\nIOS, OAK, others\r\nApp\r\n/ sai-cli\r\n🐇\r\n\r\n\r\n🖥 Metashape\r\nAny\r\nMetashape\r\n🐇\r\n\r\n\r\n🖥 RealityCapture\r\nAny\r\nRealityCapture\r\n🐇\r\n\r\n\r\n🖥 ODM\r\nAny\r\nODM\r\n🐇\r\n\r\n\r\n👓 Aria\r\nAria glasses\r\nProject Aria\r\n🐇\r\n\r\n\r\n🛠 Custom\r\nAny\r\nCamera Poses\r\n🐇\r\n\r\n\r\n\r\n一个简单的处理的代码示例如下： ns-process-data &#123;video,images,polycam,record3d&#125; --data &#123;DATA_PATH&#125; --output-dir &#123;PROCESSED_DATA_DIR&#125;\r\n完整的参数集可参考此处。在教程网站也有更多其他类型数据的捕获教程。\r\n踩坑记录 点击展开colmap feature_extractor报错：\r\n报错内容：\r\n\r\n解决方案：\r\n\r\n确认xcb插件是否安装：\r\nsudo apt updatesudo apt install libxcb-*\r\n设置环境变量：\r\nexport QT_QPA_PLATFORM_PLUGIN_PATH=/home/felix/.local/lib/python3.10/site-packages/cv2/qt/pluginsexport QT_QPA_PLATFORM=offscreen\r\n（路径记得改成自己的路径）\r\n\r\n这样应该就可以了，至少博主执行这两串代码后能成功运行。（虽然这个处理过程确实很慢）\r\n\r\n二、模型训练\r\n可直接对处理后的数据用nerfacto方法进行训练：\r\nns-train nerfacto --data &#123;PROCESSED_DATA_DIR&#125;\r\n还有一些比较常用的参数：\r\n--help #显示帮助菜单--output-dir #输出文件夹--steps-per-save #多少step保存一次，默认为2000--max-num-iterations #最大iteration，默认为30000--mixed-precision #是否启用混合精度，默认为true--load-config #从配置文件读取，可从之前的训练中继续\r\n训练结果默认会保存在outputs文件夹中。其中包含一个config.yml文件，记录了训练的配置信息。可通过读取它来进行接续的训练或者可视化：\r\nns-viewer --load-config &#123;outputs/.../config.yml&#125;\r\n三、模型导出\r\n在nerfstudio里支持三种类型的模型导出：3d gaussion、point cloud和mesh。这里只提及mesh的导出。\r\n最简单的导出指令为：\r\nns-export poisson --load-config CONFIG.yml --output-dir OUTPUT_DIR\r\n而在可视化界面中，可以自动生成导出指令：\r\n\r\n其中可以对参数进行调整，比如Use Crop可以自动裁剪保留中心物体，# Faces可以控制产生的面数的上限，# Points控制点数的上限。最后生成的指令为以下结构：\r\nns-export poisson --load-config &#123;outputs/.../config.yml&#125; --output-dir &#123;OUTPUT_DIR&#125; --target-num-faces 50000 --num-pixels-per-side 2048 --num-points 1000000 --remove-outliers True --normal-method open3d --obb_center 0.0000000000 0.0000000000 0.0000000000 --obb_rotation 0.0000000000 0.0000000000 0.0000000000 --obb_scale 1.0000000000 1.0000000000 1.0000000000\r\n其中possion\r\n表示使用泊松重建方法，nerfstudio还支持其他的重建方法，如marching-cubes和tsdf等，但效果都不尽如人意（可参考我之前的文章NeRF\r\nStudio简要教程 | FelixChristian's Blog，里面有重建结果）。\r\n四、凸分解\r\n在我之前的文章Mujoco\r\n- CoACD简略教程 | FelixChristian's\r\nBlog中有介绍到CoACD工具的安装和使用。\r\n当编译完CoACD之后，可直接执行凸分解指令： ./main -i PATH_OF_YOUR_MESH -o PATH_OF_OUTPUT\r\n需要注意的是，两个PATH都是需要以具体的.obj文件结尾的。最终导出的单个.obj文件是以多个组件结合的。如果直接导入mujoco，它还是会对整个文件再次进行替换。\r\n为了在mujoco中实现正常的碰撞。必须把单个.obj文件导出为多个.stl文件，再在mujoco的.xml文件中进行引用。\r\n这里借助了Blender的Super-Batch-Export插件：\r\n\r\n需要注意的是，根据mujoco的官方文档介绍：\r\n\r\nMuJoCo can load triangulated meshes from OBJ files and binary STL.\r\nSoftware such as MeshLab can be\r\nused to convert from other formats. While any collection of triangles\r\ncan be loaded and visualized as a mesh, the collision detector works\r\nwith the convex hull. There are compile-time options for scaling the\r\nmesh, as well as fitting a primitive geometric shape to it. The mesh can\r\nalso be used to automatically infer inertial properties – by treating it\r\nas a union of triangular pyramids and combining their masses and\r\ninertias. Note that meshes have no color, instead the mesh is colored\r\nusing the material properties of the referencing geom. In contrast, all\r\nspatial properties are determined by the mesh data. MuJoCo supports both\r\nOBJ and a custom binary file format for normals and texture coordinates.\r\nMeshes can also be embedded directly in the XML.\r\n\r\nmujoco可以从.obj文件和.stl文件加载三角网格，所以在用Blender进行批量导出时，需要选择这两种格式。\r\n五、导入mujoco\r\n以.obj文件为例（.stl文件同理）：\r\n在上一步中导出的文件为convex_0.obj、convex_1.obj、...\r\n、convex_n.obj的形式。在我的FelixChristian011226/Mujoco-RL仓库中，编写了一个简单的脚本gen_convex.py，用于生成.xml文件中相应的引用。使用过程如下：\r\n\r\n首先，将生成的多个.obj文件，存放在./terrain/mesh/&#123;your_mesh_name&#125;/文件夹中。\r\n然后进入./terrain目录，使用如下指令：\r\npython3 ./gen_component/gen_convex.py &#123;model_folder&#125; &#123;indent_level&#125; &#123;total_count&#125;\r\n其中model_folder对应your_mesh_name，即./terrain/mesh/目录下存放.obj文件的文件夹名称。\r\nindent_level是缩进量，一般为2，仅仅为了.xml文件中美观而设置。\r\ntotal_count是总的.obj文件数量，即最后一个文件的下标加一。\r\n最后将.terrain/component/convex.xml文件中的前半部分内容，复制到对应的.xml的&lt;asset&gt;部分，后半部分内容复制到&lt;worldbody&gt;部分。然后给其设置纹理，如：\r\n&lt;material name=&quot;mat_&#123;name&#125;&quot; rgba=&quot;0.8 0.8 0.8 1&quot;/&gt;\r\n其中name应该和文件夹名对应。\r\n最后再运行对应的.xml文件即可。\r\n\r\n为了简化这个过程，我提供了一个自行编写的带GUI的python程序decomposite.py，可以执行它进行凸分解和导入过程，链接在此：decomposite.py。\r\n由于程序是用wxPython库实现的，需要自行使用pip安装，如遇gtk+报错可执行以下指令安装gtk+3：\r\nsudo apt install -y libgtk-3-dev\r\n","categories":["机器仿真"],"tags":["mujoco","NeRF","CoACD","mesh"]},{"title":"Photometric Stereo","url":"/2024/12/09/10-PhotometricStereo/","content":"1. Experiment Purpose\r\nThe goal of this experiment is to implement the Photometric Stereo\r\nalgorithm, which uses multiple images under different lighting\r\ndirections to estimate the surface normal vectors and albedo, and then\r\nre-render the image under a specified lighting direction.\r\n\r\nNormal Vector Calculation: According to the\r\nLambertian model formula ,\r\nwhere  is the albedo,  is the normal vector,\r\nand  is the\r\nlighting direction. The albedo \r\nand lighting direction  can be uniquely\r\ndetermined when at least three images with known lighting directions are\r\nprovided.\r\nShadow and Highlight Processing: Shadows and\r\nhighlights break the linear Lambertian model. A simple solution is to\r\nsort all the observations for each pixel and discard a certain\r\npercentage of the brightest and darkest pixels to remove shadows and\r\nhighlights.\r\n\r\n2. Experiment Principle\r\n\r\nAccording to the Lambertian model (a purely diffuse reflection\r\nmodel), for each point on the surface, there is a fixed albedo , and the intensity of the\r\nreflected light  depends only on\r\nthe albedo  and the angle\r\n of the incident light (the\r\nangle between the light direction and the normal vector). The formula is\r\nas follows: \r\n\r\nWhen we have images under different lighting directions, we can use\r\nthe Lambertian model to analyze each pixel individually. With three\r\nlighting angles, we can express the Lambertian model equation in matrix\r\nform as:  Since the reflected light intensity  and the lighting directions  are known, we can\r\nuniquely determine the albedo  and normal vector  through matrix inversion\r\nwhen three different lighting directions are available.\r\nWith the albedo and normal vectors, re-rendering the image under a\r\nspecified lighting direction is easy; we just apply the Lambertian model\r\nto calculate the light intensity at each pixel.\r\n3. Experiment Content\r\nIn the provided code framework, the primary task is to implement the\r\nmyPMS.m file.\r\n\r\nFirst, I modified the original function declarations because the\r\nfinal output requires three images: the normal map, albedo map, and\r\nre-rendered image, while the original framework only output the normal\r\nmap. So, I added the albedo map and re-rendered image to the function's\r\nreturn. The input now also includes a new parameter\r\nshadow_removal_percentage, which specifies the percentage\r\nof the brightest and darkest pixels to be discarded when handling\r\nshadows and highlights (e.g., if the value is 20, then the darkest 20%\r\nand the brightest 20% pixels are discarded).\r\nIn the data preprocessing phase, N, albedo,\r\nand re_rendered_img represent the normal map, albedo map,\r\nand re-rendered image, respectively. They are all three-channel images.\r\nI is used to temporarily store the light intensities for\r\neach channel.\r\n\r\nThrough simple loops, the original images were first processed with a\r\nmask to extract the subject and avoid the influence of the\r\nbackground. Then, for each RGB channel, the given\r\nlight_intensity was used to divide the original image,\r\nretrieving the true light intensity I for each channel.\r\n\r\nThen, for each pixel, the light intensities of all the images were\r\nsorted, and the brightest and darkest pixels of the given percentage\r\nwere discarded, storing the results in I_col_filtered.\r\nBased on the formula ,\r\nwe can first compute  by using\r\nthe least squares method on the filtered light directions\r\ns_filtered and light intensities\r\nI_col_filtered. The norm of the result gives the albedo\r\nvalue, and dividing the result by the albedo gives the unit normal\r\nvector.\r\n\r\nWith this information, re-rendering becomes straightforward. Simply\r\ntraverse the pixels and apply the Lambertian model formula for each\r\npixel. Since the assignment did not require the RGB components of the\r\nincident light, I assumed them to be 1 by default.\r\nThus, the main function for Photometric Stereo is complete. Below are\r\nsome small modifications to the Baseline:\r\n\r\nThe function call now includes the modified inputs and outputs. The\r\nshadow and highlight removal percentage is set to 20, meaning the\r\ndarkest 20% and the brightest 20% pixels are discarded. To make the\r\nfinal re-rendered image more visually appealing, it is also\r\nnormalized.\r\nHere is the full code for Photometric Stereo (expand to view):\r\n(On my GitHub repository ComputerVision/Homework\r\n1 you can also find the code)\r\nFolding Click to view morefunction [N, albedo, re_rendered_img] = L2_PMS(data, m, shadow_removal_percentage)    num_images = size(data.s, 1);    [height, width, ~] = size(data.imgs{1});        N = zeros(height, width, 3);    albedo = zeros(height, width, 3);    re_rendered_img = zeros(height, width, 3);    I = zeros(num_images, length(m), 3);        % Extract pixel intensities for each image    for c = 1:3        for i = 1:num_images            img = double(data.imgs{i});            img = img(m);            img = img / data.L(i, c);            I(i, :, c) = img;        end    end        for c = 1:3    % Remove shadows and highlights, compute normals and albedo        for i = 1:length(m)            I_col = I(:, i, c);                        % Sort intensities and remove shadows/highlights            [sorted_I, idx] = sort(I_col);            num_to_remove = round(length(sorted_I) * shadow_removal_percentage / 100);            valid_idx = idx(num_to_remove+1:end-num_to_remove);                        % Filter light source directions and intensities            s_filtered = data.s(valid_idx, :);            I_col_filtered = sorted_I(num_to_remove+1:end-num_to_remove);                        % Solve for normal and albedo            A = s_filtered \\ I_col_filtered;            albedo_val = norm(A);            norm_A = A / albedo_val;            [row, col] = ind2sub([height, width], m(i));            N(row, col, :) = norm_A';            albedo(row, col, c) = albedo_val;        end    end    % Re-render the image using recovered normals and albedo    viewing_direction = [0, 0, 1];    for c = 1:3        for i = 1:height            for j = 1:width                normal = squeeze(N(i, j, :));                if norm(normal) &gt; 0                    re_rendered_img(i, j, c) = max(0, dot(normal, viewing_direction)) * albedo(i, j, c);                end            end        end    end    end    \r\n\r\n4. Experiment Results\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nNormal Map\r\nAlbedo Map\r\nRe-rendered Picture\r\n\r\n\r\n\r\n\r\nbear\r\n\r\n\r\n\r\n\r\n\r\nbuddha\r\n\r\n\r\n\r\n\r\n\r\ncat\r\n\r\n\r\n\r\n\r\n\r\npot\r\n\r\n\r\n\r\n\r\n\r\n\r\n","categories":["研究生课程","计算机视觉"],"tags":["Photometric Stereo"]},{"title":"Panorama Stitching","url":"/2024/12/28/13-PanoramaStitching/","content":"Setting\r\n\r\nIf the camera center is fixed, or if the scene is planar, different\r\nimages of the same scene are related by a homography. In this project,\r\nyou will implement an algorithm to calculate the homography between two\r\nimages. Several images of each scene will be provided. Your program\r\nshould generate image mosaics according to these estimated\r\nhomographies.\r\n\r\nTo accomplish panorama stitching, the following pipeline is commonly\r\nused:\r\n\r\nFeature detection.\r\nFeature matching.\r\nHomography estimation.\r\nPanorama stitching.\r\n\r\nThe data given has four folders, each including 3-6 photos on\r\ndifferent perspectives. Note that the sequence of the photos are not\r\nconcerned to the actual shooting angle.\r\nEnvironment\r\n\r\nOS: Windows 11 24H2\r\nLib: Python 2.7, Opencv 3.4.2.16\r\n\r\nAlgorithm\r\n1. The Basic\r\nAlgorithm\r\n1.1 Feature\r\nDetection, Descriptor, and Matching\r\nThe first step in panorama stitching is identifying common features\r\nbetween pairs of images. These features allow us to align the images\r\ncorrectly. The code uses the Matchers class to perform this\r\nstep, which utilizes the SURF (Speeded-Up Robust\r\nFeatures) algorithm and FLANN-based\r\nmatching.\r\nFeature Detection and Descriptor Computation: \r\n\r\nSURF is used to detect distinctive keypoints in\r\neach image and compute descriptors. Keypoints are regions in the image\r\nthat are easily identifiable and remain consistent across different\r\nviews of the scene (such as corners, edges, or blobs). The descriptors\r\nare mathematical representations of the keypoints that allow for\r\ncomparison between images.\r\nIn the Matchers class, the method\r\n_get_features(image):\r\nkeypoints, descriptors = self.surf.detectAndCompute(gray, None)\r\nconverts the image to grayscale, detects keypoints, and computes the\r\ndescriptors for matching.\r\n\r\nMatching Keypoints: \r\nOnce the descriptors are extracted, the match method in\r\nthe Matchers class uses the FLANN (Fast Library for\r\nApproximate Nearest Neighbors) matcher to find the best matches\r\nbetween keypoints from two images:\r\nmatches = self.flann.knnMatch(features2['des'], features1['des'], k=2)\r\nFor each keypoint in image2, the two closest matches are\r\nfound in image1. The code then applies the ratio\r\ntest (Lowe’s ratio test) to filter out poor matches. Only good\r\nmatches are retained for the homography calculation:\r\ngood_matches.append((m.trainIdx, m.queryIdx))\r\n1.2 Homography\r\nEstimation\r\nOnce keypoints are matched, the next step is to estimate the\r\nhomography matrix, which represents the transformation\r\nrequired to align the two images. This transformation is essential to\r\nalign the images in the panorama seamlessly.\r\nHomography Calculation: \r\nThe match method uses the RANSAC\r\nalgorithm (Random Sample Consensus) to calculate the homography\r\nmatrix. RANSAC helps to eliminate outliers, ensuring that only reliable\r\nmatches are used to compute the transformation.\r\n\r\nThe keypoints of the matching pairs are converted into\r\nfloat32 coordinates:\r\npoints1 = np.float32([features1['kp'][i].pt for (i, _) in good_matches])points2 = np.float32([features2['kp'][i].pt for (_, i) in good_matches])\r\nThen, the findHomography function calculates the\r\nhomography matrix H:\r\nH, _ = cv2.findHomography(points2, points1, cv2.RANSAC, 4)\r\nThe result is the homography matrix H, which defines how\r\nto warp one image to align with the other.\r\n\r\nHomography Score: \r\nThe quality of the homography matrix can be assessed by calculating\r\nits norm using the _calculate_homography_score(H)\r\nmethod:\r\nreturn np.linalg.norm(H)\r\nA lower norm indicates a better match.\r\n1.3 Image Stitching\r\nWith the homography matrices in hand, the next step is to\r\nstitch the images together to form the final\r\npanorama.\r\nImage Alignment: \r\nThe Stitcher class has two main methods to handle the\r\nalignment process: _shift_left and\r\n_shift_right.\r\n\r\nLeft-side Image Alignment\r\n(_shift_left):\r\n\r\nStarting from the center image, images on the left are aligned\r\none by one by applying their respective homography matrices. The\r\ntransformation is applied using cv2.warpPerspective, which\r\nwarps the image based on the homography matrix H.\r\nThe code ensures that the resulting image has enough space to\r\naccommodate the newly aligned image by adjusting the size\r\ndynamically.\r\nImages are gradually merged using pixel-wise blending to avoid\r\nseams.\r\ntmp = cv2.warpPerspective(a, xh, dsize)\r\n\r\nRight-side Image Alignment\r\n(_shift_right):\r\n\r\nSimilarly, images on the right are aligned and stitched into the\r\npanorama. The right images are warped and blended into the final\r\nstitched image.\r\n\r\n\r\nBlending Images: \r\nTo avoid visible seams between adjacent images, the\r\n_blend_images method is used to combine the images\r\nsmoothly. It creates a weighted average of pixel values at overlapping\r\nregions:\r\nblended_image[y, x] = (    blended_image[y, x].astype(np.float32) * 0.5 +    warped_image[y, x].astype(np.float32) * 0.5).astype(np.uint8)\r\nThis ensures that the final panorama looks seamless, with no visible\r\nboundaries between the images.\r\nFinal Panorama Creation: \r\nOnce all images are aligned and blended, the final panorama is\r\ncreated, starting from the center image and progressively stitching\r\nimages on the left and right. The stitch() method returns\r\nthe final stitched image:\r\nreturn self.left_image\r\n2. Comparison of\r\ndifferent feature descriptors\r\nDescriptor Extraction:\r\n\r\nSIFT: Extracts scale-invariant keypoints and\r\ncomputes descriptors based on local image gradients\r\n(sift_descriptor function).\r\nPixel-based: Detects keypoints using corner\r\ndetection and creates descriptors from pixel values in local patches\r\n(pixel_descriptor function).\r\n\r\nDescriptor Matching: The\r\nmatch_descriptors function uses FLANN-based matching to\r\nfind potential matches between the descriptors of the two images.\r\nHomography Estimation with RANSAC: The\r\nestimate_homography function uses RANSAC to estimate a\r\ntransformation between the images and counts inliers (correct matches).\r\nRANSAC is run multiple times, and the best transformation is chosen\r\nbased on the number of inliers.\r\nRANSAC Iterations Calculation: The\r\ncalculate_ransac_iterations function computes how many\r\niterations are needed to achieve a specified success probability based\r\non the inlier ratio.\r\nPerformance Evaluation: The\r\ncompare_descriptors function compares the two descriptors\r\nby calculating the inlier ratio and required RANSAC iterations. It\r\nvisualizes matches and evaluates the descriptors' performance using\r\nRANSAC on a set of random matches.\r\nExperiments\r\n1. Image Stitching\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\ndataset\r\nresult\r\n\r\n\r\n\r\n\r\ndata1\r\n\r\n\r\n\r\ndata2\r\n\r\n\r\n\r\ndata3\r\n\r\n\r\n\r\ndata4\r\n\r\n\r\n\r\n\r\nThough my algorithm accomplishes panorama stitching, there is\r\nobviously a way to improve. My result shows that the stitchings are\r\nstarting with the picture on the edge, but it's proper to start from the\r\nmiddle. Though I've set the picture which has the most matching with\r\nother pics as the base picture, it turns out to be the the most marginal\r\none. It's confusing. Anyway, the result is sufficent, at least a lot\r\nbetter than the code I first wrote.\r\n2. Comparison of feature\r\ndescriptors\r\nThis is a matching result (taking data4/IMG_7357.JPG and\r\ndata4/IMG_7358.JPG as example):\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\ndescriptor\r\nmatching result\r\n\r\n\r\n\r\n\r\nSIFT\r\n\r\n\r\n\r\nPixel\r\n\r\n\r\n\r\n\r\nThis is the summarizing table:\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\ndataset\r\niteration (SIFT)\r\niteration (Pixel)\r\naverage ratio (SIFT)\r\naverage ratio (Pixel)\r\n\r\n\r\n\r\n\r\ndata1/112_1298.JPGdata1/112_1299.JPG\r\n24\r\n26\r\n0.41\r\n0.40\r\n\r\n\r\ndata2/IMG_0488.JPGdata2/IMG_0489.JPG\r\n22\r\n26\r\n0.42\r\n0.40\r\n\r\n\r\ndata3/IMG_0675data3/IMG_0676\r\n25\r\n25\r\n0.40\r\n0.40\r\n\r\n\r\ndata4/IMG_7357data4/IMG_7358\r\n23\r\n26\r\n0.41\r\n0.40\r\n\r\n\r\n\r\nIt shows that SIFT descriptor contributes to less iterations and\r\nbetter average inlier ratio.\r\n","categories":["研究生课程","计算机视觉"],"tags":["Panorama Stitching"]},{"title":"Genesis简要介绍与样例展示","url":"/2024/12/19/12-GenesisIntroduction/","content":"Introduction\r\nGenesis\r\n是一个物理平台，专为通用机器人/嵌入式人工智能/物理人工智能应用而设计。它同时是多种事物：\r\n\r\n从头开始重建的通用物理引擎，能够模拟各种材料和物理现象。\r\n一个轻量级、超快速、Pythonic 且用户友好的机器人仿真平台。\r\n强大且快速的逼真渲染系统。\r\n一种生成数据引擎，可将用户提示的自然语言描述转换为各种数据形式。\r\n\r\nExamples\r\n分别测试现有的examples中的场景，结果如下：\r\n1. Simulation\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nFile\r\nResult\r\n\r\n\r\n\r\n\r\ncloth_on_rigid.py\r\n\r\n\r\n\r\ncut_dragon.py\r\n\r\n\r\n\r\nflush_cubes.py\r\n\r\n\r\n\r\ngrasp_soft_cube.py\r\n\r\n\r\n\r\nsand_wheel.py\r\n\r\n\r\n\r\nsph_mpm.py\r\n\r\n\r\n\r\nsph_rigid.py\r\n\r\n\r\n\r\nwater_wheel.py\r\n\r\n\r\n\r\n\r\n(目前就找到这些)\r\n2. Training\r\n(1). drone (hover)\r\n\r\n(2). locomotion (go2)\r\n\r\n(3). modified locomotion (go2)\r\n本来尝试对go2_env增加一个mesh的障碍，但是训练中发现go2直接穿模了。于是用最简单的Box代替，做了个简单的围墙：\r\n\r\n对其进行了10000个iteration的训练：\r\n\r\n看看训练结果：\r\n\r\nErrors &amp; Solutions\r\n1.\r\nOpenGL.error.Error: Attempt to retrieve context when no valid\r\ncontext\r\n在Windows的WSL虚拟机中安装了Ubuntu系统，在直接运行测试样例时会出现如下的报错：\r\n\r\n在Github上面有类似的问题和解决方案。\r\n原因：Ubuntu 的最新版本已默认切换为使用 Wayland\r\n图形协议（来自 Xorg）。目前，pyglet 不支持 Wayland。虽然单独使用 pyglet\r\n仍然有效（感谢 xWayland），但使用 pyimgui-pyglet 会导致 PyOpenGL\r\n选择不正确的图形后端。\r\n解决方案：在运行前强制指定PyOpenGL使用Xorg：\r\nexport PYOPENGL_PLATFORM=&#x27;glx&#x27;\r\nUnsolved Problems\r\n1. Decomposition\r\ndeactivates collision\r\n在genesis中导入的mesh，在导入时可以设置参数进行凸分解。底层还是调用的CoACD工具。试图在genesis对mesh直接进行凸分解，但是发现凸分解之后偶尔会导致碰撞失效。\r\n参数设置：\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n不进行凸分解\r\n凸分解\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n在genesis的Mesh类中，有两个参数与凸分解有关：\r\n\r\nconvexify:\r\n此参数用于控制是否将Mesh直接进行凸包运算，即等同于mujoco中的默认算法，将Mesh求个凸包。实际碰撞的体积与渲染出来的实际形状会有差异。\r\ndecompose_nonconvex:\r\n此参数仅在convexify为False时有效，表示是否使用CoACD对Mesh进行凸分解。\r\n\r\n结果比对：\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n不进行凸分解\r\n凸分解\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n在实际运行中，当对复杂模型进行凸分解（即decompose_nonconvex设为True），可能会导致穿模的情况。更换模型后，发现又成功了：\r\n\r\nAnyway，目前还不清楚问题出在CoACD还是genesis。\r\n2. MJCF Import Failure\r\ngenesis中导入mujoco的场景xml时，很容易出现消失的情况。在issue#603和#287里有提到。\r\n如果需要将xml成功导入，需要给geom设置conaffinity和contype参数为0。但是这样就避免了物体间的碰撞。\r\n查阅genesis的源码，目前的推测是，genesis里的一个Entity是一个单独的单位，要对他进行material和texture等的设置，所以默认避免了单个Entity导入多个物件。\r\n","categories":["机器仿真"],"tags":["genesis"]},{"title":"Genesis分析","url":"/2025/01/11/15-GenesisAnalysis/","content":"Taichi\r\n@ti.func &amp; @ti.kernel\r\n\r\nWhen writing compute-intensive tasks, users can leverage Taichi's\r\nhigh performance computation by following a set of extra rules, and\r\nmaking use of the two decorators @ti.func and\r\n@ti.kernel. These decorators instruct Taichi to take over\r\nthe computation tasks and compile the decorated functions to machine\r\ncode using its just-in-time (JIT) compiler. As a result, calls to these\r\nfunctions are executed on multi-core CPUs or GPUs and can achieve\r\nacceleration by 50x~100x compared to native Python code.\r\n在编写计算密集型任务时，用户可以通过遵循一组额外规则并使用两个装饰器\r\n@ti.func 和 @ti.kernel 来利用 Taichi\r\n的高性能计算。这些装饰器指示 Taichi 接管计算任务，并使用其即时 (JIT)\r\n编译器将装饰函数编译为机器代码。因此，对这些函数的调用在多核 CPU 或 GPU\r\n上执行，与原生 Python 代码相比，可以实现 50 倍~100 倍的加速。\r\n\r\n@ti.kernel: 用\r\n@ti.kernel 修饰的函数称为 Taichi\r\n内核或简称为内核。这些函数是Taichi运行时接管任务的入口点，它们必须由Python代码直接调用。您可以使用原生\r\nPython\r\n来准备任务，例如从磁盘读取数据和预处理，然后调用内核将计算密集型任务卸载到\r\nTaichi。\r\n@ti.func: 用 @ti.func\r\n修饰的函数称为 Taichi 函数。这些函数是内核的构建块，只能由另一个 Taichi\r\n函数或内核调用。与普通的 Python 函数一样，您可以将任务划分为多个 Taichi\r\n函数，以增强可读性并在不同的内核中重用它们。\r\nNotes:\r\n\r\nTaichi\r\n要求内核的参数和返回值进行类型提示，除非它既没有参数也没有返回语句。\r\n内核或 Taichi 函数内的代码是 Taichi 范围的一部分。 Taichi\r\n的运行时在多核 CPU 或 GPU 设备上并行编译和执行该代码，以实现高性能计算。\r\nTaichi 作用域相当于 CUDA 的设备端。\r\nTaichi 范围之外的代码属于 Python\r\n范围。这段代码是用原生Python编写的，并由Python的虚拟机执行，而不是由Taichi的运行时执行。\r\nPython 作用域相当于 CUDA 的主机端。\r\n\r\nKernel Notes\r\n\r\n一个内核可以接受多个参数。但是，请务必注意，您不能将任意 Python\r\n对象传递给内核。这是因为 Python 对象可以是动态的，并且可能包含 Taichi\r\n编译器无法识别的数据。\r\n标量、 ti.types.matrix() 、\r\nti.types.vector() 和 ti.types.struct()\r\n按值传递，这意味着内核收到参数的副本。但是，\r\nti.types.ndarray() 和 ti.template()\r\n是通过引用传递的，这意味着对内核内部参数所做的任何更改也会影响原始值。\r\n您可以使用 ti.types.ndarray() 作为类型提示，将\r\nndarray 从 NumPy 或 tensor 从 PyTorch\r\n传递到内核。 Taichi\r\n可以识别这些数据结构的形状和数据类型，这使您可以在内核中访问它们的属性。\r\n最多允许有一个返回值，该返回值可以是标量、\r\nti.types.matrix() 或 ti.types.vector()\r\n。此外，在基于 LLVM 的后端（CPU 和 CUDA 后端）中，返回值也可以是\r\nti.types.struct() 。\r\n全局变量视为编译时常量。这意味着它在编译时获取全局变量的当前值，并且之后不会跟踪它们的更改。\r\n\r\nFunc Notes:\r\n\r\n所有的 Taichi 函数都被强制内联 -&gt; 不允许运行时递归。\r\nTaichi 函数可以接受多个参数，其中可能包括标量、\r\nti.types.matrix() 、 ti.types.vector() 、\r\nti.types.struct() 、 ti.types.ndarray() 、\r\nti.field() 类型。\r\n可以有多个返回值。\r\n\r\nComparison\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nkernel\r\nfunc\r\n\r\n\r\n\r\n\r\n调用范围\r\nPython 作用域\r\nTaichi 作用域\r\n\r\n\r\n参数的类型提示\r\n必需\r\n推荐\r\n\r\n\r\n返回值的类型提示\r\n必需\r\n推荐\r\n\r\n\r\n返回类型\r\n标量ti.types.matrix()ti.types.vector()ti.types.struct()(Only\r\non LLVM-based backends)\r\n标量ti.types.matrix()ti.types.vector()ti.types.struct()...\r\n\r\n\r\n参数中元素数量上限\r\n32（适用于 OpenGL）64（适用于其他后端）\r\n无限制\r\n\r\n\r\nreturn 语句中返回值数量上限\r\n1\r\n无限\r\n\r\n\r\n\r\n@ti.dataclass\r\n@ti.dataclassclass \tSphere:    \tcenter: vec3        \tradius: float\r\nField\r\nTaichi field 是全局数据容器，从 Python 作用域或\r\nTaichi 作用域均能访问。\r\nf_2d = ti.field(int, shape=(3, 6))  # A 2D field in the shape (3, 6)\r\n\r\nScalar field: 存储的是标量，是最基本的\r\nfield。\r\n\r\n一个0D 的标量 field 是单个标量。\r\n一个一维标量 field 是由标量组成的一个一维数组。\r\n一个二维标量 field\r\n是由标量组成的一个二维数组，以此类推。\r\n\r\n\r\n# Declares a 3x3 vector field comprising 2D vectorsf = ti.Vector.field(n=2, dtype=float, shape=(3, 3))\r\n\r\nVector field:\r\n每个元素都是N维向量的向量场。\r\n\r\n# Declares a 300x400x500 matrix field, each of its elements being a 3x2 matrixtensor_field = ti.Matrix.field(n=3, m=2, dtype=ti.f32, shape=(300, 400, 500))\r\n\r\nMatrix field: 每个元素都是矩阵。\r\n\r\n矩阵运算在编译时展开。\r\n在较大的矩阵（例如32x128\r\n）上运行可能会导致编译时间更长且性能较差。\r\n\r\n\r\n# Declares a 1D struct field using the ti.Struct.field() methodn = 10particle_field = ti.Struct.field(&#123;        &quot;pos&quot;: ti.math.vec3,        &quot;vel&quot;: ti.math.vec3,        &quot;acc&quot;: ti.math.vec3,        &quot;mass&quot;: float,  &#125;, shape=(n,))\r\n\r\nStruct field: 存储用户自定义的结构体。\r\n\r\nTaichi 编译器能够自动推断底层的数据布局并应用合适的数据读取顺序。\r\n这是 Taichi 编程语言相较其他大多数通用编程语言的一大优势。\r\nNdarray\r\n\r\nndarray\r\n总是分配一个连续的内存块，以允许与外部库进行直接的数据交换。\r\n与 field 一样，ndarray 只能在 Python\r\n作用域中构造，而不能在 Taichi 作用域中构造。也就是说，不能在 Taichi\r\n内核或函数内部构造 ndarray。\r\n当标量类型的 NumPy ndarray 或 PyTorch 张量作为参数传递给 Taichi\r\n内核时，它可以被解释为标量类型数组、向量类型数组或矩阵类型数组。这是由类型提示ti.types.ndarray()中的dtype和ndim选项控制的。\r\n\r\nGenesis\r\nRigid Solver\r\nInit\r\n作用：初始化求解器对象，设置其初始状态和配置信息。\r\n功能：\r\n\r\n调用基类 Solver 的初始化。\r\n根据 options 配置求解器行为：\r\n\r\n是否启用碰撞检测、关节限制、自碰撞。\r\n最大碰撞对数量。\r\n动力学积分器类型。\r\n是否启用休眠优化（hibernation）。\r\n\r\n初始化内部变量，如当前步数\r\n_cur_step，以及存储实体的列表 _entities。\r\n\r\nAdd Entity\r\n作用：添加一个物理实体（如刚体、机器人、无人机等）到求解器中。\r\n参数：\r\n\r\nidx：实体的全局索引，用于唯一标识。\r\nmaterial：该实体的材质类型（例如刚体或角色）。\r\nmorph：实体的形态类型（例如无人机形态）。\r\nsurface：几何表面信息（用于碰撞检测或渲染）。\r\nvisualize_contact：是否需要可视化接触点。\r\n\r\n功能：\r\n\r\n根据 material 和 morph\r\n类型选择实体的类别：\r\n\r\nAvatarEntity：角色实体。\r\nDroneEntity：无人机实体。\r\nRigidEntity：普通刚体。\r\n\r\n检查是否支持显示接触点（AvatarEntity不支持）。\r\n初始化实体对象，并将其添加到 _entities 列表中。\r\n\r\n返回值：新创建的实体对象。\r\nBuild\r\n作用：构建求解器的内部状态和数据结构，为模拟做好准备。\r\n功能：\r\n\r\n初始化全局变量（如环境数量 n_envs 和并行级别\r\n_para_level）。\r\n调用每个实体的 build 方法，设置其相关参数。\r\n初始化关键的数据结构：\r\n\r\n自由度数量、关节数量、几何体数量等统计数据。\r\n动力学矩阵（质量矩阵等）和字段（如自由度状态、几何体状态）。\r\n\r\n运行初始的正向运动学（Forward Kinematics）更新几何体状态。\r\n初始化碰撞检测器、约束求解器等模块。\r\n\r\nComparison with Genesis and\r\nMujoco\r\n0. Pipeline\r\n\r\n1. Mass Matrix\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nGenesis\r\nMujoco\r\n\r\n\r\n\r\n\r\nCRB\r\n\r\n\r\n\r\n\r\nMass Matrix\r\n\r\n\r\n\r\n\r\n\r\n在CRB的计算中，genesis和mujoco使用了一样的策略。从末端开始遍历，自下而上地累计惯性。\r\n有这个惯性矩阵之后，再计算每个自由度的惯性贡献，更新自由度状态\r\nf_ang（角力矩）和\r\nf_vel（线力矩）。然后填充这个质量矩阵。\r\n实现细节上，genesis会有一个休眠的判断，避免了不必要的计算。此外，genesis如下定义力矩计算的函数：\r\n\r\n使用@ti.func修饰，将运算从Python虚拟机移到Taichi进行加速。而mujoco中此函数的处理是按照单个元素进行处理的：\r\n\r\n这种非连续的取值以及单元素的运算可能也是导致效率降低的原因。\r\n2. Inversed Mass Matrix\r\n为解释矩阵逆的计算，先简要引入两个矩阵分解方式的介绍：\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\\(LU\\)分解\r\n\\(LDL^T\\)分解\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\ngenesis:\r\n\r\n对质量矩阵 mass_mat 进行 LU 分解。\r\n使用前向和后向替代，逐列计算逆矩阵 mass_mat_inv。\r\n\r\n在genesis中，为了简化质量矩阵的逆矩阵的计算，使用了\\(LU\\)分解。而在mujoco中，使用的是 \\(LDL^T\\)​分解。这两者我感觉本身计算量的差别不会太大。然而在genesis的计算中，对于自由度的遍历依然可以实现并行化：\r\n\r\n此外，在分解的过程中。mujoco中自己定义了函数，是通过简单的遍历实现的。\r\n\r\n在genesis中，虽然也是遍历，但是它的参数都是通过taichi重构的，可能相较于原生的C语言会有一定的加速。\r\n3. Force Calculation\r\n\r\n该函数的目的是计算每个自由度上的总力，包括控制力、阻尼力和弹性力。\r\n在力的计算中，genesis中有三种控制模式，根据代码，分别是FORCE、VELOCITY、POSITION。\r\n控制模式：\r\n\r\nFORCE 模式：\r\n\r\n直接将控制力 ctrl_force 应用于自由度。\r\n\r\nVELOCITY 模式：\r\n\r\n根据当前速度 vel 和目标速度 ctrl_vel\r\n计算阻尼力：\r\nforce = kv * (ctrl_vel - vel)\r\nkv 是速度控制的增益系数。\r\n\r\nPOSITION 模式：\r\n\r\n根据当前位置 pos 和目标位置 ctrl_pos\r\n计算弹性力和阻尼力：\r\nforce = kp * (ctrl_pos - pos) - kv * vel\r\nkp 是位置控制的增益系数。\r\n\r\n\r\n如果计算出的力大于阈值EPS，则唤醒实体。\r\n此外，force还需要根据如下方式减去弹性力和阻尼力：\r\nforce -= qpos * stiffnessforce -= damping * vel\r\nmujoco中也是三种控制，position、velocity和acceleration，大体上是一致的：\r\n\r\n4. Update Acceleration and\r\nForce\r\n\r\n遍历每个Entity的每个Link，是根节点的话，设置线加速度为重力加速度，角加速度为0。不是根节点则继承父节点的加速度。\r\n\r\n流程：\r\n\r\n遍历链接的所有自由度（从 dof_start 到\r\ndof_end）。\r\n对每个自由度：\r\n\r\n根据自由度的速度 vel，计算其对线加速度的贡献\r\ncdofd_vel * vel。\r\n计算其对角加速度的贡献 cdofd_ang * vel。\r\n\r\n\r\n力和力矩的计算与加速度计算类似。且其中调用的都是@ti.func修饰的矩阵运算函数。\r\n5. Inverse Link Force\r\n\r\n从每个实体的末端链接开始，向父链接累积力。\r\n6. Bias Force\r\n\r\n基于链接的累积力和每个自由度的运动相关量，计算偏置力。\r\n分量解释：\r\n\r\ncdof_ang 和 cdof_vel\r\n是每个自由度的运动分量（角速度分量和线速度分量）。\r\ncfrc_flat_ang 和 cfrc_flat_vel\r\n是链接的累积角力和线力。\r\n偏置力的计算：通过上述点积操作，将累积的力和自由度的运动相关联，得出自由度上的偏置力。\r\n\r\n7. Compute DOF Acceleration\r\n\r\n对每个DOF，他要遍历添加所有其他DOF给它施加的约束力。\r\n\r\n将约束力 qf_constraint 添加到当前自由度的总力上。\r\n再使用质量矩阵的逆\\(M^{-1}\\)和总力force计算加速度acc。\r\n\r\n8. Integrate\r\n基于当前自由度的加速度和速度，计算并更新自由度的状态（包括位置和速度）。\r\n基于加速度更新速度\\(v\\leftarrow\r\nv+a\\cdot\\Delta t\\)：\r\n\r\n更新位置：\r\n\r\n旋转状态和旋转增量：\r\n\r\nrot 代表刚体在全局坐标系中的当前旋转状态。\r\nang 则是时间步内的旋转变化，由角速度直接导出。\r\n旋转合并：\r\n\r\n将旋转向量\\(\\mathbf{ang}\\)转换为四元数。\r\n再通过四元数乘法更新当前的旋转状态rot。\r\n以上是FREE\r\nJOINT的更新方式，非FREE的直接根据vel更新即可：\r\n\r\n\r\n9. Contraint Force\r\n\r\n以下详细对比下genesis和mujoco在碰撞检测中的实现：\r\n\r\nBroad Phase:\r\n在genesis和mujoco中，都使用了AABB对碰撞检测对进行一个快速的排除。但是它的具体的实现细节又有所不同。\r\n在genesis中，使用的是Sweep and Prune (SAP)方式，它将所有物体AABB盒的Min点与Max点分别在XYZ轴上投影，如果在某一轴上不满足Max1 &gt; Min2 &amp;&amp; Max2 &gt; Min1则不会发生碰撞。\r\n在mujoco中，使用的是Bounding Volume Hierarchical Tree(BVH)算法。它是通过二叉树来管理所有的AABB盒。在计算碰撞时，遍历二叉树以获取所有可能的碰撞对。\r\nNarrow Phase:\r\n窄相检测的目的是根据宽相检测检测出的碰撞对再进行细致的计算，获取碰撞接触点、法向量、穿透深度等信息。\r\n窄相检测中，genesis使用的是SDF方法。根据物体的类别，SPHERE、TERRAIN、CONVEX或者其他，调用不同的函数。在SDF底层实现中，genesis使用的依然是遍历查询每个顶点，判断是否侵入另一个物体。\r\n在mujoco中，定义了一个碰撞函数检测表：\r\n\r\n不同于genesis全部计算SDF，mujoco中对于凸包与凸包间的碰撞，使用了GJK算法来判断凸包是否相交，如果检测到相交，再用EPA算法计算穿透深度和接触点。\r\nAdd Constraint:\r\n在获取到碰撞信息后，genesis会通过add_collision_constraints函数将约束施加回去。mujoco中通过函数mj_makeConstraint添加约束。\r\n\r\nPerformance Analysis\r\n1. Taichi\r\n(1). data structure\r\n\r\n在genesis的初始化中，很多变量就以taichi的数据格式进行初始化了，便于后续高效处理。\r\n(2). Parallelize\r\n\r\ngenesis使用了taichi的loop_config函数进行并行化的控制。其中serialize控制是否并行，如果将serialize设置为\r\nTrue\r\n，则for循环将串行运行。关于_para_level的定义在此：\r\n\r\n在使用CPU时不进行并行。GPU时根据场景是否分批次进行部分或全部的并行。\r\n2. hibernation\r\n\r\n功能启用:\r\n\r\nhibernation 的启用由 self._use_hibernation\r\n参数控制。\r\n当启用时，程序会在初始化中设定相关状态变量，例如\r\nself.dofs_state.hibernated 和\r\nself.links_state.hibernated\r\n标记rigid_solverrigid_solver。\r\n\r\n状态检测:\r\n\r\n判断刚体是否进入 hibernation\r\n的依据是速度和加速度的阈值（self._hibernation_thresh_vel 和\r\nself._hibernation_thresh_acc）。如果某个刚体的运动状态低于这些阈值，则会被标记为“休眠”状态rigid_solverrigid_solver。\r\n\r\n休眠机制的作用:\r\n\r\n对于被标记为 hibernated\r\n的刚体，程序会从后续的物理计算中剔除它们。这包括质量矩阵计算、动态求解以及碰撞检测等部分rigid_solver。\r\n只有当这些刚体再次被外力或约束影响而超出阈值时，它们才会被重新唤醒并重新加入计算rigid_solver。\r\n\r\n核心实现:\r\n\r\nself._func_hibernate() 函数处理刚体是否应该进入\r\nhibernation 的逻辑rigid_solver。\r\nself._func_aggregate_awake_entities()\r\n则用于更新非休眠状态的实体列表，以确保只对活跃的刚体执行仿真rigid_solver。\r\n\r\n性能优化:\r\n\r\n通过减少对静态或低动态实体的重复计算，hibernation\r\n可以显著降低复杂场景中的计算开销。\r\n适用于场景中存在大量静止或近似静止的物体的情况。\r\n\r\n","categories":["机器仿真"],"tags":["mujoco","genesis"]},{"title":"Scene Recognition with Bag of Words","url":"/2025/01/09/14-SceneRecognition/","content":"1. Setting\r\n1.1 Objective\r\n\r\nThe goal of this project is to give you a basic introduction to image\r\nrecognition. Specifically, we will examine the task of scene recognition\r\nstarting with a very simple method, e.g., tiny images and nearest\r\nneighbor classification, and then move on to bags of quantized local\r\nfeatures.\r\n\r\nThe dataset consists of images from 15 different categories. Each\r\ncategory contains multiple images, and the task is to classify these\r\nimages into their respective categories.\r\n1.2 Environment\r\n\r\nOS: Linux (Ubuntu 22.04)\r\nLib: Python 3, scikit-learn, OpenCV, NumPy.\r\n\r\n2. Algorithm\r\n2.1 Tiny Image + KNN\r\nThe Tiny Image + KNN method works by resizing images\r\nto a small fixed size and flattening the pixel values into a vector.\r\nThis vector is used as a feature representation for the image, which is\r\nthen classified using the k-Nearest Neighbors algorithm.\r\n\r\nSteps:\r\n\r\nResize the images to a smaller size (e.g., 32x32 pixels).\r\n\r\nFlatten the resized image into a 1D vector of pixel\r\nintensities.\r\n\r\nUse KNN to classify the image based on the distance between feature\r\nvectors.\r\n\r\n\r\nThis method is simple but may not capture high-level patterns or\r\nfeatures in the image, as it relies purely on raw pixel intensities.\r\n2.2 Bag of SIFT + KNN\r\nThe Bag of SIFT + KNN method involves several key\r\nsteps:\r\n\r\nSIFT Feature Extraction: Key points are detected in\r\neach image, and descriptors are computed for those key points.\r\nClustering: SIFT descriptors are clustered into a\r\npredefined number of clusters (vocabulary size).\r\nFeature Representation: Each image is represented\r\nby a histogram of the number of occurrences of each visual word (cluster\r\ncenter).\r\nKNN Classification: The image's histogram is\r\ncompared to the histograms of training images, and the class with the\r\nmost neighbors is assigned to the image.\r\n\r\nIn my code, SIFT features are extracted and clustered into visual\r\nwords using k-means clustering. The resulting vocabulary size is tested\r\nwith different numbers of clusters (10, 30, 50, 70, 100).\r\n3. Experiments\r\n3.1 Results\r\nThe following table summarizes the classification accuracy for both\r\nthe Tiny Image + KNN method and the Bag of SIFT\r\n+ KNN method with different visual vocabulary sizes.\r\n\r\n\r\n\r\nMethod\r\nVocabulary Size\r\nAverage Accuracy\r\n\r\n\r\n\r\n\r\nTiny Image + KNN\r\nN/A\r\n0.2236\r\n\r\n\r\nBag of SIFT + KNN\r\n10\r\n0.2624\r\n\r\n\r\nBag of SIFT + KNN\r\n30\r\n0.3354\r\n\r\n\r\nBag of SIFT + KNN\r\n50\r\n0.3550\r\n\r\n\r\nBag of SIFT + KNN\r\n70\r\n0.3568\r\n\r\n\r\nBag of SIFT + KNN\r\n100\r\n0.3578\r\n\r\n\r\n\r\n3.2 Accuracy Details\r\nHere are the detailed accuracy results for each category in the\r\ndataset.\r\nTiny Image + KNN Results\r\n\r\n\r\n\r\nCategory\r\nAccuracy\r\n\r\n\r\n\r\n\r\ncoast\r\n0.4154\r\n\r\n\r\nforest\r\n0.1053\r\n\r\n\r\nhighway\r\n0.5375\r\n\r\n\r\ninsidecity\r\n0.0673\r\n\r\n\r\nmountain\r\n0.1606\r\n\r\n\r\noffice\r\n0.1826\r\n\r\n\r\nopencountry\r\n0.3581\r\n\r\n\r\nstreet\r\n0.5000\r\n\r\n\r\nsuburb\r\n0.3546\r\n\r\n\r\ntallbuilding\r\n0.1719\r\n\r\n\r\nbedroom\r\n0.1638\r\n\r\n\r\nindustrial\r\n0.0900\r\n\r\n\r\nkitchen\r\n0.0818\r\n\r\n\r\nlivingroom\r\n0.1376\r\n\r\n\r\nstore\r\n0.0279\r\n\r\n\r\n\r\nBag of SIFT\r\n+ KNN Results (Different Vocabulary Sizes)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nCategory\r\nVocabulary Size = 10\r\nVocabulary Size = 30\r\nVocabulary Size = 50\r\nVocabulary Size = 70\r\nVocabulary Size = 100\r\n\r\n\r\n\r\n\r\ncoast\r\n0.3308\r\n0.3423\r\n0.3846\r\n0.3769\r\n0.3654\r\n\r\n\r\nforest\r\n0.7237\r\n0.7763\r\n0.8289\r\n0.8377\r\n0.8026\r\n\r\n\r\nhighway\r\n0.1938\r\n0.2812\r\n0.2938\r\n0.3125\r\n0.3375\r\n\r\n\r\ninsidecity\r\n0.1394\r\n0.2452\r\n0.2788\r\n0.3029\r\n0.2740\r\n\r\n\r\nmountain\r\n0.2080\r\n0.3723\r\n0.4562\r\n0.4051\r\n0.4307\r\n\r\n\r\noffice\r\n0.2696\r\n0.2174\r\n0.3304\r\n0.3478\r\n0.3130\r\n\r\n\r\nopencountry\r\n0.1871\r\n0.2516\r\n0.1839\r\n0.2290\r\n0.1935\r\n\r\n\r\nstreet\r\n0.2812\r\n0.3229\r\n0.3073\r\n0.2812\r\n0.2448\r\n\r\n\r\nsuburb\r\n0.5106\r\n0.7021\r\n0.7730\r\n0.7376\r\n0.8014\r\n\r\n\r\ntallbuilding\r\n0.1758\r\n0.2852\r\n0.3086\r\n0.3242\r\n0.3438\r\n\r\n\r\nbedroom\r\n0.1379\r\n0.2241\r\n0.1121\r\n0.1638\r\n0.1466\r\n\r\n\r\nindustrial\r\n0.1090\r\n0.1611\r\n0.1801\r\n0.2370\r\n0.1848\r\n\r\n\r\nkitchen\r\n0.1455\r\n0.2091\r\n0.1636\r\n0.1182\r\n0.1818\r\n\r\n\r\nlivingroom\r\n0.2487\r\n0.2963\r\n0.2487\r\n0.2275\r\n0.2487\r\n\r\n\r\nstore\r\n0.2744\r\n0.3442\r\n0.4744\r\n0.4512\r\n0.4977\r\n\r\n\r\n\r\n4. Conclusion\r\n\r\nThe Tiny Image + KNN method results in lower\r\naccuracy (0.2236 on average) because it relies solely on raw pixel\r\nvalues, which do not capture the complex features and patterns of the\r\nimages effectively.\r\nBag of SIFT + KNN performs much better, with the\r\naccuracy improving as the vocabulary size increases. The highest\r\naccuracy was obtained with a vocabulary size of 100, yielding an average\r\naccuracy of 0.3578.\r\nIncreasing the visual vocabulary size improves the\r\nmodel’s ability to classify images by capturing more detailed and\r\ndistinct features from the SIFT descriptors.\r\n\r\nIn conclusion, Bag of SIFT + KNN is a more effective\r\nmethod for image classification compared to Tiny Image +\r\nKNN, especially when a larger vocabulary is used.\r\n","categories":["研究生课程","计算机视觉"],"tags":["Scene Recognition"]},{"title":"Policy Gradient and Advantage Estimation","url":"/2025/07/01/18-DeepRL_lecture3/","content":"一、策略梯度推导\r\n(Policy Gradient derivation)\r\n策略优化的目标是找到一组参数 \r\n来最大化期望回报。我们使用的策略是随机性策略 ，它表示在状态  下采取动作  的概率。\r\n目标函数\r\n我们的目标是最大化期望的累积奖励：\r\n\r\n我们将一个状态-动作序列（即轨迹）表示为 ，该轨迹的回报为 。\r\n目标函数可以写成对所有可能轨迹的期望：\r\n\r\n其中  是在策略\r\n 下，轨迹  发生的概率。\r\n似然比策略梯度 (Likelihood Ratio Policy\r\nGradient)\r\n为了通过梯度上升来优化参数 ，我们需要计算目标函数  对  的梯度。推导过程如下：\r\n\r\n对目标函数求导： \r\n将梯度算子移入求和号内： \r\n应用 \"log-derivative trick\" (即 )： \r\n重新整理： \r\n将分数形式转换为对数梯度形式： \r\n这个最终的表达式 \r\n是梯度的一个期望形式 。\r\n\r\n梯度估计\r\n在实践中，我们无法遍历所有可能的轨迹。因此，我们通过从当前策略  采样  条轨迹，来近似计算梯度：\r\n\r\n这个估计是无偏的，即 。\r\n直观理解\r\n策略梯度的直观含义是：\r\n\r\n对于回报 \r\n为正的轨迹，我们增大概率 ，即增大 。\r\n对于回报 \r\n为负的轨迹，我们减小概率 ，即减小 。\r\n\r\n这种方法即使在回报函数 \r\n不连续或未知的情况下也有效。\r\n二、时间分解 (Temporal\r\ndecomposition)\r\n在上一步中，策略梯度是基于整条轨迹的概率 \r\n来计算的。为了让这个表达式更易于计算，特别是为了消除对环境动态模型（dynamics\r\nmodel）的依赖，我们将其分解到每个时间步。\r\n分解路径概率的对数梯度\r\n\r\n一条轨迹 \r\n的概率是初始状态概率、所有时间步的环境转移概率和策略概率的乘积：  其中 \r\n是环境动态，与策略参数 \r\n无关，而 \r\n是我们的策略，与 \r\n相关。\r\n对其取对数，乘积就变成了求和： \r\n对 \r\n求梯度。由于环境动态  和初始状态概率\r\n 都不依赖于 ，它们的梯度为零。因此，梯度表达式可以简化为：\r\n\r\n将梯度算子移入求和号内： \r\n\r\n最终的策略梯度估计\r\n通过将上述分解后的结果代入我们之前的梯度估计公式，我们得到了一个更实用、无需知道环境动态模型的表达式：\r\n\r\n这个估计仍然是无偏的，并且我们完全可以根据采样的轨迹数据（状态  和动作 ）以及我们的策略模型  来计算它，不再需要环境模型\r\n。\r\n三、基线缩减 (Baseline\r\nsubtraction)\r\n尽管我们推导出的策略梯度估计是无偏的，但它通常具有非常高的方差，这使得训练过程不稳定且效率低下。为了解决这个问题，我们可以引入一个“基线”（Baseline）来降低方差。\r\n引入基线\r\n我们从轨迹的回报中减去一个基线值  。新的梯度估计器形式如下：\r\n\r\n只要基线 \r\n不依赖于动作，这个新的估计器就仍然是无偏的。证明如下：\r\n我们只需证明减去项的期望为零：\r\n\r\n\r\n \r\n利用时间结构\r\n一个重要的观察是：在 \r\n时刻的策略 \r\n只会影响 \r\n时刻及之后的回报，而不会影响过去的回报。因此，我们可以将梯度估计器中的总回报\r\n\r\n替换为“未来回报总和”（reward-to-go），从而进一步降低方差。\r\n结合基线，梯度估计器可以写成：\r\n\r\n这里的基线  可以依赖于状态\r\n 。\r\n基线的选择\r\n选择一个好的基线对于有效降低方差至关重要。常见的选择有：\r\n\r\n常数基线: \r\n最优常数基线: \r\n。\r\n时变基线: \r\n。\r\n状态相关的期望回报 (价值函数):\r\n一个非常有效且常用的基线是状态价值函数  。 \r\n使用价值函数作为基线，意味着我们根据一个动作的回报比当前状态的期望回报好多少，来成比例地增加该动作的对数概率。这时， 被称为优势函数 (Advantage\r\nFunction)。\r\n\r\n四、价值函数估计\r\n(Value function estimation)\r\n为了使用状态价值函数 \r\n作为基线，我们需要一种方法来估计它的值。通常使用一个带参数  的函数逼近器（如神经网络）\r\n来进行估计。主要有两种方法：蒙特卡洛估计和自举法（Bootstrapping）。\r\n1. 蒙特卡洛估计 (Monte Carlo Estimation)\r\n这种方法使用完整的轨迹来获取回报的经验样本作为  的目标值。\r\n\r\n步骤:\r\n\r\n使用当前策略  收集一批轨迹\r\n。\r\n对于每条轨迹中的每一个时间步 ，计算从该点开始的实际累积回报（reward-to-go）:\r\n。\r\n通过最小化预测值 \r\n与实际回报之间的均方误差来更新参数 。\r\n\r\n优化目标: \r\n\r\n2. 自举法/时序差分估计 (Bootstrap Estimation / TD\r\nEstimation)\r\n这种方法不依赖于完整的轨迹，而是使用贝尔曼方程，通过下一个状态的价值估计来更新当前状态的价值。这也被称为“拟合V迭代”（Fitted\r\nV iteration）。\r\n\r\n贝尔曼方程: \r\n步骤:\r\n\r\n使用当前策略收集转换样本数据  (当前状态，动作，下一状态，奖励)。\r\n使用“TD目标值”  作为当前状态价值  的学习目标。这里的 \r\n是用旧参数计算的下一状态的价值估计。\r\n通过最小化“TD误差”  的平方来更新参数\r\n。\r\n\r\n优化目标: \r\n（这里的 \r\n是一个正则化项系数，防止更新步长过大）\r\n\r\n五、优势函数估计\r\n(Advantage Estimation (A2C/A3C/GAE))\r\n策略梯度更新的核心是优势函数 。其中， 是在状态  执行动作  后期望的回报，而  是在状态 \r\n的期望回报。使用蒙特卡洛方法直接从单次交互中估计  值（即使用 \r\n作为估计）会有很高的方差，且不利用样本间的泛化。因此，发展出了更精细的估计方法来在偏差和方差之间做权衡。\r\n这些方法通过使用一个“评论家”（Critic，即价值函数 ）来降低“行动家”（Actor，即策略\r\n）的梯度估计方差。\r\n1. 通过函数逼近降低方差\r\n我们可以用不同的方式来估计Q值，从而得到不同偏差和方差的优势函数估计。\r\n\r\n可以被展开为不同步数（k-step）的回报：\r\n\r\n1步估计: \r\n2步估计: \r\nk步估计: \r\n蒙特卡洛估计 (无穷步): \r\n\r\n2. A2C / A3C (优势行动家-评论家)\r\n异步优势行动家-评论家（Asynchronous Advantage\r\nActor-Critic）算法通常采用k步估计中的一种来计算目标Q值（例如使用5步回报）。这是一种在偏差（使用函数逼近导致）和方差（使用真实回报导致）之间的折中。\r\n3. GAE (泛化优势估计)\r\n泛化优势估计（Generalized Advantage\r\nEstimation）通过引入一个额外的参数  (其中 )，对所有k步估计进行加权平均，从而更灵活地控制偏差和方差的平衡。\r\n\r\nGAE是所有k步回报的 \r\n指数加权平均。\r\n这个思想与 TD(lambda) 和资格迹（eligibility traces）有关。\r\n\r\n使用A3C或GAE的策略梯度算法流程\r\n\r\n初始化策略网络 \r\n和价值网络 。\r\n循环以下步骤：\r\n\r\n收集数据: 在环境中执行当前策略，收集转换数据 。\r\n计算优势函数: 使用收集的数据和当前价值网络  计算Q值的估计 （例如，使用k步回报或GAE），并由此得到优势函数估计\r\n。\r\n更新价值网络: 通过最小化预测值  与目标值 \r\n之间的误差来更新评论家（价值网络）的参数 。\r\n更新策略网络: 使用计算出的优势函数估计  来更新行动家（策略网络）的参数\r\n： \r\n\r\n\r\n","categories":["强化学习"],"tags":["RL"]},{"title":"Deep Q-Learning","url":"/2025/06/17/17-DeepRL_lecture2/","content":"Q-Learning\r\n思路\r\n回顾一下Q值迭代的公式：  将其写为期望的形式： \r\n在此基础上，Q-Learning的思路就是，通过采样来代替期望：\r\n\r\n获得样本 (Receive a sample): 在当前状态  执行动作 。获得新的状态。\r\n旧的估计值 (Old estimate): 我们当前对  的估计值是 。\r\n新的样本估计 (New sample estimate):\r\n它是我们根据刚刚得到的样本，计算出的一个新的、更靠谱的“目标价值”。 \r\n注意，这个式子和第一步公式括号里的内容一模一样，但它使用的 R 和 s′\r\n是我们刚刚实际观测到的，而不是一个理论上的期望值。\r\n增量更新 (Incorporate the new estimate): \r\n这是最终的Q-Learning更新公式。我们不会用新的目标值 \r\n完全覆盖掉旧的估计值，而是像“微调”一样，把旧值向新目标值“拉”一点点。\r\n\r\n\r\nα (alpha) 是学习率 (learning\r\nrate)，一个0到1之间的小数。它控制了我们每一步更新的幅度。\r\n如果 α=0：我们完全不学习，Q值永远不变。\r\n如果\r\nα=1：我们完全抛弃旧值，用新的Target代替，这通常会导致学习过程不稳定。\r\n\r\n流程\r\n算法具体流程如下：\r\n\r\n\r\n\r\nAlgorithm: (算法流程)：\r\n\r\nStart with \r\nfor all \r\n\r\n在算法开始时，我们需要创建一个表格，用来存储所有“状态（state）”和“动作（action）”组合的值。\r\n\r\nGet initial state \r\n\r\n开始一轮学习（一个 episode）。把智能体放在环境中的一个起始位置 。\r\n\r\nFor  till\r\nconvergence\r\n\r\n这是学习的核心循环。智能体会不断地与环境交互，更新表，这个过程会重复成千上万次（是步数计数器)，直到表中的值收敛。\r\n\r\nSample action , get\r\nnext state \r\n\r\n在当前状态，智能体需要选择一个动作来执行。通常使用 -greedy (epsilon-greedy)策略：\r\n\r\n有 \r\n的概率，选择当前状态下值最大的动作（利用现有知识）。\r\n有 \r\n的概率，随机选择一个动作（探索未知可能）。\r\n\r\n执行动作后，环境会给出一个反馈：智能体进入了下一个状态，并获得一个奖励（奖励在下一步的 target\r\n计算中用到）。\r\n\r\nIf  is\r\nterminal:\r\n\r\n\r\n\r\n如果是终止状态，我们的目标值 $target$\r\n就是获得的最后那个奖励 。\r\n\r\nSample new initial state :\r\n当前这轮（episode）结束了，让智能体重新回到一个起始位置，准备开始新的一轮。\r\n\r\nelse:\r\n\r\n\r\n\r\n尚未终止。我们用上面提到的公式来计算目标值。\r\n\r\n\r\n\r\n\r\n这是整个算法最核心的更新步骤。无论 if 还是\r\nelse，我们都用计算出的 $target$\r\n来更新原始状态动作对 \r\n的值。我们用学习率  把旧的值向新的 $target$\r\n值“拉拢”一点。\r\n\r\n\r\n\r\n智能体已经移动到了新的状态。我们把设为当前状态，然后返回第4步，开启下一次“选择动作\r\n-&gt; 观察结果 -&gt; 更新值”的循环。\r\n\r\n\r\n\r\n\r\n特性\r\n核心概念：离策略 (Off-Policy) 学习\r\n\r\n定义： “行动”的策略和“学习”的策略可以不一样。\r\n表现：\r\n即使智能体为了探索而做出“非最优”的随机动作，它学习到的依然是“最优”的路径。\r\n\r\n收敛的3个前提条件 (Caveats):\r\n\r\n充分探索：\r\n必须尝试足够多的状态和动作，避免漏掉最优解。\r\n学习率(α)最终要小：\r\n保证Q值后期能稳定收敛，不剧烈波动。\r\n学习率(α)不能降太快：\r\n防止Q值在学好之前就提前“定型”，不再更新。\r\n\r\n用更严谨的表达：\r\n1. 充分探索 (Infinite Visitation)：\r\n\r\nAll states and actions are visited infinitely often\r\n\r\n含义： 所有的状态和动作对 (s,a)\r\n都必须被访问无限次。\r\n解释：\r\n这是对“充分探索”的严格数学定义。在理论证明中，需要“无限次”来保证无论初始的Q值有多差，最终都有机会被修正过来。在实际应用中，这意味着每个\r\n(s,a) 对都要被访问非常非常多次。\r\n\r\nBasically, in the limit, it doesn't matter how you select actions (!)\r\n\r\n含义：\r\n从极限的角度看，你具体如何选择动作其实不重要！\r\n解释：\r\n这是一个非常深刻的结论。它指的是，只要你的动作选择策略能保证“无限访问”这个条件，那么无论是用ε-greedy，还是其他更复杂的探索方法，最终Q-Learning都能收敛到最优策略。因为Q-Learning更新公式中的\r\nmax\r\n操作会持续地将Q值推向最优的方向，时间长了，探索时产生的“噪音”就会被淹没。\r\n\r\n\r\n2. 学习率设定条件 (Learning Rate Schedule)： \r\n这里用两个数学公式，精确地定义了上面我们讨论的“学习率(α)要变小，但不能太快变小”。这里的\r\n 指的是在时间步  时，用于更新  对的学习率。\r\n\r\n\r\n\r\n数学含义：\r\n所有学习率从头到尾加起来的总和，必须是无穷大。\r\n直观解释：\r\n这个条件保证了学习不会过早停止。你可以把学习率想象成你前进的“步长”。这个公式要求你所有步长加起来的总距离是无限的。这意味着，无论你离目标有多远，只要一直走下去，就一定能到达。如果这个总和是有限的，比如总共只能走100米，那如果你的目标在101米外，你就永远也走不到了。这对应了“学习率不能降太快”的规则。\r\n\r\n\r\n\r\n数学含义：\r\n所有学习率的平方加起来的总和，必须是一个有限的数（即收敛）。\r\n直观解释：\r\n这个条件保证了学习最终会稳定下来。它要求你的“步长”最终必须趋向于0，并且要足够快地趋向于0。这样，在学习的后期，每次更新的幅度会变得非常小，Q值才不会因为某次随机的、不好的采样而大幅波动，从而能够稳定在最优值附近。这对应了“学习率最终要足够小”的规则。\r\n\r\n\r\n局限\r\n表格方法局限性 点击展开查看\r\n\r\n核心问题：状态空间爆炸 (State Space Explosion)\r\n我们之前讲的Q-Learning，核心是那张 Q表\r\n(Q-Table)。它要求我们为环境中的每一个可能的状态都分配一行来记录Q值。\r\n对于非常简单的环境，这是可行的。但是一旦环境变得复杂，状态的数量会增长得极其恐怖，这个现象也叫\r\n“维度灾难” (Curse of Dimensionality)。\r\n图中例子可以看出，即使在状态是离散的环境中，状态数量也会暴增。在连续状态（比如机器人关节的角度、速度）的环境中，问题甚至更严重。\r\n\r\nIn realistic situations, we cannot possibly learn about every single state!\r\n\r\n在现实复杂问题中，我们不可能学习完所有状态。原因有二：\r\n\r\n没时间访问所有状态\r\n(Too many states to visit):\r\n在训练中，要把像Atari游戏里的每一种可能的游戏画面都经历一遍，需要的时间是天文数字。\r\n没内存存储所有状态\r\n(Too many states to hold):\r\n就算有时间，我们的计算机也存不下一张包含万亿、亿亿行的Q表。\r\n\r\n\r\n为了解决状态空间爆炸问题，核心思想是进行泛化(Generalization)。\r\n做法：\r\n\r\n从少量经验中学习\r\n(Learn about some small number of training states):\r\n智能体只从它在训练中实际遇到过的那些状态中学习。\r\n将经验推广到新情况\r\n(Generalize that experience to new, similar situations):\r\n这是最关键的一步。模型需要能够根据已有的经验，对从未见过但很相似的状态做出合理的价值判断。\r\n\r\n优化\r\n近似Q学习（Approximate Q-Learning)：\r\n我们不再使用一张巨大的Q表来存储每一个状态-动作对的值，而是使用一个带有参数的函数\r\n\r\n来计算（或近似）这个值。\r\n\r\n函数可以是多种形式：\r\n\r\n线性函数 (Linear function in features):\r\n\r\n\r\n\r\n神经网络 (neural net), 决策树 (decision tree), etc.\r\n\r\n\r\n新的学习规则 (Learning rule)：\r\n既然没有表格了，我们的更新方式也变了。我们不再更新表格中的一个单元格，而是更新函数的参数\r\nθ。这个更新过程使用的是梯度下降法 (Gradient\r\nDescent)。\r\n\r\n目标值 (target) 的计算:\r\n\r\n\r\n\r\n更新参数 θ:\r\n\r\n\r\n我们来分解这个公式：\r\n\r\n误差 (Error): \r\n是我们当前函数的预测值与目标值之间的差距。\r\n损失函数 (Loss Function): 我们将误差平方\r\n(error)²，这得到了一个永远为正的“损失值”。我们的目标就是让这个损失值变得尽可能小。\r\n梯度 (∇):\r\n这是微积分中的一个符号，表示“梯度”。梯度指向的是函数值增长最快的方向。所以，损失函数的梯度就指向了让损失值增大的方向。\r\n梯度下降:\r\n我们在梯度的前面放一个负号\r\n(- α∇...)，就意味着我们要让参数 θ\r\n沿着损失值下降最快的方向进行调整。α\r\n是学习率，控制着每一步调整的幅度大小。\r\n\r\n\r\n\r\nDeep Learning / Neural\r\nNetworks\r\n\r\n神经网络\r\n简单来说，神经网络通过将许多简单的计算单元（神经元）分层排列，来处理和转换信息。\r\n\r\n输入特征 (f1,f2,f3)\r\n\r\n这是网络的输入层，代表了提供给模型的信息。在强化学习中，这可能就是描述当前状态的特征。\r\n\r\n箭头和线条粗细\r\n\r\n箭头表示数据的流动方向，从左到右。\r\n线条的粗细代表了连接的权重\r\n(Weight)。粗线条表示这个连接的权重值很大，意味着前一层这个节点对后一层节点的影响力更强。这些权重就是神经网络需要学习的参数\r\nW 或 θ。\r\n\r\n计算单元（方框内的符号）\r\n\r\n求和框 (Σ):\r\n代表线性变换。每个神经元会把它所有输入信号根据连接的权重进行加权求和。\r\n激活函数框 (S形曲线):\r\n代表非线性激活。上一步加权求和的结果会经过一个非线性函数（如图中的S形函数，或上一页的ReLU函数）进行处理。这一步至关重要，它赋予了网络学习复杂模式的能力。\r\n\r\n\r\n分层结构\r\n\r\n隐藏层 (Hidden Layers):\r\n\r\n输入层和输出层之间的所有层都叫隐藏层。数据从输入层开始，逐层向后传递。\r\n每一层都会对前一层输出的结果进行新的“加权求和 +\r\n激活”计算，从而提取出更高级、更抽象的特征。第一层可能只能识别简单的模式，而更深的层可以组合这些简单模式，以识别更复杂的概念。\r\n\r\n输出层 (Output Layer):\r\n\r\n网络的最后一层，它会给出最终的计算结果。\r\n\r\n\r\n多层感知机 (MLP)\r\n\r\nMLP的关键在于，它在每一层线性变换之后，都会插入一个非线性的激活函数\r\n(activation\r\nfunction)。这个“非线性”是赋予神经网络强大能力的核心。\r\n激活函数：\r\n\r\n一个直观的MLP结构：\r\n\r\n卷积神经网络(CNN)\r\n\r\nCNN的核心思想\r\n普通的MLP（多层感知机）在处理图像时，会先把图像像素拉成一个长长的一维向量，这会破坏图像固有的空间结构（比如，哪些像素是相邻的）。\r\nCNN专门为处理像图像这样的网格状数据而设计。它不关注单个像素，而是通过“卷积核”来观察局部区域，寻找并识别像边缘、角点、纹理这样的局部模式。\r\n这张图展示了一个典型的CNN架构，从左到右处理一张汽车图片：\r\n\r\n输入 (Input):\r\n\r\n左侧的汽车图片。\r\n\r\n特征提取部分 (多个CONV RELU\r\nPOOL模块):\r\n\r\n这一长串的模块是CNN的精髓，负责从图像中逐层提取特征。\r\nCONV (卷积层):\r\n\r\n这是CNN的核心。它使用许多小的“滤波器”或“卷积核”（可以想象成一个个小小的放大镜）在整个图像上滑动。\r\n每个滤波器专门负责检测一种特定的局部特征。\r\n卷积层处理完后，会输出一系列“特征图”（Feature\r\nMaps），也就是图中那些小小的黑白图片。每一张特征图都显示了它所对应的那个特定特征在原图的哪些位置出现了。\r\n\r\nRELU (激活函数):\r\n\r\n和我们之前讨论的一样，它在卷积操作后增加非线性，增强网络的表达能力。\r\n\r\nPOOL (池化层):\r\n\r\n主要作用是降采样或压缩特征图的尺寸。最常见的是“最大池化”（Max\r\nPooling），它在一个小窗口（比如2x2区域）内只保留最大的那个值。\r\n好处： 1. 减少计算量； 2.\r\n让模型对特征在图像中的微小位置变化不那么敏感，增强模型的鲁棒性。\r\n\r\n\r\n分类部分 (FC层):\r\n\r\nFC (全连接层 - Fully Connected Layer):\r\n\r\n在经过多轮“卷积-激活-池化”的特征提取后，最终得到的复杂的特征图会被“压平”成一个长长的一维向量。\r\n这个向量随后被送入一个我们已经熟悉了的普通MLP（全连接层就是MLP）中。\r\n这个MLP负责根据提取出的高级特征，进行最终的分类或决策。\r\n\r\n\r\n输出 (Output):\r\n\r\n网络的最后一层输出对每个类别的评分。图中，\"car\"（汽车）这个类别的得分最高，因此网络正确地识别出了图片内容。\r\n\r\n\r\n优化 点击展开查看\r\n好的，这张图讲解的是优化（Optimization），即在实际中我们如何调整神经网络的参数（θ）来让它的表现变得更好。\r\n1. 这是一个非凸问题 (Non-convex problem)\r\n\r\n含义：\r\n神经网络的损失函数（我们要最小化的那个误差函数）通常是一个非凸函数。\r\n直观理解：\r\n\r\n凸问题：像一个光滑的碗。无论你从碗的哪里开始，只要一直往下走，最终总能到达唯一的最低点（全局最优解）。\r\n非凸问题：像一个坑坑洼洼、有许多山谷和山峰的复杂地形。你很容易顺着坡走到某一个山谷的谷底（一个局部最优解），但这个谷底不一定是整个地形的最低点。\r\n\r\nGradient-based methods are surprisingly effective\r\n\r\n含义：\r\n尽管理论上存在会卡在“局部最优”的风险，但在实践中，基于梯度的方法（比如我们之前讨论的梯度下降）在训练大型神经网络时出奇地有效。这至今仍然是深度学习研究中的一个热门话题。\r\n\r\n\r\n2. 如何计算梯度 (How to Compute Gradients)\r\n\r\nMinibatch stochastic gradients instead of full gradient\r\n\r\n含义：\r\n我们不使用整个数据集来计算梯度，而是使用“小批量随机梯度”。\r\n对比三种方式：\r\n\r\nFull Gradient (批量梯度下降):\r\n用全部训练数据计算一次梯度。优点是方向准，缺点是数据量大时计算极其缓慢，内存开销大。\r\nStochastic Gradient (随机梯度下降):\r\n随机选一个数据点计算一次梯度。优点是速度飞快，缺点是方向很不稳定。\r\nMinibatch Gradient (小批量梯度下降):\r\n这是两者的折中，也是最常用的方法。我们一次用一小批数据（如32,\r\n64,\r\n256个样本）来计算梯度。这样既保证了计算效率，又使得梯度的方向相对稳定。\r\n\r\n\r\n\r\n3. 梯度的自动计算 (Automatic Differentiation)\r\n\r\nGradient calculation done by Automatic differentiation\r\n\r\n含义：\r\n像神经网络这么复杂的函数，它的梯度公式手算是完全不现实的。幸运的是，计算机可以帮我们自动完成。\r\n\r\nLearn backpropagation (学习反向传播算法)\r\n\r\n反向传播 (Backpropagation)\r\n是实现自动微分的核心算法。它利用微积分中的“链式法则”，从网络的输出层开始，一层一层地向后计算出损失函数相对于网络中每一个参数（权重）的梯度。它是训练神经网络的引擎。\r\n\r\nUse a deep learning framework...\r\n\r\n含义：\r\n在实践中，我们不需要自己从零开始写反向传播算法。像\r\nTensorflow, PyTorch\r\n这样的深度学习框架已经内置了自动微分功能。我们只需要负责搭建神经网络（前向传播），框架会自动帮我们计算所有梯度。\r\n\r\n\r\n4. 最常用的优化器 (Most Common Optimizers)\r\n\r\nSGD + Momentum + Preconditioning\r\n\r\n含义：\r\n朴素的（小批量）随机梯度下降法（SGD）虽然能用，但通常我们会使用一些更高级的“优化器”来加速和稳定训练过程。\r\nMomentum (动量):\r\n\r\n思想：\r\n模拟一个从山上滚下来的球。这个球不仅受当前坡度的影响，还带有自身的惯性。这可以帮助它冲过一些小的颠簸（局部最优），并且在坡度方向一致的区域加速下降。\r\n\r\nPreconditioning (或称 Adaptive Learning Rates, 自适应学习率):\r\n\r\n思想：\r\n为网络中的每一个参数设置不同的、可自适应调整的学习率。比如，对于不经常更新的参数，我们给一个较大的学习率；对于频繁更新的参数，我们给一个较小的学习率。\r\n\r\n\r\nTry RMSProp, Adam, Adamax...\r\n\r\n这些就是集成了上述思想的流行优化器算法的名字。\r\nAdam (Adaptive Moment Estimation)\r\n是目前最常用、最受欢迎的优化器之一，因为它同时结合了动量和自适应学习率的优点，通常能快速且稳定地得到好结果。在不确定用什么优化器时，Adam通常是一个优秀的首选项。\r\n\r\n\r\n\r\nDeep Q Networks(DQN)\r\n\r\n流程\r\n\r\n好的，上图展示的是则是DQN训练算法的流程。\r\n这个算法的核心是在我们之前讨论的近似Q学习的基础上，加入了两个关键的创新点：经验回放\r\n(Experience Replay) 和 固定Q目标 (Fixed\r\nQ-Targets)，也叫目标网络。\r\n初始化阶段\r\n\r\nInitialize replay memory D to capacity N\r\n\r\n含义：\r\n初始化一个固定容量为N的“经验回放池”\r\nD。这个N可以很大，比如100万。\r\n作用 (经验回放):\r\n这是DQN的第一个关键创新。智能体不再是“现学现卖”（用完一个经验就扔掉），而是把经历过的所有事情（状态、动作、奖励等）都存到这个池子里。学习的时候，随机从池子里抽取一小批经验来学习。\r\n\r\nInitialize action-value function Q with random weights θ\r\n\r\n含义：\r\n初始化我们主要的Q网络，也叫“在线网络”（Online\r\nNetwork），其参数为 θ。\r\n\r\nInitialize target action-value function Q_hat with weights θ⁻ = θ\r\n\r\n含义：\r\n创建一个结构完全相同，参数也完全一样的“目标网络”（Target\r\nNetwork）。\r\n作用 (固定Q目标):\r\n这是DQN的第二个关键创新。在之前的算法中，计算目标值y和计算预测值都用的是同一个网络Q，这相当于“追逐一个移动的目标”，会导致训练非常不稳定。DQN用这个独立的、参数被“冻结”的目标网络来计算目标值，使得学习目标在一段时间内是固定的，从而大大增加了训练的稳定性。\r\n\r\n\r\n主训练循环\r\n\r\nFor episode = 1, M do /\r\nFor t = 1, T do:\r\n对每一episode中的每一步进行循环。\r\n与环境交互\r\n\r\n选择动作:\r\n使用在线网络Q和ε-greedy策略选择一个动作。\r\n执行动作: 在环境中执行动作，得到奖励和下一帧图像。\r\n存储经验: 将这次的完整经验\r\n(当前状态, 动作, 奖励, 下一状态)\r\n存入经验回放池D中。\r\n\r\n从经验中学习\r\n\r\n采样:\r\n从经验池D中随机抽取一小批（minibatch）经验。\r\n计算目标值y:对抽样出的每一条经验，使用目标网络\r\n来计算目标值  。\r\n\r\n⁻\r\n注意： 这里用的是参数为 θ⁻\r\n的目标网络，它的参数是旧的、被冻结的，所以y_j的值是稳定的。\r\n\r\n执行梯度下降:\r\n\r\n计算“目标值”和“在线网络Q的预测值”之间的误差。\r\n根据这个误差，计算梯度并更新在线网络的参数θ。\r\n\r\n\r\n更新目标网络 \r\n\r\nEvery C steps reset Q_hat = Q:\r\n每隔C个训练步（比如10000步），才把在线网络的最新参数θ复制给目标网络，更新θ⁻。这确保了目标网络在大部分时间内是稳定的，只进行周期性的更新。\r\n\r\n\r\n细节\r\n\r\n1. 使用Huber损失函数 (Huber Loss)\r\nDQN在计算误差时，没有使用我们之前提到的简单平方损失\r\n(error)²，而是使用了Huber损失。\r\n\r\n它是什么？\r\nHuber损失是一个“混合”损失函数，结合了平方损失和绝对值损失的优点。\r\n\r\n当误差a（即\r\ny - Q(s,a)）很小时（∣a∣≤δ），它的计算方式和平方损失一样。\r\n当误差a很大时（∣a∣&gt;δ），它的计算方式变为线性增长，类似于绝对值损失。\r\n\r\n为什么使用它？\r\n在强化学习的初期，Q网络的预测可能非常不准，导致计算出的误差a非常大。如果使用平方损失，这个巨大的误差会被平方，导致一个极其夸张的梯度，可能使网络更新“跑偏”，让训练变得非常不稳定。Huber损失对这种大的“离谱”误差惩罚更温和（线性增长而非平方增长），从而让训练过程对异常值不那么敏感，整体更加稳定、鲁棒。\r\n\r\n2. 使用RMSProp优化器\r\nDQN使用了RMSProp优化器，而不是基础的随机梯度下降（vanilla\r\nSGD）。\r\n\r\n它是什么？\r\nRMSProp是一种自适应学习率的优化器。它会为网络中的每一个参数独立地调整学习率。\r\n为什么使用它？\r\n选择一个合适的优化器对强化学习至关重要。像RMSProp这样的自适应优化器通常比基础的SGD收敛得更快、更稳定，能更好地处理强化学习中奖励稀疏、梯度变化大的问题。\r\n\r\n3. 探索率退火 (Anneal the exploration rate)\r\n这里描述了DQN中 ε-greedy\r\n策略的ε值是如何随时间变化的，这个过程叫做“退火”。\r\n\r\n它是什么？\r\n“退火”意味着ε的值会随着训练的进行而逐渐减小。\r\n具体策略：\r\n\r\n训练开始时，设置 ε = 1。\r\n在训练的前一百万步内，将ε的值线性地从1逐渐降低到一个很小的值，比如0.1或0.05。\r\n之后，ε保持在这个小值上。\r\n\r\n为什么这么做？\r\n这是一种经典的“先探索，后利用”的策略。\r\n\r\n初期 (ε = 1)：\r\n此时智能体对环境一无所知，Q值是随机的。设置ε = 1意味着它会完全随机地行动，从而最大化地探索环境，收集多样化的经验。\r\n后期 (ε = 0.1):\r\n随着学习的进行，智能体的Q函数越来越准确。降低ε的值，使得它更多地利用已学到的知识来选择最优动作，而不是随机乱撞。保留一个很小的ε值是为了确保智能体永不停止探索，以防当前策略并非完美。\r\n\r\n\r\nDQN示例-ATARI 点击展开查看\r\n\r\n\r\n\r\n改进\r\n1. 双重深度Q网络 (Double DQN, DDQN)\r\n\r\nDQN的问题：过度估计 (Overestimation)\r\n\r\nThere is an upward bias in max Q(s, a; θ)\r\n\r\n问题：\r\n标准的DQN算法在计算目标值时，存在一个系统性的“过度估计偏差”。也就是说，它倾向于高估动作的Q值。\r\n原因： 在计算目标值 ⁻\r\n时，max操作会选择并评估下一状态中Q值最大的那个动作。如果某个动作的Q值因为随机误差而被偶然高估了，max操作就会选中它，导致这个被高估的值被用于计算目标y。这种机制会系统性地将Q值越推越高，使其偏离真实值，影响策略的质量。\r\n\r\n\r\nDouble DQN的解决方案：解耦“选择”与“评估”\r\nDouble\r\nDQN的核心思想非常巧妙：将“选择最佳动作”和“评估该动作的价值”这两个步骤分离开，使用不同的网络来执行。\r\n\r\nθ for selecting the best action\r\n\r\n使用在线网络 (Online\r\nNetwork)，也就是参数为θ的那个网络，来选择下一状态s'中哪个动作是最好的。\r\n\r\nθ⁻ for evaluating the best action\r\n\r\n使用目标网络 (Target\r\nNetwork)，也就是参数为θ⁻的那个网络，来评估上一步选出的那个动作的价值究竟是多少。\r\n\r\n\r\n这样一来，即使在线网络θ对某个动作的价值产生了高估并选中了它，我们仍然会用更稳定、独立的的目标网络θ⁻来给出它的价值。因为两个网络产生相同高估的可能性很小，所以整体的过度估计偏差就被大大降低了。\r\nDouble DQN的损失函数 (The Loss)\r\n目标值y的计算公式： \r\n损失函数则是通过计算目标值和预测值的差距获得的：目标值预测值\r\n2. 优先经验回放 (Prioritized Experience Replay,\r\nPER)\r\n\r\n\r\nReplaying all transitions with equal probability is highly\r\nsuboptimal.\r\n\r\n标准的经验回放机制（从经验池中完全随机地抽样）是次优的。\r\n\r\nReplay transitions in proportion to absolute Bellman error\r\n\r\n核心思想：\r\n我们应该优先回放那些能让我们学到最多东西的经历。\r\n如何衡量“学习价值”？\r\nPER使用TD误差（TD-error）的大小来衡量一次经历的“意外程度”或“学习价值”。\r\n目标值预测值 直观理解：\r\n\r\nTD误差大：\r\n意味着“预测”和“实际结果”差距很大。这说明经历非常“出乎意料”，智能体能从中学到很多，因此赋予它很高的优先级。\r\nTD误差小：\r\n意味着“预测”和“实际结果”基本相符。这说明经历在预料之中，能学到的东西不多，因此赋予它较低的优先级。\r\n\r\n具体做法：\r\n在从经验池中抽样时，不再是完全随机，而是根据这个优先级来抽样，优先级越高的经历被抽中的概率越大。\r\nPER会使学习效率大幅提升。\r\n","categories":["强化学习"],"tags":["RL"]},{"title":"TRPO and PPO","url":"/2025/07/14/19-DeepRL_lecture4/","content":"一、代理损失 (Surrogate Loss)\r\n代理损失（Surrogate Loss）来源于重要性采样（Importance\r\nSampling）的推导：\r\n\r\n对其求梯度：\r\n\r\n在 \r\n处：\r\n\r\n该推导表明，我们可以使用代理目标（surrogate\r\nobjective）而不仅仅是梯度，从而设计更有效的优化策略。\r\n\r\n二、Step-Sizing\r\n和 Trust Region Policy Optimization (TRPO)\r\n步长问题\r\n强化学习中的步长控制尤为关键：\r\n\r\n在监督学习中，步长太大可以通过下次迭代纠正；\r\n在强化学习中，步长太大会导致策略恶化；\r\n接下来采样的数据来自这个“糟糕策略”，无法轻易纠正；\r\n常规解决方式是缩小步长，增加稳健性。\r\n\r\n简单步长控制方法\r\n\r\n使用梯度方向的线搜索；\r\n虽然简单，但代价高且不考虑一阶近似的适用区域。\r\n\r\n信赖域策略优化（Trust\r\nRegion Policy Optimization, TRPO）\r\n目标函数：\r\n\r\n约束条件：\r\n\r\nKL 散度的评估\r\n轨迹分布为：\r\n\r\n由于环境动态在正反比中相互抵消，KL 散度简化为：\r\n\r\nTRPO 的实践效果良好，如应用于：\r\n\r\nLocomotion 学习任务（如 Hopper, Walker）\r\nAtari 游戏（如 Pong, Beamrider）\r\nGAE（Generalized Advantage Estimation）与 TRPO 搭配使用\r\n\r\n\r\n三、Proximal Policy\r\nOptimization (PPO)\r\nTRPO 的局限性\r\n\r\n难以在复杂策略架构中强制执行信赖域约束；\r\n如：包含 dropout 的网络结构，或策略与价值函数共享参数；\r\n共轭梯度方法实现复杂；\r\n不利于使用诸如 Adam、RMSProp 等优秀的一阶优化器。\r\n\r\nPPO 版本 1 —— “Dual Descent\r\nTRPO”\r\n\r\n基于对偶下降法（dual descent）对  进行更新；\r\n虽然有效，但仍显复杂。\r\n\r\nPPO 版本 2 —— “Clipped\r\nSurrogate Loss”\r\n定义：\r\n\r\n目标函数为：\r\n\r\n特点：\r\n\r\n无需约束 KL；\r\n简单易实现；\r\n可以与常规的一阶优化器（如 Adam）配合；\r\n实际表现优异，被广泛用于复杂任务中。\r\n\r\nPPO 的应用实例\r\n\r\nOpenAI Five（Dota 2 游戏 AI）\r\nIn-Hand 物体重定向（OpenAI）\r\n魔方机器人手重构（OpenAI Rubik's Cube）\r\n\r\n\r\n","categories":["强化学习"],"tags":["RL"]},{"title":"DDPG and SAC","url":"/2025/07/14/20-DeepRL_lecture5/","content":"一、Deep\r\nDeterministic Policy Gradient (DDPG)\r\n基本流程：\r\n\r\nRoll-outs\r\n在当前策略下执行\r\nroll-out（执行策略并采样轨迹），加入一些噪声用于探索。\r\nQ 函数更新\r\n更新 Q 函数的目标值为：\r\n\r\n最小化以下损失函数以更新 Q：\r\n\r\n策略更新\r\n通过反向传播 Q 函数来估计策略梯度并更新策略参数：\r\n\r\n技术细节：\r\n\r\n添加噪声促进探索\r\n\r\n使用 replay buffer 和 target network（从 DQN\r\n中借鉴）以提升稳定性\r\n\r\n目标值使用 Polyak-averaging 的滞后版本的  和  进行计算\r\n\r\n\r\n总结：\r\n\r\n优点：由于是 off-policy，采样效率高\r\n\r\n缺点：容易不稳定\r\n\r\n这也引出了后续改进的算法 ——\r\nSAC，它通过在目标中加入策略熵，提升探索能力并减少策略对 Q\r\n函数偏差的过拟合。\r\n\r\n二、Soft Actor Critic (SAC)\r\nSoft Policy Iteration：\r\nSAC 的理论基础是 Soft Policy Iteration，包括以下步骤：\r\n\r\nSoft policy evaluation（策略评估）\r\n固定策略，应用 soft Bellman backup 直到收敛：\r\n\r\nSoft policy improvement（策略改进）\r\n通过信息投影来更新策略：\r\n最小化策略与 soft Q 的 KL 散度\r\n\r\n重复执行以上两步，直到收敛\r\n对于新的策略，有：\r\n\r\n\r\nSAC 算法流程：\r\n\r\n目标函数：最大化 Q 值与策略熵的组合\r\n\r\n重复以下操作：\r\n\r\n从当前策略 \r\n中执行一个动作，与环境交互，并将数据添加到 replay buffer\r\n\r\n使用采样数据来学习 V、Q 和策略 \r\n\r\n对 V 使用 soft Bellman residual 最小化\r\n\r\n对策略使用最小化 KL 散度的方式进行更新\r\n\r\n\r\n\r\n\r\n实验效果\r\nSAC 在多个真实机器人实验中表现出良好效果，如图示：\r\n\r\n\r\n\r\n总结\r\n\r\nDDPG：Deterministic 策略 + Off-policy +\r\n高采样效率，但稳定性差\r\n\r\nSAC：Stochastic 策略 + Off-policy +\r\n加入策略熵提升探索性和鲁棒性，是对 DDPG 的改进\r\n\r\n","categories":["强化学习"],"tags":["RL"]},{"title":"MDPs and Exact Solution Methods","url":"/2025/06/16/16-DeepRL_lecture1/","content":"一、马尔科夫决策过程(MDPS)\r\n\r\nMDP定义：\r\n\r\n：智能体的状态集合。\r\n：智能体可以选择的行动集合。\r\n：转变函数，定义智能体当前状态为，采取动作，进入状态的概率。\r\n：奖励函数，为上述转变分配奖励。\r\n：初始状态。\r\n：折扣因子，对未来奖励的折扣。\r\n：范围，智能体行动的时间。\r\n\r\nMDP求解：\r\n在一个MDP中，我们想要找到一个最优的策略(policy) \r\n\r\n对于一个给定的状态和时间，策略  给出一个行动。\r\n最优策略指能使奖励的期望最大的策略。\r\n\r\n如果环境是确定的，那么只需要一个最优的plan，即一系列的行动，从起始点到终点。\r\n二、精确求解方法\r\n\r\n目标：最大化随时间累计，折扣后奖励总和的期望值。\r\n（一）、值迭代\r\n最优值方程： \r\n从状态 \r\n出发并采取最优行动时的折扣奖励总和。\r\n示例 点击展开查看\r\n\r\n算法：\r\n\r\n示例 点击展开查看\r\n注意这个机器人朝向，三个箭头说明一次行动中只有三个可能的方向（这也是在笔者和室友后期验算时，发现怎么算怎么不对，最后才定位出的问题）。在agent的一次action中，它会根据最优的action选定一个方向，有0.8的概率按照最优方向行动，有0.1的概率偏向最优方向相对的左侧和右侧。\r\n\r\n第0次迭代时，所有的都被赋为0。\r\n\r\n第1次迭代时，有奖励的状态被赋值为相应的奖励值（按照公式计算不知道咋算，按照示例来看应该直接赋值就好）。\r\n\r\n第2次迭代时，则根据上一次迭代的值进行计算（奖励和惩罚区域始终保持不变）：\r\n以为例（其余相邻都是0，无需计算，惩罚区域无法跳出），我们采取的最佳action肯定是往右：\r\n\r\n需要注意，由于最佳action是向右，所以noise产生的偏差应该是向上或者向下，分别有0.1的概率，而向上撞墙会保持原地。\r\n关于reward，按照例子来看，每一步的奖励始终为0。\r\n\r\n第3次迭代时，需要根据公式更新相邻格：  处的最优方向向右，处的最优方向向上。相应更新迭代值即可。\r\n\r\n第4次迭代计算过程省略了。只需要记住最优action的方向和可能偏移的方向即可。\r\n\r\n第100次迭代时，值迭代已趋近收敛。\r\n收敛速度往往与折扣因子有关，折扣因子越趋近0，收敛速度越快。\r\n\r\n理论：\r\n值迭代总是会收敛。在收敛处，我们能获得带折扣因子的无限期问题的最优值函数，它满足贝尔曼(Bellman)方程：  （感觉有点小问题，第一个应该是小写，第一个应该是小写？同时，这里的应该也是概率函数？）\r\n为了获取最优的行动，只需要根据下面步骤执行：\r\n\r\n执行值迭代过程，直到收敛。\r\n获得，通过反推下一步行动： \r\n\r\n无限期策略是稳定的，状态下的最优行动在任何时间都是相同的。\r\n直觉：\r\n\r\n随着范围的增大，步之后能获取的奖励和随之缩减，最终趋于0，因此最终趋于。\r\n\r\n定义：Max-Norm（最大范数） \r\n解释：这是最大范数（max-norm，也叫∞-norm）的定义。它表示向量\r\n 中所有状态  对应的值  的最大绝对值。\r\n\r\n定义：γ-压缩映射（γ-Contraction）\r\n一个更新操作是 max-norm 下的 γ-压缩映射，当且仅当：\r\n\r\n解释：这是“压缩映射”的定义。它说明更新操作会将任意两个输入的差异（在\r\nmax-norm 下）收缩最多 γ 倍。如果 ，我们称这个操作是“收缩的”。\r\n\r\n定理：压缩映射必收敛到唯一不动点\r\nBanach Fixed Point Theorem（巴拿赫不动点定理）\r\n如果一个映射是压缩映射（在完备空间中），则存在唯一不动点，并且从任意初始点迭代最终都收敛于它。\r\n\r\n事实：值迭代是一个 γ-压缩映射\r\n令为Bellman更新操作，即：\r\n\r\n我们可以证明：\r\n\r\n\r\n推论：值迭代收敛到唯一固定点\r\n因为值迭代是 γ-压缩映射，根据不动点定理，值函数会收敛到唯一固定点 (\r\n )。\r\n\r\n补充事实：误差界估计（用于停止条件）\r\n若满足：\r\n\r\n则有：\r\n\r\n解释：这个不等式用于衡量我们与最优值函数  的距离。\r\n\r\n意思是：如果连续两次更新的差距已经很小（小于 ε）了，那么我们离最终解\r\n 也不会太远。\r\n\r\n\r\n（二）、策略迭代\r\n不同于值衡量状态的值，值衡量的是一个状态下的行动的价值。\r\n表示从状态出发执行动作，并之后采取最优策略时的期望总回报。\r\n最优 Q 值的贝尔曼方程：  含义解释：\r\n\r\n外层是对所有可能下一状态 \r\n的期望；\r\n：从  执行动作  到达  的概率；\r\n：即时奖励；\r\n：折扣因子；\r\n：表示在下一个状态  采取最优动作的最大回报。\r\n\r\n这是“先拿奖励，再考虑未来的最优行为”的形式。\r\nQ值迭代：  这是值迭代的 Q 值形式，含义是：\r\n\r\n每一步更新 Q 值时，使用当前的 \r\n来近似未来的最大期望回报，重复迭代直到收敛为 。\r\n\r\n示例 点击展开查看\r\n\r\n策略评估：\r\n回顾上面的值迭代：  对于给定策略，策略评估如下：  收敛时有如下公式： \r\n在此基础上，我们可以建立一个算法，对其进行评估，用评估结果再优化策略，这就是策略迭代的大致思路。\r\n\r\n策略迭代：\r\n策略迭代由两大步骤组成，并反复进行直到策略收敛：\r\n1. 策略评估（Policy Evaluation）\r\n给定当前策略 ，我们要计算它的状态值函数 。 \r\n\r\n不断迭代直到 \r\n收敛；\r\n\r\n2. 策略改进（Policy Improvement）\r\n用当前的值函数 \r\n来生成一个更好的策略 。\r\n 有效性证明：\r\n这个“评估-提升”的循环是保证有效的，它最终一定会停止，并且停止时找到的就是最优策略和最优价值函数。\r\n\r\n(1) 保证收敛 (Guarantee to converge)\r\n\r\n每次“策略提升”，新策略要么比老策略好，要么和老策略一样好（在已经达到最优的情况下）。它永远不会变得更差。\r\n因为策略总是在变好（或者不变），所以除非已经达到最优，否则你不会重复遇到同一个策略。\r\n\r\n(2) 收敛即最优 (Optimal at convergence)\r\n\r\n算法收敛（停止）的标志是：在“策略提升”那一步，发现新策略和老策略一模一样了，没有任何改进空间了。\r\n这意味着对于每个状态，当前策略 \r\n所选择的动作，本身就已经是那个能最大化“下一步期望价值”的动作了（也就是满足了策略提升\r\n 的条件）。\r\n\r\n\r\n最大熵公式\r\n次优解分布:\r\n传统的RL算法（比如我们上面讨论的Policy\r\nIteration）通常致力于找到一个唯一的、最好的策略 。但这种做法有其局限性。我们退而求其次，寻找一个次优解的分布，有如下好处：\r\n\r\n更鲁棒的策略 (More robust policy)\r\n\r\n\r\n如果环境发生变化，这个次优解的集合中，可能仍然有适用于新情况的好解法。\r\n\r\n\r\n问题：如果你只知道那条唯一的“最快路线”，万一这条路上突然发生了交通事故（环境变化），你就懵了，不知道该怎么办。你的策略瞬间失效。\r\n解决方案：如果你有好几条备选的“优秀路线”，当主路线堵车时，你可以立刻从中选择另一条路绕过去。你的整体策略对意外情况的抵抗能力（即鲁棒性）就大大增强了。\r\n在RL中，一个只学习到最优策略 \r\n的智能体，在环境发生微小变化时（例如，某个关键路径被阻挡），可能会表现得非常糟糕。但如果它掌握了多个次优策略，它就能快速适应，从“备用方案”中拿出一个来执行，表现得更加稳定和可靠。\r\n\r\n\r\n更鲁棒的学习过程 (More robust learning)\r\n\r\n\r\n如果我们能保持一个次优解的集合，我们的智能体在学习过程中就能收集到更有价值的探索数据。\r\n\r\n这主要关乎“探索与利用 (Exploration-Exploitation)\r\n”的权衡问题。\r\n\r\n问题：智能体为了学习，不能只执行它当前认为最好的动作（利用），也需要尝试一些未知的动作来发现可能更好的新策略（探索）。但怎么探索是个大学问。\r\n解决方案：如果智能体心中有好几个“优秀解法”，它的探索就可以是“在这些优秀解法之间进行尝试”。\r\n通过这种方式，智能体收集到的数据都是来自于“高水平尝试”，数据质量非常高。这可以帮助它学得更快、更稳定，并且避免过早地锁定在某个局部最优解上，从而更容易找到全局最优解。\r\n\r\n熵：\r\n定义一：熵是随机变量X不确定性的度量 (measure of uncertainty)。\r\n定义二：熵是（平均而言）编码随机变量X所需要的比特数 (number of bits\r\nrequired to encode X)。\r\n数学公式: \r\n\r\n：代表整个随机事件”。\r\n：代表一个具体的可能结果。\r\n：代表结果 xi\r\n发生的概率。\r\n：它代表了单个结果\r\n 发生时所包含的“信息量”。\r\n\r\n如果一个事件概率很高，p(xi)→1，那么\r\nlog2p(xi)→0。这件事发生不意外，信息量为0。\r\n如果一个事件概率很低，p(xi)→0，那么 log2p(xi)\r\n是个很大的负数，所以 −log2p(xi)\r\n就是个很大的正数。这件事的发生非常意外，信息量巨大！\r\n\r\n∑ip(xi)…：最后，我们把每个可能结果的“信息量”乘以它自己发生的“概率”，然后全部加起来。这其实就是在计算整个事件X的“平均信息量”或“平均意外程度”，也就是熵。\r\n\r\n最大熵MDP：\r\n\r\n在常规的MDP公式的基础上，引入了熵的参数。新引入的内容会观察策略的熵。\r\n是权衡因子，用于平衡奖励和熵。\r\n约束优化：\r\n\r\n在约束优化中，我们会在一些约束条件下求解最值问题。比如在的条件下求的最大值。\r\n为了求解，我们引入拉格朗日函数，并改为求如下值：  在最优处时，满足如下条件：  拉格朗日原理概述 点击展开查看拉格朗日原理概述：\r\n\r\n问题设定：\r\n\r\n我们希望最大化或最小化一个目标函数 。\r\n同时，必须满足一个或多个约束条件 。\r\n\r\n拉格朗日函数：\r\n\r\n为了将约束条件引入到优化问题中，我们构造拉格朗日函数：\r\n\r\n这里，\r\n是拉格朗日乘子，用于调整约束的影响。\r\n\r\n求解步骤：\r\n\r\n对拉格朗日函数进行偏微分，得到以下条件：\r\n\r\n\r\n第一个条件指示在优化点上，函数的变化率为零；第二个条件确保约束条件被满足。\r\n\r\n几何解释：\r\n\r\n几何上，优化问题可以看作是在某个函数的等高线（或曲面）上寻找最优点，而约束条件则像是这条等高线与某个边界的交点。拉格朗日乘子帮助我们找到这些交点。\r\n\r\n\r\n\r\n\r\n1. 目标函数\r\n最大化期望奖励和策略熵的加权和。熵代表策略的随机性， 是权衡因子。 \r\n2. 约束条件\r\n策略 \r\n是一个概率分布，所有动作的概率之和必须为1。 \r\n3. 求解方法\r\n这是一个带约束的优化问题，使用拉格朗日乘子法。构造拉格朗日函数\r\n。\r\n 对  的变量\r\n 和 \r\n求偏导并令其为0来寻找最优解。\r\n4. 最终解\r\n最优策略的形式是一个 Softmax\r\n分布。奖励越高的动作，被选中的概率就呈指数级越高。  其中 \r\n是归一化项（也叫配分函数），用来确保所有概率之和为1。 \r\n\r\n紧接上一步，开始计算我们找到的最优策略所对应的最优价值（Optimal\r\nValue, V）。\r\n回顾价值公式：\r\n接着，将上一页推导出的最优Softmax策略  代入上方的价值公式中。\r\n化简得到最终结果：\r\n\r\n在此基础上，我们希望把我们之前讨论的单步最大熵解法，推广到了更普遍的多步强化学习问题中。\r\n这个算法是一种基于动态规划的递归方法。在还剩  步时，处于状态  的价值，记为 ，可以被分解为两部分：\r\n\r\n执行一个动作 \r\n后获得的即时价值\r\n进入下一个状态 \r\n后，在剩下 \r\n步中所能获得的价值。\r\n\r\n这就得到了最大熵贝尔曼方程： 这部分就是 这里的 \r\n就是最大熵Q值。它代表了在状态  执行动作 ，并在后续所有步骤中都遵循最优策略所能得到的期望总价值。\r\n既然问题结构相同，我们就可以直接套用单步问题的解，只需把  替换成  即可。\r\n","categories":["强化学习"],"tags":["RL"]},{"title":"Model-based RL","url":"/2025/07/14/21-DeepRL_lecture6/","content":"一、Model-based RL\r\n基础算法\r\n模型基强化学习（Model-Based RL）的基本流程如下：\r\n对于每一次迭代 (iter = 1, 2, ...): 1. 在当前策略下收集数据。 2.\r\n使用过去的数据学习一个动态模型。 3. 利用这个动态模型来改进策略。 *\r\n可以通过在学习到的模型上进行时间反向传播（backprop-through-time）来优化策略。\r\n* 也可以将学习到的模型作为一个模拟器来运行其他强化学习算法。\r\n为什么使用Model-Based RL？\r\n\r\n数据效率：从数据中学习模型，可能比单纯的策略梯度更新带来更显著的策略提升。\r\n可重用性：学习到的模型如果足够通用，可以被复用于其他任务。\r\n\r\n为什么不一直使用Model-Based\r\nRL？\r\n尽管它有很高的样本效率（sample efficiency），但也存在一些问题：\r\n\r\n训练不稳定性\r\n无法达到与无模型（model-free）方法相同的渐进性能\r\n\r\nModel-based RL中的过拟合问题\r\n\r\n标准过拟合 (监督学习中)\r\n\r\n神经网络在训练数据上表现良好，但在测试数据上表现不佳。例如，在根据(s,\r\na)预测s_next时。\r\n\r\nModel-based RL中的新挑战\r\n\r\n策略优化倾向于利用那些没有足够数据来训练模型的区域，这会导致灾难性的失败。\r\n这个问题被称为“模型偏差”（model-bias）。\r\n\r\n\r\n二、Robust\r\nModel-based RL:Model-Ensemble TRPO (ME-TRPO)\r\nME-TRPO（模型集成信赖域策略优化）被提出来解决“模型偏差”问题。\r\nVanilla Model-Based Deep RL\r\n算法\r\n这是ME-TRPO的基础对比算法。\r\n\r\n初始化策略  和模型\r\n。\r\n初始化一个空的数据集D。\r\n循环直到策略在真实环境中表现良好：\r\n\r\n使用策略 \r\n从真实环境f中收集样本并添加到D中。\r\n循环直到性能停止提升：\r\n\r\n使用数据集D训练模型 。\r\n使用策略  从模型\r\n\r\n中收集虚拟样本。\r\n在虚拟样本上使用BPTT（Backpropagation Through Time）更新策略。\r\n评估性能 。\r\n\r\n\r\n\r\nME-TRPO 算法\r\n\r\n初始化策略 \r\n和所有模型 。\r\n初始化一个空的数据集D。\r\n循环直到策略在真实环境中表现良好：\r\n\r\n使用策略从真实系统中收集样本并添加到D中。\r\n使用数据集D训练所有模型。\r\n循环直到性能停止提升：\r\n\r\n使用策略 \r\n从模型集合 \r\n中收集虚拟样本。\r\n在虚拟样本上使用TRPO更新策略。（核心区别：使用所有模型进行优化）\r\n评估每个模型下的性能  for , ..., Κ。\r\n\r\n\r\n\r\nME-TRPO 评估\r\n实验环境:\r\n\r\n与SOTA（State of the art）方法的比较:\r\n\r\nME-TRPO在多个环境（Swimmer, Hopper, HalfCheetah, Ant, Humanoid,\r\nSnake）中与PPO, TRPO,\r\nDDPG等先进的无模型方法进行了比较。结果显示，ME-TRPO在样本效率上具有显著优势，能用更少的样本达到甚至超越其他方法的性能。\r\nME-TRPO 消融实验\r\n\r\nTRPO vs. BPTT 在标准Model-based RL中的比较\r\n\r\n\r\n实验对比了在单个模型下，使用TRPO更新策略（MB-TRPO）与使用BPTT更新策略（Vanilla\r\nMBRL）的效果。结果显示，在Snake, HalfCheetah,\r\nAnt环境中，基于TRPO的更新方式性能更优且更稳定。\r\n\r\n集成中学习的动态模型数量\r\n\r\n\r\n实验比较了使用不同数量模型（1, 5, 10,\r\n20）的ME-TRPO性能。结果显示，在HalfCheetah和Ant环境中，使用更多的模型（特别是5个或10个）可以带来更好的性能和稳定性。\r\n\r\n\r\n三、Adaptive\r\nModel-based RL: Model-based Meta-Policy Optimization (MB-MPO)\r\n解决Model-based\r\nRL的渐进性能问题\r\n\r\n问题:\r\n因为学习到的（集成）模型不完美，导致的策略在模拟中表现很好，但在真实世界中并非最优。\r\n尝试的修复方案1:\r\n学习更好的动态模型。事实证明这还不够。\r\n尝试的修复方案2: 通过元策略优化（meta-policy\r\noptimization）进行模型基强化学习，即MB-MPO。\r\n核心思想:\r\n\r\n学习一个能够代表真实世界通常如何运作的模型集成。\r\n学习一个自适应策略，该策略可以快速适应任何一个学习到的模型。\r\n这样一个自适应策略也就能快速地适应真实世界的运作方式。\r\n\r\n\r\nMB-MPO 算法\r\n需要: 内外两层步长 , \r\n\r\n初始化策略 ，模型\r\n ...  以及数据集 。\r\n循环直到策略在真实环境中表现良好:\r\n\r\n使用适应后的策略 \r\n从真实环境中采样轨迹，并添加到D中。\r\n使用D训练所有模型。\r\n对于所有模型 :\r\n\r\n使用  从模型  中采样虚拟轨迹 。\r\n使用轨迹 \r\n计算适应后的参数： \r\n使用适应后的策略  从模型  中采样虚拟轨迹 。\r\n\r\n使用轨迹  更新 ： \r\n\r\n返回最优的更新前参数 。\r\n\r\nMB-MPO 评估\r\n实验环境:\r\n\r\n与SOTA Model-Free方法的比较:\r\n\r\nMB-MPO在多个环境中与acktr, trpo, ppo,\r\nddpg等先进的无模型方法进行比较。结果显示MB-MPO（图中ours）在样本效率和渐进性能上都达到了顶尖水平。\r\n与SOTA Model-Based方法的比较:\r\n\r\nMB-MPO（图中ours)与me-trpo和mb-mpc进行了比较。在Ant, HalfCheetah,\r\nHopper,\r\nWalker2D等环境中，MB-MPO在学习速度和最终性能上均表现出明显优势。\r\n","categories":["强化学习"],"tags":["RL"]},{"title":"PhysGaussian阅读笔记","url":"/2025/12/16/22-PhysGaussian/","content":"\r\n1. 核心思想\r\n这篇论文提出了 PhysGaussian，核心理念是\r\n\"所见即所仿\" (What You See Is What You Simulate, )。它不再需要将 3DGS\r\n转换为三角形网格或四面体网格，而是直接将 3DGS\r\n中的每一个“高斯椭球”视为物理模拟中的一个“粒子”。\r\n\r\n传统流程：物体 -&gt; 转换为网格 -&gt;\r\n物理引擎解算网格形变 -&gt; 渲染引擎根据网格更新画面。\r\n本论文流程：物体 (3DGS) -&gt;\r\n物理引擎直接推算高斯球的位移和形变 -&gt;\r\n渲染引擎直接画出更新后的高斯球。\r\n\r\n\r\n2. 基础概念\r\n\r\n3D Gaussian Splatting (3DGS):\r\n\r\n通俗解释： 一种新的 3D\r\n绘图技术。以前我们用三角形拼出物体（像折纸），现在我们用无数个模糊的椭球体（高斯球）堆叠出物体。每个球有位置、颜色、大小、旋转角度。\r\n优势： 渲染速度极快，画质逼真。\r\n\r\n连续介质力学 (Continuum Mechanics):\r\n\r\n研究物体在受力时如何运动和变形的学科。它把物体看作是连续不断的材料，而不是原子堆积。\r\n关键点：它通过“形变梯度”（Deformation Gradient，记为 ）来描述物体局部是如何被拉伸、旋转或剪切的。\r\n\r\n物质点法 (MPM, Material Point Method):\r\n\r\n核心逻辑：它混合使用了粒子（Lagrangian，随物质移动）和背景网格（Eulerian，固定不动）。\r\n为什么选它：3DGS 本质是离散的“点”，MPM\r\n刚好也是基于“点”（粒子）的。两者天生契合，比基于网格的有限元法（FEM）更适合处理\r\n3DGS。\r\n\r\n\r\n\r\n3. 核心方法\r\nPhysGaussian\r\n的核心在于将图形学的属性与物理学的属性绑定在同一个对象上。\r\n3.1 统一表达 (Unified\r\nRepresentation)\r\n论文将 3DGS 的高斯核直接视为 MPM 模拟中的粒子。\r\n\r\n图形属性：位置、旋转、缩放、颜色（球谐系数）、不透明度。\r\n物理属性：质量、速度、形变梯度、应力（内部的力）。\r\n\r\n3.2 运动学演变 (Kinematics)\r\n当物体被挤压时，不仅位置变了，形状也得变。如果高斯球只是平移，渲染出来的物体看起来会像是在“平移错位”，而不是“变形”。\r\n\r\n位置更新：由 MPM 算出粒子的新速度和新位置。\r\n形状更新（协方差矩阵演变）：\r\n这是论文的精髓。根据连续介质力学，如果知道物体局部的形变梯度 （比如局部被拉长了 2\r\n倍），那么高斯球的形状（协方差矩阵 ）也应该被相应地拉伸。\r\n\r\n公式逻辑：新的形状 = 形变梯度  原始形状  形变梯度的转置。\r\n通俗解释：如果一个原本是圆球的高斯核，所在的区域被物理引擎判定为“压扁”，那么这个高斯核在渲染时就会自动变成一个扁平的飞碟形状。\r\n\r\n\r\n3.3 视觉外观的旋转 (SH\r\nRotation)\r\n3DGS 使用球谐函数 (Spherical Harmonics, SH)\r\n来表示颜色，这种颜色是随观察角度变化的（比如金属光泽）。\r\n\r\n问题：如果物体旋转了 90\r\n度，但他身上的“光泽”没跟着转，看起来就会很假。\r\n解决：利用物理计算出的局部旋转信息，对球谐系数的方向进行变换，确保光影随物体一起运动。\r\n\r\n\r\n4. 关键技术挑战与解决方案\r\n直接拿 3DGS 做仿真有两个大坑，论文给出了填坑方案。\r\n挑战一：空心壳问题\r\n(Hollow Shell Problem)\r\n3DGS 训练出来的模型通常只有表面有高斯点，内部是空的。\r\n\r\n物理上的问题：如果一个是空壳，它是没有质量的，或者质量分布不对。稍微一碰就会像气球一样瘪掉，或者无法模拟实心的弹性（比如捏一块实心橡胶）。\r\n解决方案：内部填充 (Internal Filling)\r\n\r\n利用 3DGS 的不透明度场（Opacity Field）。\r\n向物体内部发射射线，如果射线的能量从低变高再变低，说明穿过了物体。\r\n在检测到的物体“内部”空洞区域，人工生成新的不可见粒子（Opacity 为\r\n0，不参与渲染，只参与物理计算）。\r\n这样物体就有了“实心”的物理属性，碰撞和形变才真实。\r\n\r\n\r\n挑战二：长条伪影问题\r\n(Needle Artifacts)\r\n3DGS 为了拟合表面，有时候会把高斯球拉得极长（像一根针）。\r\n\r\n物理上的问题：当这些“长针”在物理模拟中发生剧烈旋转或弯曲时，它们可能会像刺猬的刺一样戳出物体表面，导致画面出现很多毛刺。\r\n解决方案：各向异性正则化 (Anisotropy Regularizer)\r\n\r\n在训练 3DGS 的阶段加入一个惩罚项（Loss）。\r\n通俗解释：训练时告诉算法，“你尽量把高斯球做得圆一点，不要太长太扁”。强制限制长轴和短轴的比例，保证粒子形状在后续物理形变中更鲁棒。\r\n\r\n\r\n\r\n5. 关于碰撞检测 (Collision)\r\n碰撞问题，在论文中是通过 MPM 算法天然解决的。\r\n\r\n原理：MPM\r\n算法中，所有粒子的动量都会先传递给背景网格（Grid），在网格上计算受力，再传回粒子。\r\n效果：\r\n\r\n物体与物体碰撞：当两个物体（两团高斯云）靠近时，它们会映射到同一个背景网格节点上。网格节点会发现“这里动量冲突了”，自动计算出反弹力。不需要显式地写代码去判断“三角形A是否穿插了三角形B”。\r\n物体与地面碰撞：通过设置网格边界条件即可实现。\r\n自碰撞：比如一块海绵折叠起来，自己碰到自己，MPM\r\n也能自动处理。\r\n\r\n\r\n\r\n6. 仿真能力展示\r\n该方法不仅能做简单的移动，还能模拟多种材质：\r\n\r\n弹性体\r\n(Elasticity)：如果冻、橡胶狐狸。形变后能恢复原状。\r\n塑性金属\r\n(Metal)：如被撞扁的易拉罐。撞扁后就扁了，不会弹回来。\r\n非牛顿流体/粘塑性\r\n(Viscoplastic)：如蛋糕上的奶油、果酱。\r\n颗粒材料 (Granular)：如沙土、散落的粉末。\r\n断裂 (Fracture)：比如撕面包。因为 3DGS\r\n是离散的点，所以“撕开”物体只是把点分开而已，不需要像网格那样处理复杂的拓扑断裂（Remeshing）。\r\n\r\n\r\n7. 总结：优点与局限性\r\n优点\r\n\r\n所见即所仿：不需要维护两套模型（渲染网格 vs\r\n物理网格），消除了几何不匹配。\r\n流程简化：省去了网格生成（Meshing）这一繁琐且容易出错的步骤。\r\n天然支持大形变与拓扑改变：撕裂、流体化、融化等效果，用点云（MPM）做比用网格做容易得多。\r\n渲染质量高：继承了 3DGS 的照片级渲染能力。\r\n\r\n局限\r\n\r\n阴影问题：目前的 3DGS\r\n很难处理动态阴影（Shadows），所以虽然物体动了，但阴影可能不真实。\r\n参数手动设置：物体的硬度、密度等物理参数需要人工指定，目前还不能完全从视频中自动学习出来。\r\n计算量：虽然比纯网格物理模拟方便，但 MPM\r\n模拟本身的计算量依然不小（虽然可以 GPU 加速）。\r\n\r\n","categories":["三维重建","物理仿真"],"tags":["3DGS"]},{"title":"Sim Anything阅读笔记","url":"/2025/12/16/23-Sim_Anything/","content":"1. 核心思想\r\n核心逻辑： 传统的物理仿真（如有限元法\r\nFEM）通常依赖网格（Mesh），而 3DGS\r\n本质上是基于点（Point-based）的表示。这篇论文通过结合 物质点法\r\n(Material Point Method, MPM)\r\n这一物理仿真算法，成功实现了直接驱动 3DGS 进行物理运动。\r\n\r\n共性连接： 3DGS 是离散的高斯球集合，MPM\r\n是离散的物理粒子集合。两者都是“粒子化”的表示，因此天然契合，不需要将\r\n3DGS 转换成三角形网格即可进行碰撞和变形仿真。\r\n\r\n\r\n2. 基础概念\r\n\r\n3D Gaussian Splatting (3DGS)：\r\n\r\n通俗解释： 一种新的 3D\r\n绘图技术。以前我们用三角形拼出物体（像折纸），现在我们用无数个模糊的椭球体（高斯球）堆叠出物体。每个球有位置、颜色、大小、旋转角度。\r\n优势： 渲染速度极快，画质逼真。\r\n\r\n物质点法 (MPM, Material Point Method)：\r\n\r\n通俗解释：\r\n一种物理仿真算法。它把物体看作一大堆沙子（粒子）。计算时，先把这些粒子映射到一个背景网格上算力（比如碰撞、挤压），算完后再把结果传回粒子，更新粒子的位置。\r\n为什么适合 3DGS：\r\n因为它不怕物体撕裂、大变形，且本身就是玩粒子的，跟 3DGS\r\n的“点”属性完美对应。\r\n\r\nMLLM (多模态大语言模型)：\r\n\r\n通俗解释： 像 GPT-4V 这样既能看图又能聊天的 AI。\r\n在本文作用：\r\n以前做物理仿真要手动设置“这个物体多重”、“那个物体多硬”。现在 AI\r\n看一眼图，就能猜出大概的物理参数（比如石头硬、海绵软）。\r\n\r\n\r\n\r\n3. Sim Anything 的工作流程\r\n这篇论文提出了一套全自动的流程，把一个静态的 3DGS\r\n场景变成可互动的物理场景。\r\n第一步：把物体“扣”出来\r\n(3D Open-vocabulary Segmentation)\r\n\r\n问题： 3DGS\r\n渲染出来是一整张图，计算机不知道哪里是“熊”，哪里是“底座”。\r\n方法： 利用现有的强大 AI 模型（SAM, Grounding DINO\r\n等）在 2D 图片上把物体识别并扣出来，然后通过算法把这些 2D 遮罩投射回 3D\r\n空间，给每个高斯球打上标签（比如这堆高斯球属于“椅子”）。\r\n补全： 把物体扣出来后，背景会有空洞。论文用了修图\r\nAI (LaMa) 把背景自动补全了。\r\n\r\n第二步：AI\r\n猜物理参数 (MLLM-based Physical Property Perception)\r\n\r\n难点： 只有外观，怎么知道物体的\r\n杨氏模量（Young's\r\nmodulus，代表硬度）、泊松比（Poisson's\r\nratio，代表受压后的横向膨胀率）和 密度？\r\n创新解法： 模仿人类直觉。\r\n\r\n看图说话： 让 AI (BLIP) 描述这个物体。\r\n猜材质： 把图和描述给 GPT-4V，让它列出可能的 K\r\n种材质（如：花岗岩、混凝土、树脂）。\r\n选最佳： 用 CLIP 模型看哪个材质跟图片最像。\r\n定参数： 确定是“花岗岩”后，再问\r\nGPT-4V：“花岗岩的物理参数大概是多少？”\r\n\r\n结果： 得到了一个物体平均的物理属性。\r\n\r\n第三步：参数分布预测 (MPDP)\r\n\r\n问题： 真实物体各处材质不一定均匀，而且 AI\r\n猜的只是个平均值。\r\n方法：\r\n训练了一个小型的神经网络，输入物体形状和平均参数，输出每个部位具体的参数分布。这让仿真更细腻。\r\n\r\n第四步：由点及面的仿真\r\n(物理核心)\r\n1. 物理引擎：MLS-MPM\r\n论文使用了 MLS-MPM (Moving Least Squares Material\r\nPoint Method) 作为物理模拟器。\r\n\r\n原理： 3DGS 的每一个高斯球，理论上都可以看作 MPM\r\n中的一个物理粒子。\r\n状态更新公式（关键）：\r\n当物体受力变形时，高斯球不仅位置要变，形状（协方差矩阵）也要变。\r\n\r\n位置更新：  (新位置) =  + 位移。\r\n形状更新： 。\r\n解释： \r\n叫做变形梯度。比如物体被压扁了，\r\n就会记录这个“压扁”的动作，然后应用到高斯球的形状 \r\n上，原本圆圆的高斯球也会跟着变扁。这就实现了视觉和物理的统一。\r\n\r\n\r\n2.\r\n采样策略：PGAS (Physical-Geometric Adaptive Sampling)\r\n\r\n痛点： 一个精细的 3DGS\r\n场景可能有几百万个高斯球。如果每个球都算物理模拟，计算量太大，显卡会冒烟。\r\n解决： 不驱动所有点，只选一部分“驱动粒子”\r\n(Driving Particles) 进行物理计算，其他点跟着动。\r\n怎么选点？ 论文设计了 PGAS 策略：\r\n\r\n硬的地方少选点：\r\n刚体不容易变形，几个点就能控制整体。\r\n软的地方多选点：\r\n软物体变形复杂，需要更多点来捕捉细节。\r\n弯曲的地方多选点：\r\n形状复杂的边缘需要更多控制点。\r\n\r\n效果： 既保证了仿真速度（单卡 RTX 3090\r\n几分钟就能算出结果），又保证了软体变形的真实感。\r\n\r\n\r\n4. 实验结果与对比\r\n\r\n速度： 相比于利用视频生成模型（Video\r\nDiffusion）来做物理仿真的方法（通常需要几小时），本方法只需要\r\n2分钟 左右。\r\n真实感： 在用户调研中，人们认为 Sim Anything\r\n生成的物理运动比 DreamGaussian4D\r\n等方法更符合物理规律（比如布料的摆动、果冻的颤动）。\r\n对比传统方法：\r\n\r\nPhysGaussian:\r\n本文方法与其类似，但增加了全自动的参数估计和更优的采样策略，不仅限于单一物体，能处理整个场景。\r\n视频生成类方法 (PhysDreamer):\r\n视频生成虽然画面好看，但容易出现“物理幻觉”（不符合牛顿定律），而且极慢。\r\n\r\n\r\n\r\n\r\n总结笔记\r\n不需要转网格： 不需要将 3DGS 转化为 Mesh 或\r\nSDF。\r\n核心是 MPM： 也就是物质点法。这是连接\r\n3DGS（视觉）和物理世界的桥梁。\r\n数据结构对应：\r\n\r\n3DGS 的 均值中心 (Mean Position) = 物理粒子的\r\n位置。\r\n3DGS 的 协方差矩阵 (Covariance) = 受物理\r\n变形梯度 (Deformation Gradient) 驱动的形状变量。\r\n\r\n性能优化关键：\r\n不要仿真每一个高斯点。使用下采样（Sub-sampling）技术，选取关键的“驱动粒子”进行物理计算，利用刚性变换\r\n(Rigid Body Transformation) 插值来驱动周围的高斯点。\r\n\r\n","categories":["三维重建","物理仿真"],"tags":["3DGS"]},{"title":"云服务器本机代理设置","url":"/2026/01/12/26-RemoteForward/","content":"1. 本地配置\r\n在本地电脑的 ~/.ssh/config\r\n文件中，找到或添加对应服务器的配置段，插入 RemoteForward\r\n指令。\r\nHost your_server_alias    HostName 服务器IP地址    Port SSH端口号    User root    # 核心指令：将远程服务器的 17890 转发到本地的 7890 (假设本地代理端口为 7890)    RemoteForward 17890 127.0.0.1:7890\r\n2. 服务器配置\r\n连接进入服务器后，执行以下命令使当前终端生效。\r\nexport http_proxy=http://127.0.0.1:17890export https_proxy=http://127.0.0.1:17890\r\n若希望每次登录服务器都自动开启该代理配置，可将上述命令写入服务器的\r\n~/.bashrc。\r\n设置git代理：\r\ngit config --global http.proxy http://127.0.0.1:17890git config --global https.proxy http://127.0.0.1:17890\r\n3. 连通性验证\r\n配置完成后，在服务器上测试是否可以正常访问外网资源：\r\ncurl -I https://www.google.com\r\n","categories":["编程经验"],"tags":["Remote"]},{"title":"MotrixSim MuJoCo 示例分析笔记","url":"/2025/12/29/25-MotrixSim_Mujoco/","content":"📁 文件概览\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n文件名\r\n场景描述\r\n核心概念\r\n\r\n\r\n\r\n\r\ngrasp_shaking_test.py\r\n机械臂抓取与摇晃测试\r\n轨迹控制、夹爪控制、录制\r\n\r\n\r\ngyroscope.py\r\n陀螺仪仿真（有重力）\r\n初始速度设置、DOF 控制\r\n\r\n\r\ngyroscope_zero_gravity.py\r\n零重力陀螺仪\r\n零重力环境、多轴旋转\r\n\r\n\r\nnewton_cradle.py\r\n牛顿摆\r\n碰撞模拟、能量传递\r\n\r\n\r\n\r\n\r\n1. 机械臂抓取摇晃测试\r\n\r\n文件: grasp_shaking_test.py\r\n场景描述\r\n这是一个使用 Franka Emika Panda\r\n机械臂进行抓取测试的仿真场景。机械臂会抓起一个物体（立方体、球或瓶子），然后进行摇晃测试或滑移测试，验证抓取的稳定性。\r\n核心功能\r\n\r\n多物体支持: 支持抓取 3 种不同物体\r\n\r\ncube - 立方体\r\nball - 球体\r\nbottle - 瓶子\r\n\r\n两种测试模式:\r\n\r\n摇晃测试 (shake): 抓取后随机摇晃手臂\r\n滑移测试 (slip):\r\n抓取后保持静止，测试物体是否滑落\r\n\r\n录制功能: 可将仿真过程录制为 MP4 视频\r\n\r\n代码结构分析\r\n# 命令行参数定义_Obj = flags.DEFINE_string(\"object\", \"cube\", \"...\")     # 物体选择_Shake = flags.DEFINE_boolean(\"shake\", True, \"...\")     # 是否摇晃_Record = flags.DEFINE_boolean(\"record\", True, \"...\")   # 是否录制\r\n仿真流程 (状态机)\r\nstateDiagram-v2\r\n    [*] --&gt; 移动到抬起位置: step 0-500\r\n    移动到抬起位置 --&gt; 移动到抓取位置: step 500-1000\r\n    移动到抓取位置 --&gt; 闭合夹爪: step 1000-1500\r\n    闭合夹爪 --&gt; 抬起物体: step 1500-2000\r\n    抬起物体 --&gt; 摇晃测试: step 2000-10000 (if shake)\r\n    抬起物体 --&gt; 保持静止: step 2000-10000 (if !shake)\r\n    摇晃测试 --&gt; 测试通过: step &gt; 10000\r\n    保持静止 --&gt; 测试通过: step &gt; 10000\r\n    摇晃测试 --&gt; 测试失败: 物体掉落 (z &lt; 0.04)\r\n    保持静止 --&gt; 测试失败: 物体掉落 (z &lt; 0.04)\r\n关键技术点\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n技术点\r\n说明\r\n代码位置\r\n\r\n\r\n\r\n\r\nKeyframe 使用\r\n使用预定义关键帧作为目标位姿\r\nmodel.key(\"home\"), model.key(\"grasp\"),\r\nmodel.key(\"lift\")\r\n\r\n\r\n线性插值\r\n使用 lerp 函数平滑过渡关节位置\r\nlerp(a, b, t)\r\n\r\n\r\n夹爪控制\r\n通过 ctrl[7] 控制夹爪开合\r\ndata.ctrl[7] = lerp(0.04, 0, ...)\r\n\r\n\r\n随机扰动\r\n摇晃时使用高斯噪声扰动\r\nnp.random.normal(0, 0.025, size=7)\r\n\r\n\r\n掉落检测\r\n监测物体 z 坐标判断是否掉落\r\nobj_pos[2] &lt; 0.04\r\n\r\n\r\n视频录制\r\n使用 imageio 保存帧序列\r\nimageio.mimwrite(...)\r\n\r\n\r\n\r\n使用示例\r\n# 抓取立方体并摇晃python examples/mujoco/grasp_shaking_test.py --object=cube --shake=True# 抓取瓶子进行滑移测试（不摇晃）python examples/mujoco/grasp_shaking_test.py --object=bottle --shake=False --record=False\r\n\r\n2. 陀螺仪仿真（有重力）\r\n\r\n文件: gyroscope.py\r\n场景描述\r\n模拟一个在重力环境下旋转的陀螺仪。陀螺仪被赋予初始角速度后，会在重力作用下产生进动（precession）现象。\r\n物理原理\r\n陀螺进动当陀螺仪高速旋转时，由于重力产生的力矩不会使其立即倾倒，而是使旋转轴绕垂直方向缓慢旋转，这就是进动现象。\r\n关键代码解析\r\n# 获取关节的自由度地址joint_name = \"ball_1\"joint_id = mujoco.mj_name2id(model, mujoco.mjtObj.mjOBJ_JOINT, joint_name)dof_vel_adr = model.jnt_dofadr[joint_id]# 设置初始角速度 (绕 Z 轴)data.qvel[dof_vel_adr + 5] = 50  # 高速自转\r\nDOF 速度地址说明\r\n对于一个 6-DOF 球关节:\r\n\r\ndof_vel_adr + 0~2: 线速度 (vx, vy, vz)\r\ndof_vel_adr + 3~5: 角速度 (wx, wy, wz)\r\n\r\n因此 dof_vel_adr + 5 设置的是绕局部 Z 轴的角速度。\r\n仿真特点\r\n\r\n相机设置: 将相机距离设为\r\n5，以便观察完整的进动运动\r\n实时同步: 使用 time.sleep\r\n保持仿真与实际时间同步\r\n\r\n\r\n3. 零重力陀螺仪\r\n\r\n文件: gyroscope_zero_gravity.py\r\n场景描述\r\n在零重力环境下模拟陀螺仪的自由旋转。由于没有重力，陀螺仪不会产生进动，而是保持角动量守恒的纯旋转运动。\r\n与有重力版本的对比\r\n\r\n\r\n\r\n特性\r\ngyroscope.py\r\ngyroscope_zero_gravity.py\r\n\r\n\r\n\r\n\r\n重力\r\n有\r\n无\r\n\r\n\r\n初始角速度\r\n单轴 (Z轴: 50)\r\n双轴 (X轴: 10, Z轴: 5)\r\n\r\n\r\n运动特点\r\n进动\r\n自由旋转\r\n\r\n\r\n\r\n关键代码解析\r\n# 设置多轴初始角速度data.qvel[dof_vel_adr + 5] = 5   # Z轴角速度data.qvel[dof_vel_adr + 3] = 10  # X轴角速度\r\n可视化功能\r\n# 每隔 2 秒切换接触点显示with viewer.lock():    viewer.opt.flags[mujoco.mjtVisFlag.mjVIS_CONTACTPOINT] = int(data.time % 2)\r\n这个功能展示了如何动态控制 MuJoCo 的可视化选项。\r\n\r\n4. 牛顿摆\r\n\r\n文件: newton_cradle.py\r\n场景描述\r\n经典的牛顿摆（Newton's\r\nCradle）物理演示。这是一个展示动量守恒和能量守恒的经典物理实验装置。\r\n物理原理\r\n牛顿摆原理当一端的球被拉起释放后，撞击静止的球组，动量和能量通过球组传递，使另一端的球以相同速度弹起。这完美展示了弹性碰撞中的动量守恒和动能守恒。\r\n代码结构\r\ndef main():    # 加载模型 (使用 MuJoCo 版本的牛顿摆)    model = mujoco.MjModel.from_xml_path(\"examples/assets/newton_cradle_mj.xml\")    data = mujoco.MjData(model)    with mujoco.viewer.launch_passive(model, data) as viewer:        # 设置相机视角 - 正侧面观察        with viewer.lock():            viewer.cam.lookat = [0, 0, 7]    # 观察点            viewer.cam.distance = 20          # 距离            viewer.cam.azimuth = -90          # 方位角 (正侧面)            viewer.cam.elevation = 0          # 仰角 (水平)\r\n相机设置说明\r\n\r\n\r\n\r\n参数\r\n值\r\n说明\r\n\r\n\r\n\r\n\r\nlookat\r\n[0, 0, 7]\r\n相机注视点，位于摆锤悬挂高度\r\n\r\n\r\ndistance\r\n20\r\n相机与注视点距离\r\n\r\n\r\nazimuth\r\n-90\r\n方位角，设为正侧面视角\r\n\r\n\r\nelevation\r\n0\r\n仰角，水平观察\r\n\r\n\r\n\r\n资源文件\r\n仓库中有两个牛顿摆模型文件：\r\n\r\nnewton_cradle_mj.xml - MuJoCo 版本\r\nnewton_cradle_mt.xml - MotrixSim 版本\r\n(可能用于对比)\r\n\r\n\r\n📊 代码模式总结\r\n通用仿真循环\r\n所有示例都遵循相同的仿真循环模式：\r\nwith mujoco.viewer.launch_passive(model, data) as viewer:    while True:        step_start = time.time()                # 1. 执行物理步进        mujoco.mj_step(model, data)                # 2. 同步渲染        viewer.sync()                # 3. 实时同步        time_until_next_step = model.opt.timestep - (time.time() - step_start)        if time_until_next_step &gt; 0:            time.sleep(time_until_next_step)\r\n关键 API 函数\r\n\r\n\r\n\r\n函数\r\n用途\r\n\r\n\r\n\r\n\r\nmujoco.MjModel.from_xml_path()\r\n从 XML 文件加载模型\r\n\r\n\r\nmujoco.MjData(model)\r\n创建仿真数据结构\r\n\r\n\r\nmujoco.mj_step(model, data)\r\n执行一步物理仿真\r\n\r\n\r\nmujoco.mj_name2id()\r\n通过名称查找对象 ID\r\n\r\n\r\nmujoco.mj_resetDataKeyframe()\r\n重置到关键帧状态\r\n\r\n\r\nviewer.launch_passive()\r\n启动被动模式查看器\r\n\r\n\r\nviewer.sync()\r\n同步渲染状态\r\n\r\n\r\n\r\n\r\n🎯 总结\r\n这些 MuJoCo 示例展示了：\r\n\r\n基础仿真: 如何加载模型、创建数据、运行仿真循环\r\n初始条件设置: 如何设置初始速度和位置\r\n控制接口: 如何通过 data.ctrl\r\n控制执行器\r\n轨迹规划: 如何使用插值实现平滑运动\r\n状态监测: 如何读取和判断仿真状态\r\n可视化控制: 如何设置相机和切换显示选项\r\n录制功能: 如何捕获帧并保存为视频\r\n\r\n注意这些示例使用的是原生 MuJoCo Python API，与 MotrixSim\r\n的 API 有所不同。MotrixSim 提供了更高级的抽象和更丰富的功能，参见\r\nexamples/ 目录下的其他示例。\r\n","categories":["机器仿真"],"tags":["Motrix"]},{"title":"MotrixSim 仓库分析与 go2.py 详解","url":"/2025/12/29/24-MotrixSim/","content":"🔍 仓库分析\r\n特征分析\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n特征\r\n分析\r\n\r\n\r\n\r\n\r\n核心引擎代码\r\n❌ 未公开。motrixsim 核心通过\r\npip install motrixsim 从 PyPI 安装\r\n\r\n\r\nPython 绑定\r\n❌ 未公开。底层使用 Rust 实现，只暴露 Python API\r\n\r\n\r\n示例代码\r\n✅ 完整。包含 39+ 个 Python 示例文件\r\n\r\n\r\nAPI 文档\r\n✅ 完整。可以从 https://motrixsim.readthedocs.io 访问\r\n\r\n\r\n资源文件\r\n✅ 完整。包含机器人模型、场景、策略网络等\r\n\r\n\r\n\r\n仓库结构\r\nmotrixsim-docs/├── docs/                    # 文档源码│   └── source/              # 包含 API 参考、教程等├── examples/                # ⭐ 示例代码 (39+ 个 Python 文件)│   ├── go1.py               # 四足机器人 Go1 示例│   ├── go2.py               # 四足机器人 Go2 示例│   ├── g1_keyboard_control.py  # 人形机器人 G1 键盘控制│   ├── robotic_arm.py       # 机械臂示例│   └── assets/              # 模型和资源文件├── legged_gym/              # 腿式机器人强化学习环境├── pyproject.toml           # 依赖配置├── README.md                # 项目说明└── LICENSE                  # Apache 2.0 许可证\r\n重要核心物理引擎代码未公开\r\nmotrixsim\r\n的核心实现（物理求解器、碰撞检测、渲染引擎等）是闭源的。\r\n这个仓库只提供：使用说明、API 示例、机器人模型文件。\r\n\r\n📖 go2.py 详细解读\r\nGitHub: examples/go2.py\r\n功能概述\r\n这个脚本演示了一个四足机器人 Go2\r\n在物理仿真环境中自主行走的完整流程：\r\nflowchart LR\r\n    A[加载场景模型] --&gt; B[创建物理数据]\r\n    B --&gt; C[加载神经网络策略]\r\n    C --&gt; D[仿真循环]\r\n    D --&gt; E{机器人摔倒?}\r\n    E --&gt;|是| F[重置场景]\r\n    E --&gt;|否| G[收集观测数据]\r\n    F --&gt; D\r\n    G --&gt; H[神经网络推理]\r\n    H --&gt; I[应用动作到关节]\r\n    I --&gt; J[物理引擎步进]\r\n    J --&gt; K[渲染显示]\r\n    K --&gt; D\r\n\r\n逐段代码解析\r\n1️⃣ 导入模块\r\nimport timeimport randomfrom collections import dequeimport numpy as npimport onnxruntime as ortfrom scipy.spatial.transform import Rotationfrom motrixsim import SceneData, SceneModel, load_model, stepfrom motrixsim.render import CaptureTask, RenderApp\r\n\r\n\r\n\r\n模块\r\n用途\r\n\r\n\r\n\r\n\r\ntime\r\n用于帧率控制 (time.sleep)\r\n\r\n\r\nrandom\r\n生成随机运动目标\r\n\r\n\r\ndeque\r\n双端队列，用于管理截图任务\r\n\r\n\r\nnumpy\r\n数值计算（向量、矩阵操作）\r\n\r\n\r\nonnxruntime\r\n运行神经网络模型（ONNX 格式）\r\n\r\n\r\nscipy.spatial.transform.Rotation\r\n四元数与旋转矩阵的转换\r\n\r\n\r\nmotrixsim.SceneData\r\n存储仿真的动态状态（位置、速度、力）\r\n\r\n\r\nmotrixsim.SceneModel\r\n场景的静态描述（几何、物理参数）\r\n\r\n\r\nmotrixsim.load_model\r\n从 MJCF/XML 文件加载场景\r\n\r\n\r\nmotrixsim.step\r\n执行一次物理仿真步进\r\n\r\n\r\nmotrixsim.render.RenderApp\r\n可视化渲染窗口\r\n\r\n\r\nmotrixsim.render.CaptureTask\r\n异步截图任务\r\n\r\n\r\n\r\n\r\n2️⃣ 全局参数定义\r\ndefault_joint_pos = np.array([0.1, 0.9, -1.8, -0.1, 0.9, -1.8, 0.1, 0.9, -1.8, -0.1, 0.9, -1.8])action_scale = 0.5lin_vel_scale = 1.0ang_vel_scale = 1.5\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n参数\r\n说明\r\n\r\n\r\n\r\n\r\ndefault_joint_pos\r\n机器人 12 个关节的默认角度（弧度）。Go2 有 4\r\n条腿，每条腿 3 个关节（髋/大腿/小腿）\r\n\r\n\r\naction_scale\r\n神经网络输出的动作缩放因子（0.5 表示输出值减半）\r\n\r\n\r\nlin_vel_scale\r\n线速度目标缩放因子（Go2 是 1.0，比 Go1 的 0.7 更高）\r\n\r\n\r\nang_vel_scale\r\n角速度目标缩放因子\r\n\r\n\r\n\r\nGo2 vs Go1 差异\r\nGo2 躯干名称：\"base\"，Go1 是 \"trunk\"\r\nGo2 第一个执行器：\"FL_hip\"，Go1 是\r\n\"FR_hip\"\r\nGo2 线速度缩放 1.0，Go1 是 0.7\r\n\r\n关节顺序：[FL_hip, FL_thigh, FL_calf, FR_hip, FR_thigh, FR_calf, RL_hip, RL_thigh, RL_calf, RR_hip, RR_thigh, RR_calf]\r\n- FL = Front Left（前左腿） - FR = Front Right（前右腿） - RL = Rear\r\nLeft（后左腿） - RR = Rear Right（后右腿）\r\n\r\n3️⃣ 观测数据收集函数\r\ndef compute_observations(last_actions, target_action, model: SceneModel, data: SceneData):    # 获取身体索引    body_index = model.get_body_index(\"base\")  # Go2 使用 \"base\"，Go1 使用 \"trunk\"    body = model.get_body(body_index)    # 线速度 (3D) - 机器人在本地坐标系的速度    linear_vel = model.get_sensor_value(\"local_linvel\", data)        # 角速度 (3D) - 陀螺仪读数    gyro = model.get_sensor_value(\"gyro\", data)        # 重力方向 (3D) - 表示机器人姿态    pose = body.get_pose(data)    inv_rotation = Rotation.from_quat(pose[3:7]).inv()    gravity = inv_rotation.apply(np.array([0.0, 0.0, -1.0]))    # 关节位置和速度    dof_pos = body.get_joint_dof_pos(data)    dof_vel = body.get_joint_dof_vel(data)        # 使用 np.hstack 高效拼接所有观测量    obs = np.hstack([        linear_vel,                      # 线速度 (3)        gyro,                            # 角速度 (3)        gravity,                         # 重力方向 (3)        dof_pos - default_joint_pos,     # 关节位置差 (12)        dof_vel,                         # 关节速度 (12)        last_actions,                    # 上一帧动作 (12)        target_action                    # 目标速度 (3)    ])    return obs\r\n这是神经网络的\"眼睛\" -\r\n收集机器人当前状态作为输入。\r\n\r\n\r\n\r\n观测量\r\n维度\r\n说明\r\n\r\n\r\n\r\n\r\n线速度\r\n3\r\n机器人身体在本地坐标系的速度 (x, y, z)\r\n\r\n\r\n角速度\r\n3\r\n机器人绕三个轴的旋转速度（陀螺仪读数）\r\n\r\n\r\n重力方向\r\n3\r\n重力在机器人本体坐标系中的方向，反映倾斜程度\r\n\r\n\r\n关节位置差\r\n12\r\n当前关节角度与默认姿态的偏差\r\n\r\n\r\n关节速度\r\n12\r\n各关节的角速度\r\n\r\n\r\n上一帧动作\r\n12\r\n上一步施加的控制命令\r\n\r\n\r\n目标速度\r\n3\r\n期望的线速度 (x, y) 和角速度 (yaw)\r\n\r\n\r\n总计\r\n48\r\n神经网络输入维度\r\n\r\n\r\n\r\n为什么要包含上一帧动作这让神经网络了解控制的连续性，避免输出剧烈抖动的动作。这是强化学习中常用的技巧。\r\nGo2 代码优化使用 np.hstack() 一次性拼接数组，比 Go1 的\r\nobs.extend() 方式更高效。\r\n\r\n4️⃣ 目标更新函数\r\ndef update_target(goback, body_position: np.ndarray):    target_action = [0, 0]    if goback:        # 返回原点：计算指向原点的方向        v = -body_position[:2]           # 取 x, y 坐标，取反指向原点        norm = v / np.linalg.norm(v)     # 归一化为单位向量        target_action = [norm[0] * lin_vel_scale, norm[1] * , 0]    else:        # 随机方向：生成 -2 到 2 范围内的随机速度        x = random.random() * 4.0 - 2.0        y = random.random() * 4.0 - 2.0        rot = random.random() * 4.0 - 2.0        v = np.array([x, y])        norm = v / np.linalg.norm(v)        target_action = [norm[0] * lin_vel_scale, norm[1] * lin_vel_scale, rot * ang_vel_scale]    return target_action\r\n这个函数生成机器人的运动目标：\r\n\r\n随机漫游模式\r\n(goback=False)：生成随机方向和转向速度\r\n返回原点模式\r\n(goback=True)：计算指向原点的方向，直线返回\r\n\r\n返回的 target_action 包含 3\r\n个值：[vx, vy, yaw_rate]（前后速度、左右速度、转向速度）\r\n\r\n5️⃣ 动作应用函数\r\ndef apply_actions(actions, model: SceneModel, data: SceneData):    start_actuator_index = model.get_actuator_index(\"FL_hip\")  # Go2 从 FL_hip 开始    for index, act in enumerate(actions):        actuator_index = start_actuator_index + index        ctrl = act * action_scale + default_joint_pos[index]  # 缩放 + 偏移        actuator = model.get_actuator(actuator_index)        actuator.set_ctrl(data, ctrl)\r\n这是神经网络的\"手\" - 将决策转化为关节控制。\r\n控制流程：\r\n\r\n神经网络输出 12 个动作值（范围约 -1 到 1）\r\n每个动作乘以 action_scale（0.5）进行缩放\r\n加上默认关节角度\r\ndefault_joint_pos，得到目标关节角度\r\n通过 actuator.set_ctrl() 设置关节电机的目标位置\r\n\r\n计算示例： 神经网络输出: act = 0.5缩放后: 0.5 * 0.5 = 0.25加上默认值: 0.25 + 0.1 = 0.35 (弧度)\r\n\r\n6️⃣ 摔倒检测函数\r\ndef is_fall(model: SceneModel, data: SceneData):    pose = model.get_link(\"base\").get_pose(data)  # Go2 使用 \"base\"    rotation = Rotation.from_quat(pose[3:7])    rotated_z_axis = rotation.apply(np.array([0.0, 0.0, 1.0]))    thr = 0.3    dot = np.dot(rotated_z_axis, np.array([0.0, 0.0, 1.0]))    return dot &lt; thr\r\n判断机器人是否摔倒的逻辑：\r\n\r\n获取机器人躯干(base)的姿态四元数\r\n[x, y, z, w]\r\n将四元数转换为旋转对象\r\n计算机器人的\"上方向\"（本地 Z 轴）在世界坐标系中的朝向\r\n与世界坐标系的 Z 轴（垂直向上）做点积\r\n如果点积 &lt; 0.3，说明机器人已经严重倾斜/翻倒\r\n\r\n\r\n\r\n\r\n点积值\r\n含义\r\n\r\n\r\n\r\n\r\n1.0\r\n完全直立\r\n\r\n\r\n0.7\r\n倾斜约 45°\r\n\r\n\r\n0.3\r\n倾斜约 72°（触发重置）\r\n\r\n\r\n0.0\r\n完全侧翻 90°\r\n\r\n\r\n-1.0\r\n完全翻转（背朝下）\r\n\r\n\r\n\r\n\r\n7️⃣ 主函数详解\r\n初始化渲染和加载模型\r\ndef main():    # 创建渲染窗口    with RenderApp() as render:        render.opt.set_left_panel_vis(True)  # 显示左侧控制面板                # 加载场景模型        path = \"examples/assets/go2/scene_flat.xml\"        model = load_model(path)\r\nscene_flat.xml 包含： - Go2 机器人模型（引用\r\ngo2_mjx.xml） - 平坦地面 - 光照设置 - 物理参数\r\n\r\n相机配置\r\ncameras = model.cameras# RGB 相机cameras[0].set_render_target(\"image\", 320, 240)# 深度相机cameras[1].set_render_target(\"image\", 640, 480)cameras[1].depth_only = True       # 只输出深度图cameras[1].set_near_far(0.1, 1)    # 深度范围 0.1-1 米# 可预览的相机列表（None 表示自由视角）preview_cameras = [None, *cameras[2:]]preview_camera_idx = 0\r\n\r\n创建物理数据和加载神经网络\r\n# 启动渲染render.launch(model)# 创建物理仿真状态数据data = SceneData(model)# 加载预训练神经网络session = ort.InferenceSession(\"examples/assets/go2/go2_policy.onnx\",                                 providers=[\"CPUExecutionProvider\"])input_name = session.get_inputs()[0].name   # 输入张量名称output_name = session.get_outputs()[0].name # 输出张量名称\r\n神经网络规格： - 输入：48 维浮点数向量 - 输出：12 维浮点数向量（12\r\n个关节的控制命令）\r\n\r\n控制变量初始化\r\nlast_actions = [0] * 12           # 上一帧动作（初始为零）n_infer_interval = 5              # 每 5 个物理步执行一次推理（Go1 是 10）n_set_tartget_interval = 750      # 每 750 步更换目标go_back = False                   # 是否返回原点nsteps = 0                        # 步数计数target_action = [0.5, 0, 0]       # 初始目标：向前走capture_tasks = deque()           # 截图任务队列capture_index = 0                 # 截图编号\r\nGo2 vs Go1 差异Go2 的 n_infer_interval = 5，比 Go1 的 10\r\n更频繁，意味着控制频率更高。\r\n\r\n主仿真循环\r\nwhile True:    for _ in range(4):        # 物理引擎步进        step(model, data)        # 摔倒检测与重置        if is_fall(model, data):            data = SceneData(model)  # 重新创建数据以重置场景        # 步数计数        nsteps += 1        if nsteps % n_infer_interval == 0:            # 每 750 步更换运动目标            if nsteps % n_set_tartget_interval == 0:                body_pose = model.get_body(model.get_body_index(\"base\")).get_pose(data)                target_action = update_target(go_back, body_pose[:3])                go_back = not go_back  # 切换模式                        # 收集观测数据            obs = compute_observations(last_actions, target_action, model, data)            # 准备输入（reshape 为 [1, 48]）            input_data = np.array(obs).reshape(1, -1).astype(np.float32)            # 神经网络推理            outputs = session.run([output_name], {input_name: input_data})            actions = outputs[0][0]            # 应用动作            apply_actions(actions, model, data)            last_actions = actions\r\n时序分析： - 每帧执行 4 次物理步进（提高仿真精度） -\r\n每 5 个物理步进执行一次神经网络推理（控制频率 = 仿真频率 / 5） - 每 750\r\n步（约几秒钟）切换一次运动目标\r\n\r\n截图功能\r\n# 按空格键截图if render.input.is_key_just_pressed(\"space\"):    rcam = render.get_camera(0)    capture_tasks.append((capture_index, rcam.capture()))    capture_index += 1# 同步渲染并限制帧率render.sync(data)time.sleep(1/60.)  # 限制为 60 FPS# 处理截图任务while len(capture_tasks) &gt; 0:    idx, task = capture_tasks[0]    if task.state != \"pending\":        capture_tasks.popleft()        img = task.take_image()        os.makedirs(\"shot\", exist_ok=True)        img.save_to_disk(f\"shot/capture_{idx}.png\")    else:        break\r\n\r\n相机切换\r\n# 右方向键：下一个相机if render.input.is_key_just_pressed(\"right\"):    preview_camera_idx = (preview_camera_idx + 1) % len(preview_cameras)    render.set_main_camera(preview_cameras[preview_camera_idx])# 左方向键：上一个相机if render.input.is_key_just_pressed(\"left\"):    preview_camera_idx = (preview_camera_idx + len(preview_cameras) - 1) % len(preview_cameras)    render.set_main_camera(preview_cameras[preview_camera_idx])\r\n\r\n📖 go2_keyboard_control.py\r\n详细解读\r\nGitHub: examples/go2_keyboard_control.py\r\n功能概述\r\n通过键盘实时控制 Go2 机器人的移动方向和速度。与\r\ngo2.py\r\n的主要区别是使用面向对象设计和高级\r\nAPI。\r\nflowchart TB\r\n    subgraph 初始化\r\n        A[加载模型] --&gt; B[创建 OnnxController]\r\n        B --&gt; C[启动渲染循环]\r\n    end\r\n    \r\n    subgraph \"渲染循环 (60 FPS)\"\r\n        D[读取键盘输入] --&gt; E[更新 command 向量]\r\n        E --&gt; F[get_control 物理步进]\r\n        F --&gt; G[神经网络推理]\r\n        G --&gt; H[应用动作]\r\n        H --&gt; I[渲染同步]\r\n    end\r\n    \r\n    C --&gt; D\r\n    I --&gt; D\r\n\r\n全局参数\r\ndefault_joint_pos = np.array([0.1, 0.9, -1.8, -0.1, 0.9, -1.8, 0.1, 0.9, -1.8, -0.1, 0.9, -1.8])action_scale = 0.5lin_vel_scale = 2.0   # 比 go2.py 的 1.0 更快！ang_vel_scale = 3.0   # 比 go2.py 的 1.5 更快！\r\n键盘控制版本更快键盘控制版本的速度更快，因为手动控制需要更灵敏的响应。\r\n\r\nOnnxController 类详解\r\n这是与 go2.py 最大的区别 ——\r\n使用面向对象设计封装所有控制逻辑：\r\n构造函数\r\nclass OnnxController:    def __init__(        self,        model: SceneModel,        policy_path: str,        default_angles: np.ndarray,        ctrl_dt: float,          # 控制周期（秒）        action_scale: float = 0.5,    ):        self._model = model        self._data = SceneData(self._model)  # 创建物理数据                # 加载神经网络        self._policy = ort.InferenceSession(policy_path, providers=[\"CPUExecutionProvider\"])        self._input_name = self._policy.get_inputs()[0].name        self._output_name = self._policy.get_outputs()[0].name                # 控制参数        self.command = np.zeros(3, dtype=np.float32)  # [vx, vy, yaw_rate] 外部可修改        self._action_scale = action_scale        self._default_angles = default_angles.copy()        self._last_action = np.zeros_like(default_angles, dtype=np.float32)                # 计算子步数：每个控制周期内执行多少次物理步进        self._counter = 0        self._n_substeps = int(round(ctrl_dt / self._model.options.timestep))\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n属性\r\n说明\r\n\r\n\r\n\r\n\r\ncommand\r\n速度指令向量\r\n[前后, 左右, 转向]，键盘输入直接修改此属性\r\n\r\n\r\n_n_substeps\r\n每次神经网络推理之间的物理步进次数\r\n\r\n\r\n_last_action\r\n上一帧的动作输出，用于观测\r\n\r\n\r\n\r\n控制频率计算示例： ctrl_dt = 0.02 秒 (20ms, 即 50Hz)timestep = 0.002 秒 (默认物理步长)n_substeps = 0.02 / 0.002 = 10即：每 10 次物理步进执行一次神经网络推理\r\n\r\n观测收集方法\r\ndef get_obs(self, model: SceneModel, data: SceneData, command):    body = model.get_body(model.get_body_index(\"base\"))        linear_vel = model.get_sensor_value(\"local_linvel\", data)    gyro = model.get_sensor_value(\"gyro\", data)        pose = body.get_pose(data)    inv_rotation = Rotation.from_quat(pose[3:7]).inv()    gravity = inv_rotation.apply(np.array([0.0, 0.0, -1.0]))        dof_pos = body.get_joint_dof_pos(data)    dof_vel = body.get_joint_dof_vel(data)        obs = np.hstack([        linear_vel, gyro, gravity,        dof_pos - self._default_angles,        dof_vel,        self._last_action,   # 注意：使用实例变量        command    ])    return obs.astype(np.float32)\r\n\r\n核心控制方法\r\ndef get_control(self):    self._counter += 1    step(self._model, self._data)  # 物理步进        # 摔倒检测    if self.is_fall(self._model, self._data):        self._data = SceneData(self._model)  # 重置        self.command = np.zeros(3, dtype=np.float32)  # 清空指令        # 按控制频率执行推理    if self._counter % self._n_substeps == 0:        obs = self.get_obs(self._model, self._data, self.command)        outputs = self._policy.run([self._output_name], {self._input_name: obs.reshape(1, -1)})        actions = outputs[0][0]        self._last_action = actions.copy()        self.apply_actions(actions, self._model, self._data)\r\n这个方法每调用一次执行一个物理步进，并在适当的时机执行神经网络推理。\r\n\r\n主函数详解\r\ndef main():    with RenderApp() as render:        render.opt.set_left_panel_vis(True)        path = \"examples/assets/go2/scene_flat.xml\"        model = load_model(path)        render.launch(model)        # 创建控制器        policy = OnnxController(            model,            policy_path=\"examples/assets/go2/go2_policy.onnx\",            ctrl_dt=0.02,               # 控制周期 20ms (50Hz)            default_angles=default_joint_pos,            action_scale=action_scale,        )\r\n\r\n键盘输入处理\r\ninput = render.inputdef render_step():    # 前后移动    if input.is_key_pressed(\"up\") or input.is_key_pressed(\"w\"):        policy.command[0] = 1.0 * lin_vel_scale   # 前进    elif input.is_key_pressed(\"down\") or input.is_key_pressed(\"s\"):        policy.command[0] = -1.0 * lin_vel_scale  # 后退    else:        policy.command[0] = 0.0    # 左右平移    if input.is_key_pressed(\"left\"):        policy.command[1] = 0.5 * lin_vel_scale   # 左移    elif input.is_key_pressed(\"right\"):        policy.command[1] = -0.5 * lin_vel_scale  # 右移    else:        policy.command[1] = 0.0    # 转向    if input.is_key_pressed(\"a\"):        policy.command[2] = 2.0 * ang_vel_scale   # 左转    elif input.is_key_pressed(\"d\"):        policy.command[2] = -2.0 * ang_vel_scale  # 右转    else:        policy.command[2] = 0.0    render.sync(policy.data)\r\n🎮 完整键盘控制映射表\r\n\r\n\r\n\r\n按键\r\n功能\r\ncommand[0]\r\ncommand[1]\r\ncommand[2]\r\n\r\n\r\n\r\n\r\nW / ↑\r\n前进\r\n2.0\r\n0\r\n0\r\n\r\n\r\nS / ↓\r\n后退\r\n-2.0\r\n0\r\n0\r\n\r\n\r\n←\r\n左平移\r\n0\r\n1.0\r\n0\r\n\r\n\r\n→\r\n右平移\r\n0\r\n-1.0\r\n0\r\n\r\n\r\nA\r\n左转\r\n0\r\n0\r\n6.0\r\n\r\n\r\nD\r\n右转\r\n0\r\n0\r\n-6.0\r\n\r\n\r\n\r\n\r\nrender_loop 高级 API\r\nprint(\"Keyboard Controls:\")print(\"- Press W / Up Arrow to move forward\")print(\"- Press S / Down Arrow to move backward\")print(\"- Press Left Arrow to move left\")print(\"- Press Right Arrow to move right\")  print(\"- Press A to rotate left\")print(\"- Press D to rotate right\")run.render_loop(    model.options.timestep,   # 物理时间步长    60,                       # 目标帧率 (FPS)    policy.get_control,       # 物理更新回调（每物理步调用）    render_step               # 渲染更新回调（每帧调用）)\r\nrun.render_loop 的工作原理：\r\n\r\n根据目标帧率计算每帧需要执行多少次物理步进\r\n每个物理步进调用 policy.get_control()\r\n每帧结束调用 render_step() 处理输入和渲染\r\n自动处理时间同步，保持稳定帧率\r\n\r\n这比手动写 while True 循环更简洁、更可靠。\r\n\r\n🔄 go2.py vs\r\ngo2_keyboard_control.py 完整对比\r\n\r\n\r\n\r\n特性\r\ngo2.py\r\ngo2_keyboard_control.py\r\n\r\n\r\n\r\n\r\n控制模式\r\n自动随机漫游\r\n键盘手动控制\r\n\r\n\r\n代码风格\r\n函数式（独立函数）\r\n面向对象 (OnnxController 类)\r\n\r\n\r\nlin_vel_scale\r\n1.0\r\n2.0 (更快)\r\n\r\n\r\nang_vel_scale\r\n1.5\r\n3.0 (更快)\r\n\r\n\r\nn_infer_interval\r\n5 步\r\n由 ctrl_dt 计算\r\n\r\n\r\n主循环\r\n手动 while True\r\nrun.render_loop() 高级 API\r\n\r\n\r\n帧率控制\r\ntime.sleep(1/60)\r\nrender_loop 自动管理\r\n\r\n\r\n相机截图\r\n✅ 支持\r\n❌ 无\r\n\r\n\r\n相机切换\r\n✅ 支持\r\n❌ 无\r\n\r\n\r\n代码行数\r\n~207 行\r\n~178 行\r\n\r\n\r\n\r\n\r\n📁 相关文件\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n文件\r\n说明\r\n\r\n\r\n\r\n\r\ngo2.py\r\n自动漫游示例\r\n\r\n\r\ngo2_keyboard_control.py\r\n键盘控制示例\r\n\r\n\r\nscene_flat.xml\r\nGo2 场景定义\r\n\r\n\r\ngo2_mjx.xml\r\nGo2 机器人模型\r\n\r\n\r\ngo2_policy.onnx\r\n预训练神经网络\r\n\r\n\r\n\r\n\r\n🧠 神经网络策略说明\r\ngo2_policy.onnx├── 输入: 48 维向量│   ├── 线速度 (3)│   ├── 角速度 (3)│   ├── 重力方向 (3)│   ├── 关节位置差 (12)│   ├── 关节速度 (12)│   ├── 上一帧动作 (12)│   └── 目标速度 (3)│└── 输出: 12 维向量    └── 12 个关节的控制命令\r\n这个神经网络是通过强化学习 (Reinforcement Learning)\r\n训练得到的，让机器人学会： - 保持平衡 - 按照给定速度行走 -\r\n适应不同地形\r\n","categories":["机器仿真"],"tags":["Motrix"]}]